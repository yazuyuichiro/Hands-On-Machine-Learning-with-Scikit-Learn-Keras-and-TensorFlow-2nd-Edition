{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BTbwtfrfVCm"
      },
      "source": [
        "**Chapter 16 – Natural Language Processing with RNNs and Attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvLa0bMqfVCo"
      },
      "source": [
        "_This notebook contains all the sample code in chapter 16._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwdlz1EKfVCo"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/ageron/handson-ml2/blob/master/16_nlp_with_rnns_and_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-ml2/blob/master/16_nlp_with_rnns_and_attention.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI562jwnfVCp"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC-znAfsfVCp"
      },
      "source": [
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0.\n",
        "\n",
        "まず、一般的なモジュールをいくつかインポートし、MatplotLib がインラインで図を描画することを確認し、図を保存する関数を用意しましょう。また、Python 3.5以降がインストールされていること（Python 2.xでも動作しますが、非推奨なのでPython 3の使用を強く推奨します）、Scikit-Learn ≥0.20 と TensorFlow ≥2.0 も確認しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Vvej6EdfVCp"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Is this notebook running on Colab or Kaggle?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
        "\n",
        "if IS_COLAB:\n",
        "    %pip install -q -U tensorflow-addons\n",
        "    %pip install -q -U transformers\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "    if IS_KAGGLE:\n",
        "        print(\"Go to Settings > Accelerator and select GPU.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"nlp\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_eq5UJDfVCr"
      },
      "source": [
        "# Char-RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fEZokq9fVCr"
      },
      "source": [
        "## Splitting a sequence into batches of shuffled windows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDy27IFMfVCr"
      },
      "source": [
        "For example, let's split the sequence 0 to 14 into windows of length 5, each shifted by 2 (e.g.,`[0, 1, 2, 3, 4]`, `[2, 3, 4, 5, 6]`, etc.), then shuffle them, and split them into inputs (the first 4 steps) and targets (the last 4 steps) (e.g., `[2, 3, 4, 5, 6]` would be split into `[[2, 3, 4, 5], [3, 4, 5, 6]]`), then create batches of 3 such input/target pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHlo87m8fVCr",
        "outputId": "663061d4-8ea7-4e21-aac2-508adc12784a",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "____________________ Batch 0 \n",
            "X_batch\n",
            "[[6 7 8 9]\n",
            " [2 3 4 5]\n",
            " [4 5 6 7]]\n",
            "===== \n",
            "Y_batch\n",
            "[[ 7  8  9 10]\n",
            " [ 3  4  5  6]\n",
            " [ 5  6  7  8]]\n",
            "____________________ Batch 1 \n",
            "X_batch\n",
            "[[ 0  1  2  3]\n",
            " [ 8  9 10 11]\n",
            " [10 11 12 13]]\n",
            "===== \n",
            "Y_batch\n",
            "[[ 1  2  3  4]\n",
            " [ 9 10 11 12]\n",
            " [11 12 13 14]]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "n_steps = 5\n",
        "dataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\n",
        "dataset = dataset.window(n_steps, shift=2, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(n_steps))\n",
        "dataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:]))\n",
        "dataset = dataset.batch(3).prefetch(1)\n",
        "for index, (X_batch, Y_batch) in enumerate(dataset):\n",
        "    print(\"_\" * 20, \"Batch\", index, \"\\nX_batch\")\n",
        "    print(X_batch.numpy())\n",
        "    print(\"=\" * 5, \"\\nY_batch\")\n",
        "    print(Y_batch.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nif_sN8yfVCs"
      },
      "source": [
        "## Loading the Data and Preparing the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04SnjMH0fVCs",
        "outputId": "d7709ff3-b35b-4ca9-efde-642fe1bf3121"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYjTiOa_fVCs",
        "outputId": "1f404e81-d2de-4a9c-9226-e40a3cd9e946"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(shakespeare_text[:148])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eD-AGmX-fVCs",
        "outputId": "aed075cd-1cf7-437f-fa2e-7d9b27152eb9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\".join(sorted(set(shakespeare_text.lower())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xR8I9tlfVCs"
      },
      "outputs": [],
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(shakespeare_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wX_-UpufVCs",
        "outputId": "29afe73a-14d8-4626-98f9-8381fac671f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[20, 6, 9, 8, 3]]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.texts_to_sequences([\"First\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbBjMCjCfVCs",
        "outputId": "0244258a-1db7-4ede-a558-3c7346f10556"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['f i r s t']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3JdgaE_fVCs"
      },
      "outputs": [],
      "source": [
        "max_id = len(tokenizer.word_index) # number of distinct characters\n",
        "dataset_size = tokenizer.document_count # total number of characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkIiyaXhfVCt"
      },
      "outputs": [],
      "source": [
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
        "train_size = dataset_size * 90 // 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHE0pAtefVCt"
      },
      "source": [
        "**Note**: in previous versions of this code, we used `dataset.repeat()` now to make the dataset \"infinite\", and later in the notebook we set the `steps_per_epoch` argument when calling the `model.fit()` method. This was needed to work around some TensorFlow bugs. However, since these bugs have now been fixed, we can simplify the code: no need for `dataset.repeat()` or `steps_per_epoch` anymore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zi_7FFv2fVCt"
      },
      "outputs": [],
      "source": [
        "n_steps = 100\n",
        "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
        "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjKFogZ5fVCt"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELkY3A-yfVCt"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt35H4zZfVCt"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bjrj_3LQfVCt"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GG2hdPTzfVCt"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpsQPCvWfVCt",
        "outputId": "6bff1496-6213-405d-a026-50ae80965f13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 100, 39) (32, 100)\n"
          ]
        }
      ],
      "source": [
        "for X_batch, Y_batch in dataset.take(1):\n",
        "    print(X_batch.shape, Y_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGDX7TB5fVCt"
      },
      "source": [
        "## Creating and Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2Flu343fVCt"
      },
      "source": [
        "**Warning**: the following code may take up to 24 hours to run, depending on your hardware. If you use a GPU, it may take just 1 or 2 hours, or less."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJa8dosUfVCt"
      },
      "source": [
        "**Note**: the `GRU` class will only use the GPU (if you have one) when using the default values for the following arguments: `activation`, `recurrent_activation`, `recurrent_dropout`, `unroll`, `use_bias` and `reset_after`. This is why I commented out `recurrent_dropout=0.2` (compared to the book)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7esRfq1RfVCt",
        "outputId": "8c4bef44-1b1b-45e1-f98b-cff878b142f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "31368/31368 [==============================] - 7150s 228ms/step - loss: 1.4671\n",
            "Epoch 2/10\n",
            "31368/31368 [==============================] - 7094s 226ms/step - loss: 1.3614\n",
            "Epoch 3/10\n",
            "31368/31368 [==============================] - 7063s 225ms/step - loss: 1.3404\n",
            "Epoch 4/10\n",
            "31368/31368 [==============================] - 7039s 224ms/step - loss: 1.3311\n",
            "Epoch 5/10\n",
            "31368/31368 [==============================] - 7056s 225ms/step - loss: 1.3256\n",
            "Epoch 6/10\n",
            "31368/31368 [==============================] - 7049s 225ms/step - loss: 1.3209\n",
            "Epoch 7/10\n",
            "31368/31368 [==============================] - 7068s 225ms/step - loss: 1.3166\n",
            "Epoch 8/10\n",
            "31368/31368 [==============================] - 7030s 224ms/step - loss: 1.3138\n",
            "Epoch 9/10\n",
            "31368/31368 [==============================] - 7061s 225ms/step - loss: 1.3120\n",
            "Epoch 10/10\n",
            "31368/31368 [==============================] - 7177s 229ms/step - loss: 1.3105\n"
          ]
        }
      ],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
        "                     #dropout=0.2, recurrent_dropout=0.2),\n",
        "                     dropout=0.2),\n",
        "    keras.layers.GRU(128, return_sequences=True,\n",
        "                     #dropout=0.2, recurrent_dropout=0.2),\n",
        "                     dropout=0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
        "                                                    activation=\"softmax\"))\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "history = model.fit(dataset, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laAcKjxkfVCu"
      },
      "source": [
        "## Using the Model to Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zONGleapfVCu"
      },
      "outputs": [],
      "source": [
        "def preprocess(texts):\n",
        "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "    return tf.one_hot(X, max_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoXPfQ0lfVCu"
      },
      "source": [
        "**Warning**: the `predict_classes()` method is deprecated. Instead, we must use `np.argmax(model(X_new), axis=-1)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdVcOd_pfVCu",
        "outputId": "22729b77-a133-4c41-a30e-d536fe2817cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'u'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_new = preprocess([\"How are yo\"])\n",
        "#Y_pred = model.predict_classes(X_new)\n",
        "Y_pred = np.argmax(model(X_new), axis=-1)\n",
        "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJTgS8AbfVCu",
        "outputId": "b01f584c-9d85-43b7-c8fc-681f0beaf786"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 2, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 2, 1, 0, 2, 1,\n",
              "        0, 1, 2, 1, 1, 1, 2, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 2]])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zreivXRAfVCu"
      },
      "outputs": [],
      "source": [
        "def next_char(text, temperature=1):\n",
        "    X_new = preprocess([text])\n",
        "    y_proba = model(X_new)[0, -1:, :]\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2aukp7_fVCu",
        "outputId": "f5ba4dc4-0f8a-41e1-f25d-ce852f620a84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'u'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "next_char(\"How are yo\", temperature=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMFzpDFyfVCu"
      },
      "outputs": [],
      "source": [
        "def complete_text(text, n_chars=50, temperature=1):\n",
        "    for _ in range(n_chars):\n",
        "        text += next_char(text, temperature)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfxqZNCGfVCu",
        "outputId": "efda9614-96ff-4424-9d40-6fb54d0e5d48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the belly the charges of the other words\n",
            "and belly \n"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "print(complete_text(\"t\", temperature=0.2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FirWFtYhfVCu",
        "outputId": "f1c5f3b0-8de1-420a-81f8-ff599267cc66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "thing! they know't.\n",
            "\n",
            "biondello:\n",
            "for you are the own\n"
          ]
        }
      ],
      "source": [
        "print(complete_text(\"t\", temperature=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-pdIDxrfVCu",
        "outputId": "997d7464-cdd6-41cf-d0ac-6c4c92ba12cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "th no cyty\n",
            "use ffor was firive this toighingaber; b\n"
          ]
        }
      ],
      "source": [
        "print(complete_text(\"t\", temperature=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69Do-0NifVCu"
      },
      "source": [
        "## Stateful RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gU0cPuKdfVCv"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj9QvudrfVCv"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "dataset = dataset.batch(1)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra99y0sMfVCv"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
        "datasets = []\n",
        "for encoded_part in encoded_parts:\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
        "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "    datasets.append(dataset)\n",
        "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGKO0tV8fVCv"
      },
      "source": [
        "**Note**: once again, I commented out `recurrent_dropout=0.2` (compared to the book) so you can get GPU acceleration (if you have one)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t54TgpafVCv"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
        "                     #dropout=0.2, recurrent_dropout=0.2,\n",
        "                     dropout=0.2,\n",
        "                     batch_input_shape=[batch_size, None, max_id]),\n",
        "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
        "                     #dropout=0.2, recurrent_dropout=0.2),\n",
        "                     dropout=0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
        "                                                    activation=\"softmax\"))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcplXLxafVCv"
      },
      "outputs": [],
      "source": [
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs):\n",
        "        self.model.reset_states()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhagWs-xfVCv",
        "outputId": "f2173c29-e459-40a1-c0bc-cf94ee092d38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "313/313 [==============================] - 62s 198ms/step - loss: 2.6189\n",
            "Epoch 2/50\n",
            "313/313 [==============================] - 58s 187ms/step - loss: 2.2091\n",
            "Epoch 3/50\n",
            "313/313 [==============================] - 56s 178ms/step - loss: 2.0775\n",
            "Epoch 4/50\n",
            "313/313 [==============================] - 56s 179ms/step - loss: 2.4689\n",
            "Epoch 5/50\n",
            "313/313 [==============================] - 56s 179ms/step - loss: 2.3274\n",
            "Epoch 6/50\n",
            "313/313 [==============================] - 57s 183ms/step - loss: 2.1412\n",
            "Epoch 7/50\n",
            "313/313 [==============================] - 57s 183ms/step - loss: 2.0748\n",
            "Epoch 8/50\n",
            "313/313 [==============================] - 56s 179ms/step - loss: 1.9850\n",
            "Epoch 9/50\n",
            "313/313 [==============================] - 56s 179ms/step - loss: 1.9465\n",
            "Epoch 10/50\n",
            "313/313 [==============================] - 56s 179ms/step - loss: 1.8995\n",
            "Epoch 11/50\n",
            "313/313 [==============================] - 57s 182ms/step - loss: 1.8576\n",
            "Epoch 12/50\n",
            "313/313 [==============================] - 56s 179ms/step - loss: 1.8510\n",
            "Epoch 13/50\n",
            "313/313 [==============================] - 57s 184ms/step - loss: 1.8038\n",
            "Epoch 14/50\n",
            "313/313 [==============================] - 56s 178ms/step - loss: 1.7867\n",
            "Epoch 15/50\n",
            "313/313 [==============================] - 56s 180ms/step - loss: 1.7635\n",
            "Epoch 16/50\n",
            "313/313 [==============================] - 56s 179ms/step - loss: 1.7270\n",
            "Epoch 17/50\n",
            "313/313 [==============================] - 58s 184ms/step - loss: 1.7097\n",
            "<<31 more lines>>\n",
            "313/313 [==============================] - 58s 185ms/step - loss: 1.5998\n",
            "Epoch 34/50\n",
            "313/313 [==============================] - 58s 184ms/step - loss: 1.5954\n",
            "Epoch 35/50\n",
            "313/313 [==============================] - 58s 185ms/step - loss: 1.5944\n",
            "Epoch 36/50\n",
            "313/313 [==============================] - 57s 183ms/step - loss: 1.5902\n",
            "Epoch 37/50\n",
            "313/313 [==============================] - 57s 183ms/step - loss: 1.5893\n",
            "Epoch 38/50\n",
            "313/313 [==============================] - 59s 187ms/step - loss: 1.5845\n",
            "Epoch 39/50\n",
            "313/313 [==============================] - 57s 183ms/step - loss: 1.5821\n",
            "Epoch 40/50\n",
            "313/313 [==============================] - 59s 187ms/step - loss: 1.5798\n",
            "Epoch 41/50\n",
            "313/313 [==============================] - 57s 181ms/step - loss: 1.5794\n",
            "Epoch 42/50\n",
            "313/313 [==============================] - 57s 182ms/step - loss: 1.5774\n",
            "Epoch 43/50\n",
            "313/313 [==============================] - 57s 182ms/step - loss: 1.5755\n",
            "Epoch 44/50\n",
            "313/313 [==============================] - 58s 186ms/step - loss: 1.5735\n",
            "Epoch 45/50\n",
            "313/313 [==============================] - 58s 186ms/step - loss: 1.5714\n",
            "Epoch 46/50\n",
            "313/313 [==============================] - 57s 181ms/step - loss: 1.5686\n",
            "Epoch 47/50\n",
            "313/313 [==============================] - 57s 181ms/step - loss: 1.5675\n",
            "Epoch 48/50\n",
            "313/313 [==============================] - 56s 180ms/step - loss: 1.5657\n",
            "Epoch 49/50\n",
            "313/313 [==============================] - 58s 185ms/step - loss: 1.5654\n",
            "Epoch 50/50\n",
            "313/313 [==============================] - 57s 182ms/step - loss: 1.5620\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8d45d95d10>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "history = model.fit(dataset, epochs=50,\n",
        "                    callbacks=[ResetStatesCallback()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI8L3BVqfVCv"
      },
      "source": [
        "To use the model with different batch sizes, we need to create a stateless copy. We can get rid of dropout since it is only used during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKiLv9XGfVCv"
      },
      "outputs": [],
      "source": [
        "stateless_model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
        "                                                    activation=\"softmax\"))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuTpjDIofVCv"
      },
      "source": [
        "To set the weights, we first need to build the model (so the weights get created):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFYvYlsPfVCv"
      },
      "outputs": [],
      "source": [
        "stateless_model.build(tf.TensorShape([None, None, max_id]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT1TAIVNfVCv"
      },
      "outputs": [],
      "source": [
        "stateless_model.set_weights(model.get_weights())\n",
        "model = stateless_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfZ1I1gifVCv",
        "outputId": "2b7e22ac-f8ef-498d-fe36-9bcb1b9645e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tor:\n",
            "in the negver up how it thou like him;\n",
            "when it\n"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "print(complete_text(\"t\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkqyoV8dfVCv"
      },
      "source": [
        "# Sentiment Analysis(感情分析)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNqf7dm5Ih-A"
      },
      "source": [
        "https://note.com/mlai/n/ndd0643e2a843\n",
        "https://www.tensorflow.org/tutorials/keras/text_classification\n",
        "https://atmarkit.itmedia.co.jp/ait/articles/2007/22/news028.html\n",
        "https://qiita.com/hkambe/items/8c56ca8f0bbb4f895dee\n",
        "https://www.youtube.com/watch?v=Vo9Atzozv0w&t=629s\n",
        "https://www.imdb.com/\n",
        "https://www.kkaneko.jp/ai/classify/imdb.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxQCls-qfVCv"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crH-2BSkfVCv"
      },
      "source": [
        "You can load the IMDB dataset easily:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKPAJIj8fVCv"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()\n",
        "#KerasのIMDbをロードするための関数\n",
        "#英語で書かれた5万本の映画レビュー（25,000 本が訓練用、25,000 本がテスト用）を抽出したもの。\n",
        "#個々のレビューが映画に対して（1）好意的か、（0）そうでないかを表す二項ターゲットを付けたもの。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwdxWbTnsepV",
        "outputId": "44e52d79-d8ac-4767-b090-e9f6fd857e8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS3U-hYcssjz",
        "outputId": "12de7216-0329-4ede-d964-f0473eb1178d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Mdy4-dQs6-O",
        "outputId": "0ccba893-4f48-4719-9f59-51e1411b8b4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 22665,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 21631,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 19193,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 10311,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 31050,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 12118,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[0]\n",
        "#映画のレビュー文。前処理がされており、配列の個々の整数は単語を表している。小さな値の整数は頻出語を表している。\n",
        "#頻度順にインデックス番号を割り当てられている。（つまり、小さい値の整数は頻出語を表している。）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-Iv6sxSSYpe",
        "outputId": "bf4760f9-8d94-46bf-8844-74e9ae5b0c5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_train[0]) #つまり、０番は、218単語で構成される文章であることが分かる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F9TsowQxJOz"
      },
      "outputs": [],
      "source": [
        "#X_train[100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_pWSPZ2xJXW"
      },
      "outputs": [],
      "source": [
        "#len(X_train[100]) #それぞれ文章の長さが異なることが分かる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcUfC0aTycsJ"
      },
      "outputs": [],
      "source": [
        "#長さが異なるのは扱いが難しいので処理する必要がある。sequences_to_matrixで、配列から行列にする。\n",
        "#from keras.preprocessing.text import Tokenizer\n",
        "#tokenizer = Tokenizer(num_words=10000)\n",
        "#X_train = tokenizer.sequences_to_matrix(X_train,mode=\"binary\")\n",
        "#X_test = tokenizer.sequences_to_matrix(X_test, mode=\"binary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfsRxXGd21X2"
      },
      "outputs": [],
      "source": [
        "#X_train.shape#文章25,000個、単語数が10,000個（num_words=10000で指定したため）。\n",
        "#つまり、２５，０００行、10,000列の行列になっている。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o8pfEcM7Y5F"
      },
      "outputs": [],
      "source": [
        "#X_train[0]#X_trainの０番目がどうなっているか見てみる。つまり、０番の単語は入ってない、１番の単語は入ってる、２番の単語は入ってない・・・という意味。\n",
        "#https://www.youtube.com/watch?v=Vo9Atzozv0w&t=629s 23分"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuyMtQ775X3v"
      },
      "outputs": [],
      "source": [
        "#文章はナムパイ配列（array）に変わった。ラベルも、ナムパイ配列に変更する。\n",
        "#y_train =np.asarray(y_train).astype(\"float32\")\n",
        "#y_test =np.asarray(y_test).astype(\"float32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxRXGGhR9F3L"
      },
      "outputs": [],
      "source": [
        "#モデルの精度を確認するため、検証用のセットを用意する\n",
        "#（２５，０００の文章を訓練用データと実際に使うデータに分ける）\n",
        "#from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_o7koTb9CqBZ"
      },
      "outputs": [],
      "source": [
        "#X_train, X_val, y_train, y_val =train_test_split(X_train, y_train,test_size=0.1, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aWW2hveCtgP"
      },
      "outputs": [],
      "source": [
        "#print(X_train.shape, X_val.shape)#22500が実際に学習に用いる用、２５００が検証に用いる用、10,000は、行列の単語の列を指す。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSeayaHe5CBQ",
        "outputId": "1c43c22e-f8dd-44ea-fa4a-0f51e8550fe6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 22665,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 21631,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 19193,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 10311,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 31050,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 12118,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfcYV_MBUe7U",
        "outputId": "52b018e6-f055-4bcf-ed3f-f37ad5c9aed8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train[0]#つまり、上記の２１８単語から構成される文章は、肯定的なレビューであることが分かる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBxo1-9PpLfl",
        "outputId": "6cabdcaa-542f-44aa-fce1-e907ef981b79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 1, 0])"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train#0か１のラベルが入っている。１の時は肯定的、０の時は否定的"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLjQasXHfVCw",
        "outputId": "f0d6597a-7e45-4431-ea41-c61e4290facb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[0][:10]#数字が入っている。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pmGSgKuDfVCw",
        "outputId": "a547082b-c393-4892-cd49-2ce6a6b9ca3f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<sos> this film was just brilliant casting location scenery story'"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_index = keras.datasets.imdb.get_word_index()#数字が日本語に戻る　→ <sos> this film was just brilliant casting location scenery story\n",
        "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
        "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "    id_to_word[id_] = token\n",
        "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])\n",
        "\n",
        "#word_index 最初の１から３は、予約語です。(\"<pad>\", \"<sos>\", \"<unk>\")\n",
        "#例えば、brilliantを検索すると、527だが、＋３すると、530になる。\n",
        "#https://www.kkaneko.jp/ai/classify/imdb.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL6ZJn9b_cq8",
        "outputId": "9585addc-135a-48e1-9a1d-3e59c06680f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'fawn': 34701,\n",
              " 'tsukino': 52006,\n",
              " 'nunnery': 52007,\n",
              " 'sonja': 16816,\n",
              " 'vani': 63951,\n",
              " 'woods': 1408,\n",
              " 'spiders': 16115,\n",
              " 'hanging': 2345,\n",
              " 'woody': 2289,\n",
              " 'trawling': 52008,\n",
              " \"hold's\": 52009,\n",
              " 'comically': 11307,\n",
              " 'localized': 40830,\n",
              " 'disobeying': 30568,\n",
              " \"'royale\": 52010,\n",
              " \"harpo's\": 40831,\n",
              " 'canet': 52011,\n",
              " 'aileen': 19313,\n",
              " 'acurately': 52012,\n",
              " \"diplomat's\": 52013,\n",
              " 'rickman': 25242,\n",
              " 'arranged': 6746,\n",
              " 'rumbustious': 52014,\n",
              " 'familiarness': 52015,\n",
              " \"spider'\": 52016,\n",
              " 'hahahah': 68804,\n",
              " \"wood'\": 52017,\n",
              " 'transvestism': 40833,\n",
              " \"hangin'\": 34702,\n",
              " 'bringing': 2338,\n",
              " 'seamier': 40834,\n",
              " 'wooded': 34703,\n",
              " 'bravora': 52018,\n",
              " 'grueling': 16817,\n",
              " 'wooden': 1636,\n",
              " 'wednesday': 16818,\n",
              " \"'prix\": 52019,\n",
              " 'altagracia': 34704,\n",
              " 'circuitry': 52020,\n",
              " 'crotch': 11585,\n",
              " 'busybody': 57766,\n",
              " \"tart'n'tangy\": 52021,\n",
              " 'burgade': 14129,\n",
              " 'thrace': 52023,\n",
              " \"tom's\": 11038,\n",
              " 'snuggles': 52025,\n",
              " 'francesco': 29114,\n",
              " 'complainers': 52027,\n",
              " 'templarios': 52125,\n",
              " '272': 40835,\n",
              " '273': 52028,\n",
              " 'zaniacs': 52130,\n",
              " '275': 34706,\n",
              " 'consenting': 27631,\n",
              " 'snuggled': 40836,\n",
              " 'inanimate': 15492,\n",
              " 'uality': 52030,\n",
              " 'bronte': 11926,\n",
              " 'errors': 4010,\n",
              " 'dialogs': 3230,\n",
              " \"yomada's\": 52031,\n",
              " \"madman's\": 34707,\n",
              " 'dialoge': 30585,\n",
              " 'usenet': 52033,\n",
              " 'videodrome': 40837,\n",
              " \"kid'\": 26338,\n",
              " 'pawed': 52034,\n",
              " \"'girlfriend'\": 30569,\n",
              " \"'pleasure\": 52035,\n",
              " \"'reloaded'\": 52036,\n",
              " \"kazakos'\": 40839,\n",
              " 'rocque': 52037,\n",
              " 'mailings': 52038,\n",
              " 'brainwashed': 11927,\n",
              " 'mcanally': 16819,\n",
              " \"tom''\": 52039,\n",
              " 'kurupt': 25243,\n",
              " 'affiliated': 21905,\n",
              " 'babaganoosh': 52040,\n",
              " \"noe's\": 40840,\n",
              " 'quart': 40841,\n",
              " 'kids': 359,\n",
              " 'uplifting': 5034,\n",
              " 'controversy': 7093,\n",
              " 'kida': 21906,\n",
              " 'kidd': 23379,\n",
              " \"error'\": 52041,\n",
              " 'neurologist': 52042,\n",
              " 'spotty': 18510,\n",
              " 'cobblers': 30570,\n",
              " 'projection': 9878,\n",
              " 'fastforwarding': 40842,\n",
              " 'sters': 52043,\n",
              " \"eggar's\": 52044,\n",
              " 'etherything': 52045,\n",
              " 'gateshead': 40843,\n",
              " 'airball': 34708,\n",
              " 'unsinkable': 25244,\n",
              " 'stern': 7180,\n",
              " \"cervi's\": 52046,\n",
              " 'dnd': 40844,\n",
              " 'dna': 11586,\n",
              " 'insecurity': 20598,\n",
              " \"'reboot'\": 52047,\n",
              " 'trelkovsky': 11037,\n",
              " 'jaekel': 52048,\n",
              " 'sidebars': 52049,\n",
              " \"sforza's\": 52050,\n",
              " 'distortions': 17633,\n",
              " 'mutinies': 52051,\n",
              " 'sermons': 30602,\n",
              " '7ft': 40846,\n",
              " 'boobage': 52052,\n",
              " \"o'bannon's\": 52053,\n",
              " 'populations': 23380,\n",
              " 'chulak': 52054,\n",
              " 'mesmerize': 27633,\n",
              " 'quinnell': 52055,\n",
              " 'yahoo': 10307,\n",
              " 'meteorologist': 52057,\n",
              " 'beswick': 42577,\n",
              " 'boorman': 15493,\n",
              " 'voicework': 40847,\n",
              " \"ster'\": 52058,\n",
              " 'blustering': 22922,\n",
              " 'hj': 52059,\n",
              " 'intake': 27634,\n",
              " 'morally': 5621,\n",
              " 'jumbling': 40849,\n",
              " 'bowersock': 52060,\n",
              " \"'porky's'\": 52061,\n",
              " 'gershon': 16821,\n",
              " 'ludicrosity': 40850,\n",
              " 'coprophilia': 52062,\n",
              " 'expressively': 40851,\n",
              " \"india's\": 19500,\n",
              " \"post's\": 34710,\n",
              " 'wana': 52063,\n",
              " 'wang': 5283,\n",
              " 'wand': 30571,\n",
              " 'wane': 25245,\n",
              " 'edgeways': 52321,\n",
              " 'titanium': 34711,\n",
              " 'pinta': 40852,\n",
              " 'want': 178,\n",
              " 'pinto': 30572,\n",
              " 'whoopdedoodles': 52065,\n",
              " 'tchaikovsky': 21908,\n",
              " 'travel': 2103,\n",
              " \"'victory'\": 52066,\n",
              " 'copious': 11928,\n",
              " 'gouge': 22433,\n",
              " \"chapters'\": 52067,\n",
              " 'barbra': 6702,\n",
              " 'uselessness': 30573,\n",
              " \"wan'\": 52068,\n",
              " 'assimilated': 27635,\n",
              " 'petiot': 16116,\n",
              " 'most\\x85and': 52069,\n",
              " 'dinosaurs': 3930,\n",
              " 'wrong': 352,\n",
              " 'seda': 52070,\n",
              " 'stollen': 52071,\n",
              " 'sentencing': 34712,\n",
              " 'ouroboros': 40853,\n",
              " 'assimilates': 40854,\n",
              " 'colorfully': 40855,\n",
              " 'glenne': 27636,\n",
              " 'dongen': 52072,\n",
              " 'subplots': 4760,\n",
              " 'kiloton': 52073,\n",
              " 'chandon': 23381,\n",
              " \"effect'\": 34713,\n",
              " 'snugly': 27637,\n",
              " 'kuei': 40856,\n",
              " 'welcomed': 9092,\n",
              " 'dishonor': 30071,\n",
              " 'concurrence': 52075,\n",
              " 'stoicism': 23382,\n",
              " \"guys'\": 14896,\n",
              " \"beroemd'\": 52077,\n",
              " 'butcher': 6703,\n",
              " \"melfi's\": 40857,\n",
              " 'aargh': 30623,\n",
              " 'playhouse': 20599,\n",
              " 'wickedly': 11308,\n",
              " 'fit': 1180,\n",
              " 'labratory': 52078,\n",
              " 'lifeline': 40859,\n",
              " 'screaming': 1927,\n",
              " 'fix': 4287,\n",
              " 'cineliterate': 52079,\n",
              " 'fic': 52080,\n",
              " 'fia': 52081,\n",
              " 'fig': 34714,\n",
              " 'fmvs': 52082,\n",
              " 'fie': 52083,\n",
              " 'reentered': 52084,\n",
              " 'fin': 30574,\n",
              " 'doctresses': 52085,\n",
              " 'fil': 52086,\n",
              " 'zucker': 12606,\n",
              " 'ached': 31931,\n",
              " 'counsil': 52088,\n",
              " 'paterfamilias': 52089,\n",
              " 'songwriter': 13885,\n",
              " 'shivam': 34715,\n",
              " 'hurting': 9654,\n",
              " 'effects': 299,\n",
              " 'slauther': 52090,\n",
              " \"'flame'\": 52091,\n",
              " 'sommerset': 52092,\n",
              " 'interwhined': 52093,\n",
              " 'whacking': 27638,\n",
              " 'bartok': 52094,\n",
              " 'barton': 8775,\n",
              " 'frewer': 21909,\n",
              " \"fi'\": 52095,\n",
              " 'ingrid': 6192,\n",
              " 'stribor': 30575,\n",
              " 'approporiately': 52096,\n",
              " 'wobblyhand': 52097,\n",
              " 'tantalisingly': 52098,\n",
              " 'ankylosaurus': 52099,\n",
              " 'parasites': 17634,\n",
              " 'childen': 52100,\n",
              " \"jenkins'\": 52101,\n",
              " 'metafiction': 52102,\n",
              " 'golem': 17635,\n",
              " 'indiscretion': 40860,\n",
              " \"reeves'\": 23383,\n",
              " \"inamorata's\": 57781,\n",
              " 'brittannica': 52104,\n",
              " 'adapt': 7916,\n",
              " \"russo's\": 30576,\n",
              " 'guitarists': 48246,\n",
              " 'abbott': 10553,\n",
              " 'abbots': 40861,\n",
              " 'lanisha': 17649,\n",
              " 'magickal': 40863,\n",
              " 'mattter': 52105,\n",
              " \"'willy\": 52106,\n",
              " 'pumpkins': 34716,\n",
              " 'stuntpeople': 52107,\n",
              " 'estimate': 30577,\n",
              " 'ugghhh': 40864,\n",
              " 'gameplay': 11309,\n",
              " \"wern't\": 52108,\n",
              " \"n'sync\": 40865,\n",
              " 'sickeningly': 16117,\n",
              " 'chiara': 40866,\n",
              " 'disturbed': 4011,\n",
              " 'portmanteau': 40867,\n",
              " 'ineffectively': 52109,\n",
              " \"duchonvey's\": 82143,\n",
              " \"nasty'\": 37519,\n",
              " 'purpose': 1285,\n",
              " 'lazers': 52112,\n",
              " 'lightened': 28105,\n",
              " 'kaliganj': 52113,\n",
              " 'popularism': 52114,\n",
              " \"damme's\": 18511,\n",
              " 'stylistics': 30578,\n",
              " 'mindgaming': 52115,\n",
              " 'spoilerish': 46449,\n",
              " \"'corny'\": 52117,\n",
              " 'boerner': 34718,\n",
              " 'olds': 6792,\n",
              " 'bakelite': 52118,\n",
              " 'renovated': 27639,\n",
              " 'forrester': 27640,\n",
              " \"lumiere's\": 52119,\n",
              " 'gaskets': 52024,\n",
              " 'needed': 884,\n",
              " 'smight': 34719,\n",
              " 'master': 1297,\n",
              " \"edie's\": 25905,\n",
              " 'seeber': 40868,\n",
              " 'hiya': 52120,\n",
              " 'fuzziness': 52121,\n",
              " 'genesis': 14897,\n",
              " 'rewards': 12607,\n",
              " 'enthrall': 30579,\n",
              " \"'about\": 40869,\n",
              " \"recollection's\": 52122,\n",
              " 'mutilated': 11039,\n",
              " 'fatherlands': 52123,\n",
              " \"fischer's\": 52124,\n",
              " 'positively': 5399,\n",
              " '270': 34705,\n",
              " 'ahmed': 34720,\n",
              " 'zatoichi': 9836,\n",
              " 'bannister': 13886,\n",
              " 'anniversaries': 52127,\n",
              " \"helm's\": 30580,\n",
              " \"'work'\": 52128,\n",
              " 'exclaimed': 34721,\n",
              " \"'unfunny'\": 52129,\n",
              " '274': 52029,\n",
              " 'feeling': 544,\n",
              " \"wanda's\": 52131,\n",
              " 'dolan': 33266,\n",
              " '278': 52133,\n",
              " 'peacoat': 52134,\n",
              " 'brawny': 40870,\n",
              " 'mishra': 40871,\n",
              " 'worlders': 40872,\n",
              " 'protags': 52135,\n",
              " 'skullcap': 52136,\n",
              " 'dastagir': 57596,\n",
              " 'affairs': 5622,\n",
              " 'wholesome': 7799,\n",
              " 'hymen': 52137,\n",
              " 'paramedics': 25246,\n",
              " 'unpersons': 52138,\n",
              " 'heavyarms': 52139,\n",
              " 'affaire': 52140,\n",
              " 'coulisses': 52141,\n",
              " 'hymer': 40873,\n",
              " 'kremlin': 52142,\n",
              " 'shipments': 30581,\n",
              " 'pixilated': 52143,\n",
              " \"'00s\": 30582,\n",
              " 'diminishing': 18512,\n",
              " 'cinematic': 1357,\n",
              " 'resonates': 14898,\n",
              " 'simplify': 40874,\n",
              " \"nature'\": 40875,\n",
              " 'temptresses': 40876,\n",
              " 'reverence': 16822,\n",
              " 'resonated': 19502,\n",
              " 'dailey': 34722,\n",
              " '2\\x85': 52144,\n",
              " 'treize': 27641,\n",
              " 'majo': 52145,\n",
              " 'kiya': 21910,\n",
              " 'woolnough': 52146,\n",
              " 'thanatos': 39797,\n",
              " 'sandoval': 35731,\n",
              " 'dorama': 40879,\n",
              " \"o'shaughnessy\": 52147,\n",
              " 'tech': 4988,\n",
              " 'fugitives': 32018,\n",
              " 'teck': 30583,\n",
              " \"'e'\": 76125,\n",
              " 'doesn’t': 40881,\n",
              " 'purged': 52149,\n",
              " 'saying': 657,\n",
              " \"martians'\": 41095,\n",
              " 'norliss': 23418,\n",
              " 'dickey': 27642,\n",
              " 'dicker': 52152,\n",
              " \"'sependipity\": 52153,\n",
              " 'padded': 8422,\n",
              " 'ordell': 57792,\n",
              " \"sturges'\": 40882,\n",
              " 'independentcritics': 52154,\n",
              " 'tempted': 5745,\n",
              " \"atkinson's\": 34724,\n",
              " 'hounded': 25247,\n",
              " 'apace': 52155,\n",
              " 'clicked': 15494,\n",
              " \"'humor'\": 30584,\n",
              " \"martino's\": 17177,\n",
              " \"'supporting\": 52156,\n",
              " 'warmongering': 52032,\n",
              " \"zemeckis's\": 34725,\n",
              " 'lube': 21911,\n",
              " 'shocky': 52157,\n",
              " 'plate': 7476,\n",
              " 'plata': 40883,\n",
              " 'sturgess': 40884,\n",
              " \"nerds'\": 40885,\n",
              " 'plato': 20600,\n",
              " 'plath': 34726,\n",
              " 'platt': 40886,\n",
              " 'mcnab': 52159,\n",
              " 'clumsiness': 27643,\n",
              " 'altogether': 3899,\n",
              " 'massacring': 42584,\n",
              " 'bicenntinial': 52160,\n",
              " 'skaal': 40887,\n",
              " 'droning': 14360,\n",
              " 'lds': 8776,\n",
              " 'jaguar': 21912,\n",
              " \"cale's\": 34727,\n",
              " 'nicely': 1777,\n",
              " 'mummy': 4588,\n",
              " \"lot's\": 18513,\n",
              " 'patch': 10086,\n",
              " 'kerkhof': 50202,\n",
              " \"leader's\": 52161,\n",
              " \"'movie\": 27644,\n",
              " 'uncomfirmed': 52162,\n",
              " 'heirloom': 40888,\n",
              " 'wrangle': 47360,\n",
              " 'emotion\\x85': 52163,\n",
              " \"'stargate'\": 52164,\n",
              " 'pinoy': 40889,\n",
              " 'conchatta': 40890,\n",
              " 'broeke': 41128,\n",
              " 'advisedly': 40891,\n",
              " \"barker's\": 17636,\n",
              " 'descours': 52166,\n",
              " 'lots': 772,\n",
              " 'lotr': 9259,\n",
              " 'irs': 9879,\n",
              " 'lott': 52167,\n",
              " 'xvi': 40892,\n",
              " 'irk': 34728,\n",
              " 'irl': 52168,\n",
              " 'ira': 6887,\n",
              " 'belzer': 21913,\n",
              " 'irc': 52169,\n",
              " 'ire': 27645,\n",
              " 'requisites': 40893,\n",
              " 'discipline': 7693,\n",
              " 'lyoko': 52961,\n",
              " 'extend': 11310,\n",
              " 'nature': 873,\n",
              " \"'dickie'\": 52170,\n",
              " 'optimist': 40894,\n",
              " 'lapping': 30586,\n",
              " 'superficial': 3900,\n",
              " 'vestment': 52171,\n",
              " 'extent': 2823,\n",
              " 'tendons': 52172,\n",
              " \"heller's\": 52173,\n",
              " 'quagmires': 52174,\n",
              " 'miyako': 52175,\n",
              " 'moocow': 20601,\n",
              " \"coles'\": 52176,\n",
              " 'lookit': 40895,\n",
              " 'ravenously': 52177,\n",
              " 'levitating': 40896,\n",
              " 'perfunctorily': 52178,\n",
              " 'lookin': 30587,\n",
              " \"lot'\": 40898,\n",
              " 'lookie': 52179,\n",
              " 'fearlessly': 34870,\n",
              " 'libyan': 52181,\n",
              " 'fondles': 40899,\n",
              " 'gopher': 35714,\n",
              " 'wearying': 40901,\n",
              " \"nz's\": 52182,\n",
              " 'minuses': 27646,\n",
              " 'puposelessly': 52183,\n",
              " 'shandling': 52184,\n",
              " 'decapitates': 31268,\n",
              " 'humming': 11929,\n",
              " \"'nother\": 40902,\n",
              " 'smackdown': 21914,\n",
              " 'underdone': 30588,\n",
              " 'frf': 40903,\n",
              " 'triviality': 52185,\n",
              " 'fro': 25248,\n",
              " 'bothers': 8777,\n",
              " \"'kensington\": 52186,\n",
              " 'much': 73,\n",
              " 'muco': 34730,\n",
              " 'wiseguy': 22615,\n",
              " \"richie's\": 27648,\n",
              " 'tonino': 40904,\n",
              " 'unleavened': 52187,\n",
              " 'fry': 11587,\n",
              " \"'tv'\": 40905,\n",
              " 'toning': 40906,\n",
              " 'obese': 14361,\n",
              " 'sensationalized': 30589,\n",
              " 'spiv': 40907,\n",
              " 'spit': 6259,\n",
              " 'arkin': 7364,\n",
              " 'charleton': 21915,\n",
              " 'jeon': 16823,\n",
              " 'boardroom': 21916,\n",
              " 'doubts': 4989,\n",
              " 'spin': 3084,\n",
              " 'hepo': 53083,\n",
              " 'wildcat': 27649,\n",
              " 'venoms': 10584,\n",
              " 'misconstrues': 52191,\n",
              " 'mesmerising': 18514,\n",
              " 'misconstrued': 40908,\n",
              " 'rescinds': 52192,\n",
              " 'prostrate': 52193,\n",
              " 'majid': 40909,\n",
              " 'climbed': 16479,\n",
              " 'canoeing': 34731,\n",
              " 'majin': 52195,\n",
              " 'animie': 57804,\n",
              " 'sylke': 40910,\n",
              " 'conditioned': 14899,\n",
              " 'waddell': 40911,\n",
              " '3\\x85': 52196,\n",
              " 'hyperdrive': 41188,\n",
              " 'conditioner': 34732,\n",
              " 'bricklayer': 53153,\n",
              " 'hong': 2576,\n",
              " 'memoriam': 52198,\n",
              " 'inventively': 30592,\n",
              " \"levant's\": 25249,\n",
              " 'portobello': 20638,\n",
              " 'remand': 52200,\n",
              " 'mummified': 19504,\n",
              " 'honk': 27650,\n",
              " 'spews': 19505,\n",
              " 'visitations': 40912,\n",
              " 'mummifies': 52201,\n",
              " 'cavanaugh': 25250,\n",
              " 'zeon': 23385,\n",
              " \"jungle's\": 40913,\n",
              " 'viertel': 34733,\n",
              " 'frenchmen': 27651,\n",
              " 'torpedoes': 52202,\n",
              " 'schlessinger': 52203,\n",
              " 'torpedoed': 34734,\n",
              " 'blister': 69876,\n",
              " 'cinefest': 52204,\n",
              " 'furlough': 34735,\n",
              " 'mainsequence': 52205,\n",
              " 'mentors': 40914,\n",
              " 'academic': 9094,\n",
              " 'stillness': 20602,\n",
              " 'academia': 40915,\n",
              " 'lonelier': 52206,\n",
              " 'nibby': 52207,\n",
              " \"losers'\": 52208,\n",
              " 'cineastes': 40916,\n",
              " 'corporate': 4449,\n",
              " 'massaging': 40917,\n",
              " 'bellow': 30593,\n",
              " 'absurdities': 19506,\n",
              " 'expetations': 53241,\n",
              " 'nyfiken': 40918,\n",
              " 'mehras': 75638,\n",
              " 'lasse': 52209,\n",
              " 'visability': 52210,\n",
              " 'militarily': 33946,\n",
              " \"elder'\": 52211,\n",
              " 'gainsbourg': 19023,\n",
              " 'hah': 20603,\n",
              " 'hai': 13420,\n",
              " 'haj': 34736,\n",
              " 'hak': 25251,\n",
              " 'hal': 4311,\n",
              " 'ham': 4892,\n",
              " 'duffer': 53259,\n",
              " 'haa': 52213,\n",
              " 'had': 66,\n",
              " 'advancement': 11930,\n",
              " 'hag': 16825,\n",
              " \"hand'\": 25252,\n",
              " 'hay': 13421,\n",
              " 'mcnamara': 20604,\n",
              " \"mozart's\": 52214,\n",
              " 'duffel': 30731,\n",
              " 'haq': 30594,\n",
              " 'har': 13887,\n",
              " 'has': 44,\n",
              " 'hat': 2401,\n",
              " 'hav': 40919,\n",
              " 'haw': 30595,\n",
              " 'figtings': 52215,\n",
              " 'elders': 15495,\n",
              " 'underpanted': 52216,\n",
              " 'pninson': 52217,\n",
              " 'unequivocally': 27652,\n",
              " \"barbara's\": 23673,\n",
              " \"bello'\": 52219,\n",
              " 'indicative': 12997,\n",
              " 'yawnfest': 40920,\n",
              " 'hexploitation': 52220,\n",
              " \"loder's\": 52221,\n",
              " 'sleuthing': 27653,\n",
              " \"justin's\": 32622,\n",
              " \"'ball\": 52222,\n",
              " \"'summer\": 52223,\n",
              " \"'demons'\": 34935,\n",
              " \"mormon's\": 52225,\n",
              " \"laughton's\": 34737,\n",
              " 'debell': 52226,\n",
              " 'shipyard': 39724,\n",
              " 'unabashedly': 30597,\n",
              " 'disks': 40401,\n",
              " 'crowd': 2290,\n",
              " 'crowe': 10087,\n",
              " \"vancouver's\": 56434,\n",
              " 'mosques': 34738,\n",
              " 'crown': 6627,\n",
              " 'culpas': 52227,\n",
              " 'crows': 27654,\n",
              " 'surrell': 53344,\n",
              " 'flowless': 52229,\n",
              " 'sheirk': 52230,\n",
              " \"'three\": 40923,\n",
              " \"peterson'\": 52231,\n",
              " 'ooverall': 52232,\n",
              " 'perchance': 40924,\n",
              " 'bottom': 1321,\n",
              " 'chabert': 53363,\n",
              " 'sneha': 52233,\n",
              " 'inhuman': 13888,\n",
              " 'ichii': 52234,\n",
              " 'ursla': 52235,\n",
              " 'completly': 30598,\n",
              " 'moviedom': 40925,\n",
              " 'raddick': 52236,\n",
              " 'brundage': 51995,\n",
              " 'brigades': 40926,\n",
              " 'starring': 1181,\n",
              " \"'goal'\": 52237,\n",
              " 'caskets': 52238,\n",
              " 'willcock': 52239,\n",
              " \"threesome's\": 52240,\n",
              " \"mosque'\": 52241,\n",
              " \"cover's\": 52242,\n",
              " 'spaceships': 17637,\n",
              " 'anomalous': 40927,\n",
              " 'ptsd': 27655,\n",
              " 'shirdan': 52243,\n",
              " 'obscenity': 21962,\n",
              " 'lemmings': 30599,\n",
              " 'duccio': 30600,\n",
              " \"levene's\": 52244,\n",
              " \"'gorby'\": 52245,\n",
              " \"teenager's\": 25255,\n",
              " 'marshall': 5340,\n",
              " 'honeymoon': 9095,\n",
              " 'shoots': 3231,\n",
              " 'despised': 12258,\n",
              " 'okabasho': 52246,\n",
              " 'fabric': 8289,\n",
              " 'cannavale': 18515,\n",
              " 'raped': 3537,\n",
              " \"tutt's\": 52247,\n",
              " 'grasping': 17638,\n",
              " 'despises': 18516,\n",
              " \"thief's\": 40928,\n",
              " 'rapes': 8926,\n",
              " 'raper': 52248,\n",
              " \"eyre'\": 27656,\n",
              " 'walchek': 52249,\n",
              " \"elmo's\": 23386,\n",
              " 'perfumes': 40929,\n",
              " 'spurting': 21918,\n",
              " \"exposition'\\x85\": 52250,\n",
              " 'denoting': 52251,\n",
              " 'thesaurus': 34740,\n",
              " \"shoot'\": 40930,\n",
              " 'bonejack': 49759,\n",
              " 'simpsonian': 52253,\n",
              " 'hebetude': 30601,\n",
              " \"hallow's\": 34741,\n",
              " 'desperation\\x85': 52254,\n",
              " 'incinerator': 34742,\n",
              " 'congratulations': 10308,\n",
              " 'humbled': 52255,\n",
              " \"else's\": 5924,\n",
              " 'trelkovski': 40845,\n",
              " \"rape'\": 52256,\n",
              " \"'chapters'\": 59386,\n",
              " '1600s': 52257,\n",
              " 'martian': 7253,\n",
              " 'nicest': 25256,\n",
              " 'eyred': 52259,\n",
              " 'passenger': 9457,\n",
              " 'disgrace': 6041,\n",
              " 'moderne': 52260,\n",
              " 'barrymore': 5120,\n",
              " 'yankovich': 52261,\n",
              " 'moderns': 40931,\n",
              " 'studliest': 52262,\n",
              " 'bedsheet': 52263,\n",
              " 'decapitation': 14900,\n",
              " 'slurring': 52264,\n",
              " \"'nunsploitation'\": 52265,\n",
              " \"'character'\": 34743,\n",
              " 'cambodia': 9880,\n",
              " 'rebelious': 52266,\n",
              " 'pasadena': 27657,\n",
              " 'crowne': 40932,\n",
              " \"'bedchamber\": 52267,\n",
              " 'conjectural': 52268,\n",
              " 'appologize': 52269,\n",
              " 'halfassing': 52270,\n",
              " 'paycheque': 57816,\n",
              " 'palms': 20606,\n",
              " \"'islands\": 52271,\n",
              " 'hawked': 40933,\n",
              " 'palme': 21919,\n",
              " 'conservatively': 40934,\n",
              " 'larp': 64007,\n",
              " 'palma': 5558,\n",
              " 'smelling': 21920,\n",
              " 'aragorn': 12998,\n",
              " 'hawker': 52272,\n",
              " 'hawkes': 52273,\n",
              " 'explosions': 3975,\n",
              " 'loren': 8059,\n",
              " \"pyle's\": 52274,\n",
              " 'shootout': 6704,\n",
              " \"mike's\": 18517,\n",
              " \"driscoll's\": 52275,\n",
              " 'cogsworth': 40935,\n",
              " \"britian's\": 52276,\n",
              " 'childs': 34744,\n",
              " \"portrait's\": 52277,\n",
              " 'chain': 3626,\n",
              " 'whoever': 2497,\n",
              " 'puttered': 52278,\n",
              " 'childe': 52279,\n",
              " 'maywether': 52280,\n",
              " 'chair': 3036,\n",
              " \"rance's\": 52281,\n",
              " 'machu': 34745,\n",
              " 'ballet': 4517,\n",
              " 'grapples': 34746,\n",
              " 'summerize': 76152,\n",
              " 'freelance': 30603,\n",
              " \"andrea's\": 52283,\n",
              " '\\x91very': 52284,\n",
              " 'coolidge': 45879,\n",
              " 'mache': 18518,\n",
              " 'balled': 52285,\n",
              " 'grappled': 40937,\n",
              " 'macha': 18519,\n",
              " 'underlining': 21921,\n",
              " 'macho': 5623,\n",
              " 'oversight': 19507,\n",
              " 'machi': 25257,\n",
              " 'verbally': 11311,\n",
              " 'tenacious': 21922,\n",
              " 'windshields': 40938,\n",
              " 'paychecks': 18557,\n",
              " 'jerk': 3396,\n",
              " \"good'\": 11931,\n",
              " 'prancer': 34748,\n",
              " 'prances': 21923,\n",
              " 'olympus': 52286,\n",
              " 'lark': 21924,\n",
              " 'embark': 10785,\n",
              " 'gloomy': 7365,\n",
              " 'jehaan': 52287,\n",
              " 'turaqui': 52288,\n",
              " \"child'\": 20607,\n",
              " 'locked': 2894,\n",
              " 'pranced': 52289,\n",
              " 'exact': 2588,\n",
              " 'unattuned': 52290,\n",
              " 'minute': 783,\n",
              " 'skewed': 16118,\n",
              " 'hodgins': 40940,\n",
              " 'skewer': 34749,\n",
              " 'think\\x85': 52291,\n",
              " 'rosenstein': 38765,\n",
              " 'helmit': 52292,\n",
              " 'wrestlemanias': 34750,\n",
              " 'hindered': 16826,\n",
              " \"martha's\": 30604,\n",
              " 'cheree': 52293,\n",
              " \"pluckin'\": 52294,\n",
              " 'ogles': 40941,\n",
              " 'heavyweight': 11932,\n",
              " 'aada': 82190,\n",
              " 'chopping': 11312,\n",
              " 'strongboy': 61534,\n",
              " 'hegemonic': 41342,\n",
              " 'adorns': 40942,\n",
              " 'xxth': 41346,\n",
              " 'nobuhiro': 34751,\n",
              " 'capitães': 52298,\n",
              " 'kavogianni': 52299,\n",
              " 'antwerp': 13422,\n",
              " 'celebrated': 6538,\n",
              " 'roarke': 52300,\n",
              " 'baggins': 40943,\n",
              " 'cheeseburgers': 31270,\n",
              " 'matras': 52301,\n",
              " \"nineties'\": 52302,\n",
              " \"'craig'\": 52303,\n",
              " 'celebrates': 12999,\n",
              " 'unintentionally': 3383,\n",
              " 'drafted': 14362,\n",
              " 'climby': 52304,\n",
              " '303': 52305,\n",
              " 'oldies': 18520,\n",
              " 'climbs': 9096,\n",
              " 'honour': 9655,\n",
              " 'plucking': 34752,\n",
              " '305': 30074,\n",
              " 'address': 5514,\n",
              " 'menjou': 40944,\n",
              " \"'freak'\": 42592,\n",
              " 'dwindling': 19508,\n",
              " 'benson': 9458,\n",
              " 'white’s': 52307,\n",
              " 'shamelessness': 40945,\n",
              " 'impacted': 21925,\n",
              " 'upatz': 52308,\n",
              " 'cusack': 3840,\n",
              " \"flavia's\": 37567,\n",
              " 'effette': 52309,\n",
              " 'influx': 34753,\n",
              " 'boooooooo': 52310,\n",
              " 'dimitrova': 52311,\n",
              " 'houseman': 13423,\n",
              " 'bigas': 25259,\n",
              " 'boylen': 52312,\n",
              " 'phillipenes': 52313,\n",
              " 'fakery': 40946,\n",
              " \"grandpa's\": 27658,\n",
              " 'darnell': 27659,\n",
              " 'undergone': 19509,\n",
              " 'handbags': 52315,\n",
              " 'perished': 21926,\n",
              " 'pooped': 37778,\n",
              " 'vigour': 27660,\n",
              " 'opposed': 3627,\n",
              " 'etude': 52316,\n",
              " \"caine's\": 11799,\n",
              " 'doozers': 52317,\n",
              " 'photojournals': 34754,\n",
              " 'perishes': 52318,\n",
              " 'constrains': 34755,\n",
              " 'migenes': 40948,\n",
              " 'consoled': 30605,\n",
              " 'alastair': 16827,\n",
              " 'wvs': 52319,\n",
              " 'ooooooh': 52320,\n",
              " 'approving': 34756,\n",
              " 'consoles': 40949,\n",
              " 'disparagement': 52064,\n",
              " 'futureistic': 52322,\n",
              " 'rebounding': 52323,\n",
              " \"'date\": 52324,\n",
              " 'gregoire': 52325,\n",
              " 'rutherford': 21927,\n",
              " 'americanised': 34757,\n",
              " 'novikov': 82196,\n",
              " 'following': 1042,\n",
              " 'munroe': 34758,\n",
              " \"morita'\": 52326,\n",
              " 'christenssen': 52327,\n",
              " 'oatmeal': 23106,\n",
              " 'fossey': 25260,\n",
              " 'livered': 40950,\n",
              " 'listens': 13000,\n",
              " \"'marci\": 76164,\n",
              " \"otis's\": 52330,\n",
              " 'thanking': 23387,\n",
              " 'maude': 16019,\n",
              " 'extensions': 34759,\n",
              " 'ameteurish': 52332,\n",
              " \"commender's\": 52333,\n",
              " 'agricultural': 27661,\n",
              " 'convincingly': 4518,\n",
              " 'fueled': 17639,\n",
              " 'mahattan': 54014,\n",
              " \"paris's\": 40952,\n",
              " 'vulkan': 52336,\n",
              " 'stapes': 52337,\n",
              " 'odysessy': 52338,\n",
              " 'harmon': 12259,\n",
              " 'surfing': 4252,\n",
              " 'halloran': 23494,\n",
              " 'unbelieveably': 49580,\n",
              " \"'offed'\": 52339,\n",
              " 'quadrant': 30607,\n",
              " 'inhabiting': 19510,\n",
              " 'nebbish': 34760,\n",
              " 'forebears': 40953,\n",
              " 'skirmish': 34761,\n",
              " 'ocassionally': 52340,\n",
              " \"'resist\": 52341,\n",
              " 'impactful': 21928,\n",
              " 'spicier': 52342,\n",
              " 'touristy': 40954,\n",
              " \"'football'\": 52343,\n",
              " 'webpage': 40955,\n",
              " 'exurbia': 52345,\n",
              " 'jucier': 52346,\n",
              " 'professors': 14901,\n",
              " 'structuring': 34762,\n",
              " 'jig': 30608,\n",
              " 'overlord': 40956,\n",
              " 'disconnect': 25261,\n",
              " 'sniffle': 82201,\n",
              " 'slimeball': 40957,\n",
              " 'jia': 40958,\n",
              " 'milked': 16828,\n",
              " 'banjoes': 40959,\n",
              " 'jim': 1237,\n",
              " 'workforces': 52348,\n",
              " 'jip': 52349,\n",
              " 'rotweiller': 52350,\n",
              " 'mundaneness': 34763,\n",
              " \"'ninja'\": 52351,\n",
              " \"dead'\": 11040,\n",
              " \"cipriani's\": 40960,\n",
              " 'modestly': 20608,\n",
              " \"professor'\": 52352,\n",
              " 'shacked': 40961,\n",
              " 'bashful': 34764,\n",
              " 'sorter': 23388,\n",
              " 'overpowering': 16120,\n",
              " 'workmanlike': 18521,\n",
              " 'henpecked': 27662,\n",
              " 'sorted': 18522,\n",
              " \"jōb's\": 52354,\n",
              " \"'always\": 52355,\n",
              " \"'baptists\": 34765,\n",
              " 'dreamcatchers': 52356,\n",
              " \"'silence'\": 52357,\n",
              " 'hickory': 21929,\n",
              " 'fun\\x97yet': 52358,\n",
              " 'breakumentary': 52359,\n",
              " 'didn': 15496,\n",
              " 'didi': 52360,\n",
              " 'pealing': 52361,\n",
              " 'dispite': 40962,\n",
              " \"italy's\": 25262,\n",
              " 'instability': 21930,\n",
              " 'quarter': 6539,\n",
              " 'quartet': 12608,\n",
              " 'padmé': 52362,\n",
              " \"'bleedmedry\": 52363,\n",
              " 'pahalniuk': 52364,\n",
              " 'honduras': 52365,\n",
              " 'bursting': 10786,\n",
              " \"pablo's\": 41465,\n",
              " 'irremediably': 52367,\n",
              " 'presages': 40963,\n",
              " 'bowlegged': 57832,\n",
              " 'dalip': 65183,\n",
              " 'entering': 6260,\n",
              " 'newsradio': 76172,\n",
              " 'presaged': 54150,\n",
              " \"giallo's\": 27663,\n",
              " 'bouyant': 40964,\n",
              " 'amerterish': 52368,\n",
              " 'rajni': 18523,\n",
              " 'leeves': 30610,\n",
              " 'macauley': 34767,\n",
              " 'seriously': 612,\n",
              " 'sugercoma': 52369,\n",
              " 'grimstead': 52370,\n",
              " \"'fairy'\": 52371,\n",
              " 'zenda': 30611,\n",
              " \"'twins'\": 52372,\n",
              " 'realisation': 17640,\n",
              " 'highsmith': 27664,\n",
              " 'raunchy': 7817,\n",
              " 'incentives': 40965,\n",
              " 'flatson': 52374,\n",
              " 'snooker': 35097,\n",
              " 'crazies': 16829,\n",
              " 'crazier': 14902,\n",
              " 'grandma': 7094,\n",
              " 'napunsaktha': 52375,\n",
              " 'workmanship': 30612,\n",
              " 'reisner': 52376,\n",
              " \"sanford's\": 61306,\n",
              " '\\x91doña': 52377,\n",
              " 'modest': 6108,\n",
              " \"everything's\": 19153,\n",
              " 'hamer': 40966,\n",
              " \"couldn't'\": 52379,\n",
              " 'quibble': 13001,\n",
              " 'socking': 52380,\n",
              " 'tingler': 21931,\n",
              " 'gutman': 52381,\n",
              " 'lachlan': 40967,\n",
              " 'tableaus': 52382,\n",
              " 'headbanger': 52383,\n",
              " 'spoken': 2847,\n",
              " 'cerebrally': 34768,\n",
              " \"'road\": 23490,\n",
              " 'tableaux': 21932,\n",
              " \"proust's\": 40968,\n",
              " 'periodical': 40969,\n",
              " \"shoveller's\": 52385,\n",
              " 'tamara': 25263,\n",
              " 'affords': 17641,\n",
              " 'concert': 3249,\n",
              " \"yara's\": 87955,\n",
              " 'someome': 52386,\n",
              " 'lingering': 8424,\n",
              " \"abraham's\": 41511,\n",
              " 'beesley': 34769,\n",
              " 'cherbourg': 34770,\n",
              " 'kagan': 28624,\n",
              " 'snatch': 9097,\n",
              " \"miyazaki's\": 9260,\n",
              " 'absorbs': 25264,\n",
              " \"koltai's\": 40970,\n",
              " 'tingled': 64027,\n",
              " 'crossroads': 19511,\n",
              " 'rehab': 16121,\n",
              " 'falworth': 52389,\n",
              " 'sequals': 52390,\n",
              " ...}"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_index = keras.datasets.imdb.get_word_index()#word_indexの中身を見てる。辞書型で入っている。\n",
        "word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ggg9kkOj44c"
      },
      "outputs": [],
      "source": [
        "#モバイルデバイスやウェブブラウザにモデルをデプロイ（利用）しようとしており、毎回別の前処理関数\n",
        "#を書くようなことは避けたいなら、TensorFlow オペレーションだけを使って前処理をこなせば、\n",
        "#それをモデル自体に組み込める。その方法を見てみよう。まず、TFDS（13 章参照）を使ってテキスト形式でオリジナルのIMDbレビューをロードする （と本に記載があるが、とりあえず、Kerasではなく、TensorFlowでもできるといる理解で先に進める。)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk_CdV1lfVCw"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "#TensorFlow Datasetsライブラリに搭載されている。\n",
        "#具体的にはtensorflow_datasetsモジュール（＝tfds）のtfds.load()関数を呼び出せばよい。\n",
        "#この関数は、Web上からデータセットをロードする（＝読み込む）ためのもの。\n",
        "#https://atmarkit.itmedia.co.jp/ait/articles/2007/22/news028.html\n",
        "\n",
        "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
        "\n",
        "#with_info=Trueのオプションを付けるとdatasetsの中身がどういう形式になっているか、print(info)で確認できるようになる。\n",
        "#https://note.com/mlai/n/nbcb832d6ba48\n",
        "#as_supervised=Trueのオプションにより、各データの中身が{text: レビュー文字列, label: ラベルを表す数字}という辞書型から\n",
        "#(レビュー文字列, ラベルを表す数字)というタプル型に変更されています。\n",
        "#https://note.com/mlai/n/nfc6358e4102d#Np9XJ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0i_ANCghA8v",
        "outputId": "e52b970f-034c-4c3e-c4de-99f3487d1fce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='imdb_reviews',\n",
            "    full_name='imdb_reviews/plain_text/1.0.0',\n",
            "    description=\"\"\"\n",
            "    Large Movie Review Dataset. This is a dataset for binary sentiment\n",
            "    classification containing substantially more data than previous benchmark\n",
            "    datasets. We provide a set of 25,000 highly polar movie reviews for training,\n",
            "    and 25,000 for testing. There is additional unlabeled data for use as well.\n",
            "    \"\"\",\n",
            "    config_description=\"\"\"\n",
            "    Plain text\n",
            "    \"\"\",\n",
            "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
            "    data_path='/root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0',\n",
            "    file_format=tfrecord,\n",
            "    download_size=80.23 MiB,\n",
            "    dataset_size=129.83 MiB,\n",
            "    features=FeaturesDict({\n",
            "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
            "        'text': Text(shape=(), dtype=string),\n",
            "    }),\n",
            "    supervised_keys=('text', 'label'),\n",
            "    disable_shuffling=False,\n",
            "    splits={\n",
            "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
            "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
            "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
            "    },\n",
            "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
            "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
            "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
            "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
            "      month     = {June},\n",
            "      year      = {2011},\n",
            "      address   = {Portland, Oregon, USA},\n",
            "      publisher = {Association for Computational Linguistics},\n",
            "      pages     = {142--150},\n",
            "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
            "    }\"\"\",\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(info)\n",
        "#データセットの説明が見れる\n",
        "#大規模な映画レビューのデータセット。\n",
        "#これは、感情分類のためのデータセットで、学習用に25,000件、テスト用に25,000件の映画レビューが提供されています。また、ラベルのないデータも追加で利用できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiMhL-fNfVCw",
        "outputId": "db954016-b340-4863-82dd-e77a1ee407a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['train', 'test', 'unsupervised'])"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasets.keys()\n",
        "#trainは訓練用データ、映画のレビューが書かれた英文＋ラベル (ラベルは、0がネガティブ、1がポジティブ）\n",
        "#testは評価用データ、映画のレビューが書かれた英文＋ラベル\n",
        "#unsupervisedはラベルがない、映画のレビューが書かれた英文のみのデータ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_ytbKxJfVCw"
      },
      "outputs": [],
      "source": [
        "#train_size,test_size を確認してみる\n",
        "train_size = info.splits[\"train\"].num_examples\n",
        "test_size = info.splits[\"test\"].num_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxeZEPLJfVCw",
        "outputId": "b2ffa7cf-483c-4401-9463-084afe1bc56b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(25000, 25000)"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_size, test_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDNQ7ewNfVCw",
        "outputId": "caaa1be5-b7e1-4b96-c4bf-b980f581d395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
            "Label: 0 = Negative\n",
            "\n",
            "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
            "Label: 0 = Negative\n",
            "\n",
            "Review: Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Moun ...\n",
            "Label: 0 = Negative\n",
            "\n",
            "Review: This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful perf ...\n",
            "Label: 1 = Positive\n",
            "\n",
            "Review: As others have mentioned, all the women that go nude in this film are mostly absolutely gorgeous. The plot very ably shows the hypocrisy of the female libido. When men are around they want to be pursu ...\n",
            "Label: 1 = Positive\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for X_batch, y_batch in datasets[\"train\"].batch(5).take(1):\n",
        "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
        "        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")#先頭から２００文字まで\n",
        "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
        "        print()\n",
        "#batch(2).take(1):について\n",
        "#25000件のデータのうち、いくつ取り出すか。\n",
        "#take() メソッドを使うと、先頭の要素だけを返す Dataset オブジェクトを作れる。\n",
        "#例えば、batch(5).take(1):であれば、25000件のデータのうち、５個に分けて、分けたものの、１つを取り出す。\n",
        "#https://note.com/mlai/n/nbcb832d6ba48\n",
        "#https://blog.amedama.jp/entry/tf-dataset-api#Datasetbatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMtecNEgq838",
        "outputId": "8644a54e-2652-4d9f-a109-d6b3397f1f08"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=string, numpy=\n",
              "array([b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\",\n",
              "       b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.',\n",
              "       b'Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.',\n",
              "       b'This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.',\n",
              "       b'As others have mentioned, all the women that go nude in this film are mostly absolutely gorgeous. The plot very ably shows the hypocrisy of the female libido. When men are around they want to be pursued, but when no \"men\" are around, they become the pursuers of a 14 year old boy. And the boy becomes a man really fast (we should all be so lucky at this age!). He then gets up the courage to pursue his true love.'],\n",
              "      dtype=object)>"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_batch#X_batchの中身を見てみる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGhYUQfsmuzI",
        "outputId": "f06e84fc-55e8-48be-cb14-b97d7746fd01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([0, 0, 0, 1, 1])>"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_batch#y_batchの中身を見てみる。numpy=arrayでラベルを確認できる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZv1I2wKfVCw"
      },
      "outputs": [],
      "source": [
        "#前処理関数を作成する。\n",
        "def preprocess(X_batch, y_batch):\n",
        "    X_batch = tf.strings.substr(X_batch, 0, 300)#関数の意味　レビューの先頭３００文字だけを残し、それ以降の部分を切り捨てる。（訓練時間短縮、最初の１、２個の文を見れば大体検討がつく）\n",
        "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")#正規表現を使って、<br />タグをスペースに置き換える\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")#英字とシングルクォーテーション以外の文字をスペースに置き換える []\n",
        "    X_batch = tf.strings.split(X_batch)\n",
        "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch    #すべてのレビューにパディングトークンの\"<pad>\"を付加して同じ長さになるようにする。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laN8NAstfVCw",
        "outputId": "0971053c-e7ef-4e4a-fe6e-d51023cdb11c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(5, 59), dtype=string, numpy=\n",
              " array([[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie',\n",
              "         b\"Don't\", b'be', b'lured', b'in', b'by', b'Christopher',\n",
              "         b'Walken', b'or', b'Michael', b'Ironside', b'Both', b'are',\n",
              "         b'great', b'actors', b'but', b'this', b'must', b'simply', b'be',\n",
              "         b'their', b'worst', b'role', b'in', b'history', b'Even',\n",
              "         b'their', b'great', b'acting', b'could', b'not', b'redeem',\n",
              "         b'this', b\"movie's\", b'ridiculous', b'storyline', b'This',\n",
              "         b'movie', b'is', b'an', b'early', b'nineties', b'US',\n",
              "         b'propaganda', b'pi', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "         b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>'],\n",
              "        [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep',\n",
              "         b'during', b'films', b'but', b'this', b'is', b'usually', b'due',\n",
              "         b'to', b'a', b'combination', b'of', b'things', b'including',\n",
              "         b'really', b'tired', b'being', b'warm', b'and', b'comfortable',\n",
              "         b'on', b'the', b'sette', b'and', b'having', b'just', b'eaten',\n",
              "         b'a', b'lot', b'However', b'on', b'this', b'occasion', b'I',\n",
              "         b'fell', b'asleep', b'because', b'the', b'film', b'was',\n",
              "         b'rubbish', b'The', b'plot', b'development', b'was', b'constant',\n",
              "         b'Cons', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "         b'<pad>'],\n",
              "        [b'Mann', b'photographs', b'the', b'Alberta', b'Rocky',\n",
              "         b'Mountains', b'in', b'a', b'superb', b'fashion', b'and',\n",
              "         b'Jimmy', b'Stewart', b'and', b'Walter', b'Brennan', b'give',\n",
              "         b'enjoyable', b'performances', b'as', b'they', b'always',\n",
              "         b'seem', b'to', b'do', b'But', b'come', b'on', b'Hollywood',\n",
              "         b'a', b'Mountie', b'telling', b'the', b'people', b'of',\n",
              "         b'Dawson', b'City', b'Yukon', b'to', b'elect', b'themselves',\n",
              "         b'a', b'marshal', b'yes', b'a', b'marshal', b'and', b'to', b'e',\n",
              "         b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "         b'<pad>', b'<pad>', b'<pad>', b'<pad>'],\n",
              "        [b'This', b'is', b'the', b'kind', b'of', b'film', b'for', b'a',\n",
              "         b'snowy', b'Sunday', b'afternoon', b'when', b'the', b'rest',\n",
              "         b'of', b'the', b'world', b'can', b'go', b'ahead', b'with',\n",
              "         b'its', b'own', b'business', b'as', b'you', b'descend', b'into',\n",
              "         b'a', b'big', b'arm', b'chair', b'and', b'mellow', b'for', b'a',\n",
              "         b'couple', b'of', b'hours', b'Wonderful', b'performances',\n",
              "         b'from', b'Cher', b'and', b'Nicolas', b'Cage', b'as', b'always',\n",
              "         b'gently', b'row', b'the', b'plot', b'along', b'There', b'are',\n",
              "         b'no', b'rapids', b'to', b'cr'],\n",
              "        [b'As', b'others', b'have', b'mentioned', b'all', b'the',\n",
              "         b'women', b'that', b'go', b'nude', b'in', b'this', b'film',\n",
              "         b'are', b'mostly', b'absolutely', b'gorgeous', b'The', b'plot',\n",
              "         b'very', b'ably', b'shows', b'the', b'hypocrisy', b'of', b'the',\n",
              "         b'female', b'libido', b'When', b'men', b'are', b'around',\n",
              "         b'they', b'want', b'to', b'be', b'pursued', b'but', b'when',\n",
              "         b'no', b'men', b'are', b'around', b'they', b'become', b'the',\n",
              "         b'pursuers', b'of', b'a', b'year', b'old', b'boy', b'And',\n",
              "         b'the', b'boy', b'becomes', b'<pad>', b'<pad>', b'<pad>']],\n",
              "       dtype=object)>,\n",
              " <tf.Tensor: shape=(5,), dtype=int64, numpy=array([0, 0, 0, 1, 1])>)"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocess(X_batch, y_batch)#preprocess()関数でそれらを単語の短いシーケンスに変換する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m3bDwqWfVCw"
      },
      "outputs": [],
      "source": [
        "from collections import Counter#Counterを使用して個々の単語の出現回数を数える\n",
        "\n",
        "vocabulary = Counter()\n",
        "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
        "    for review in X_batch:\n",
        "        vocabulary.update(list(review.numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LMUxQyXFfzT",
        "outputId": "f0918f9c-d25e-41d2-f507-ee0ecdc3b741"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({b'This': 6672,\n",
              "         b'was': 14950,\n",
              "         b'an': 5183,\n",
              "         b'absolutely': 460,\n",
              "         b'terrible': 519,\n",
              "         b'movie': 14945,\n",
              "         b\"Don't\": 229,\n",
              "         b'be': 5888,\n",
              "         b'lured': 6,\n",
              "         b'in': 18966,\n",
              "         b'by': 4717,\n",
              "         b'Christopher': 99,\n",
              "         b'Walken': 30,\n",
              "         b'or': 3302,\n",
              "         b'Michael': 332,\n",
              "         b'Ironside': 6,\n",
              "         b'Both': 66,\n",
              "         b'are': 5665,\n",
              "         b'great': 2407,\n",
              "         b'actors': 1087,\n",
              "         b'but': 7724,\n",
              "         b'this': 18490,\n",
              "         b'must': 786,\n",
              "         b'simply': 394,\n",
              "         b'their': 1817,\n",
              "         b'worst': 1101,\n",
              "         b'role': 544,\n",
              "         b'history': 339,\n",
              "         b'Even': 348,\n",
              "         b'acting': 1904,\n",
              "         b'could': 1658,\n",
              "         b'not': 6325,\n",
              "         b'redeem': 13,\n",
              "         b\"movie's\": 78,\n",
              "         b'ridiculous': 231,\n",
              "         b'storyline': 248,\n",
              "         b'is': 25719,\n",
              "         b'early': 443,\n",
              "         b'nineties': 18,\n",
              "         b'US': 130,\n",
              "         b'propaganda': 55,\n",
              "         b'pi': 6,\n",
              "         b'<pad>': 214309,\n",
              "         b'I': 27019,\n",
              "         b'have': 7031,\n",
              "         b'been': 2336,\n",
              "         b'known': 281,\n",
              "         b'to': 27707,\n",
              "         b'fall': 146,\n",
              "         b'asleep': 65,\n",
              "         b'during': 426,\n",
              "         b'films': 2121,\n",
              "         b'usually': 285,\n",
              "         b'due': 207,\n",
              "         b'a': 38564,\n",
              "         b'combination': 78,\n",
              "         b'of': 33983,\n",
              "         b'things': 682,\n",
              "         b'including': 177,\n",
              "         b'really': 2924,\n",
              "         b'tired': 96,\n",
              "         b'being': 1269,\n",
              "         b'warm': 55,\n",
              "         b'and': 33431,\n",
              "         b'comfortable': 19,\n",
              "         b'on': 7817,\n",
              "         b'the': 61137,\n",
              "         b'sette': 1,\n",
              "         b'having': 472,\n",
              "         b'just': 3636,\n",
              "         b'eaten': 12,\n",
              "         b'lot': 1025,\n",
              "         b'However': 523,\n",
              "         b'occasion': 16,\n",
              "         b'fell': 112,\n",
              "         b'because': 2072,\n",
              "         b'film': 10668,\n",
              "         b'rubbish': 69,\n",
              "         b'The': 11879,\n",
              "         b'plot': 1979,\n",
              "         b'development': 113,\n",
              "         b'constant': 42,\n",
              "         b'Cons': 6,\n",
              "         b'Mann': 24,\n",
              "         b'photographs': 12,\n",
              "         b'Alberta': 1,\n",
              "         b'Rocky': 17,\n",
              "         b'Mountains': 4,\n",
              "         b'superb': 181,\n",
              "         b'fashion': 47,\n",
              "         b'Jimmy': 51,\n",
              "         b'Stewart': 107,\n",
              "         b'Walter': 65,\n",
              "         b'Brennan': 9,\n",
              "         b'give': 612,\n",
              "         b'enjoyable': 211,\n",
              "         b'performances': 353,\n",
              "         b'as': 8578,\n",
              "         b'they': 3008,\n",
              "         b'always': 753,\n",
              "         b'seem': 397,\n",
              "         b'do': 1593,\n",
              "         b'But': 1478,\n",
              "         b'come': 645,\n",
              "         b'Hollywood': 463,\n",
              "         b'Mountie': 2,\n",
              "         b'telling': 118,\n",
              "         b'people': 2086,\n",
              "         b'Dawson': 20,\n",
              "         b'City': 120,\n",
              "         b'Yukon': 6,\n",
              "         b'elect': 2,\n",
              "         b'themselves': 171,\n",
              "         b'marshal': 4,\n",
              "         b'yes': 165,\n",
              "         b'e': 206,\n",
              "         b'kind': 586,\n",
              "         b'for': 8660,\n",
              "         b'snowy': 4,\n",
              "         b'Sunday': 60,\n",
              "         b'afternoon': 60,\n",
              "         b'when': 2812,\n",
              "         b'rest': 245,\n",
              "         b'world': 649,\n",
              "         b'can': 2337,\n",
              "         b'go': 991,\n",
              "         b'ahead': 82,\n",
              "         b'with': 9073,\n",
              "         b'its': 1701,\n",
              "         b'own': 547,\n",
              "         b'business': 123,\n",
              "         b'you': 5500,\n",
              "         b'descend': 2,\n",
              "         b'into': 1794,\n",
              "         b'big': 804,\n",
              "         b'arm': 22,\n",
              "         b'chair': 34,\n",
              "         b'mellow': 5,\n",
              "         b'couple': 343,\n",
              "         b'hours': 232,\n",
              "         b'Wonderful': 34,\n",
              "         b'from': 4321,\n",
              "         b'Cher': 17,\n",
              "         b'Nicolas': 27,\n",
              "         b'Cage': 66,\n",
              "         b'gently': 7,\n",
              "         b'row': 31,\n",
              "         b'along': 323,\n",
              "         b'There': 1230,\n",
              "         b'no': 2305,\n",
              "         b'rapids': 1,\n",
              "         b'cr': 15,\n",
              "         b'As': 850,\n",
              "         b'others': 310,\n",
              "         b'mentioned': 87,\n",
              "         b'all': 4655,\n",
              "         b'women': 355,\n",
              "         b'that': 14752,\n",
              "         b'nude': 29,\n",
              "         b'mostly': 182,\n",
              "         b'gorgeous': 86,\n",
              "         b'very': 3571,\n",
              "         b'ably': 4,\n",
              "         b'shows': 535,\n",
              "         b'hypocrisy': 5,\n",
              "         b'female': 168,\n",
              "         b'libido': 6,\n",
              "         b'When': 804,\n",
              "         b'men': 284,\n",
              "         b'around': 730,\n",
              "         b'want': 676,\n",
              "         b'pursued': 9,\n",
              "         b'become': 289,\n",
              "         b'pursuers': 1,\n",
              "         b'year': 733,\n",
              "         b'old': 1096,\n",
              "         b'boy': 302,\n",
              "         b'And': 1063,\n",
              "         b'becomes': 200,\n",
              "         b'which': 2178,\n",
              "         b'should': 1022,\n",
              "         b'seen': 2562,\n",
              "         b'anybody': 52,\n",
              "         b'interested': 138,\n",
              "         b'effected': 2,\n",
              "         b'suffering': 30,\n",
              "         b'eating': 43,\n",
              "         b'disorder': 14,\n",
              "         b'It': 4038,\n",
              "         b'amazingly': 40,\n",
              "         b'accurate': 90,\n",
              "         b'sensitive': 35,\n",
              "         b'portrayal': 120,\n",
              "         b'bulimia': 3,\n",
              "         b'teenage': 84,\n",
              "         b'girl': 540,\n",
              "         b'causes': 28,\n",
              "         b'symptoms': 1,\n",
              "         b'played': 504,\n",
              "         b'one': 6475,\n",
              "         b'most': 2038,\n",
              "         b'brilliant': 311,\n",
              "         b'young': 846,\n",
              "         b'actresses': 86,\n",
              "         b'working': 175,\n",
              "         b'c': 169,\n",
              "         b'Okay': 137,\n",
              "         b'Penelope': 4,\n",
              "         b'Keith': 17,\n",
              "         b'Miss': 52,\n",
              "         b'Herringbone': 1,\n",
              "         b'Tweed': 1,\n",
              "         b'B': 327,\n",
              "         b'E': 56,\n",
              "         b'Backbone': 2,\n",
              "         b'England': 79,\n",
              "         b\"She's\": 56,\n",
              "         b'killed': 170,\n",
              "         b'off': 1249,\n",
              "         b'first': 2791,\n",
              "         b'scene': 683,\n",
              "         b\"that's\": 479,\n",
              "         b'right': 645,\n",
              "         b'folks': 78,\n",
              "         b'show': 1561,\n",
              "         b'has': 4178,\n",
              "         b'backbone': 3,\n",
              "         b'Peter': 223,\n",
              "         b\"O'Toole\": 9,\n",
              "         b\"Ol'\": 1,\n",
              "         b'Colonel': 9,\n",
              "         b'Cricket': 1,\n",
              "         b'First': 541,\n",
              "         b'War': 203,\n",
              "         b'now': 815,\n",
              "         b'emblazered': 1,\n",
              "         b'Lord': 53,\n",
              "         b'Manor': 2,\n",
              "         b'based': 493,\n",
              "         b'genuine': 44,\n",
              "         b's': 1219,\n",
              "         b'novel': 359,\n",
              "         b'Journalist': 1,\n",
              "         b'Colin': 21,\n",
              "         b'McInnes': 2,\n",
              "         b'wrote': 198,\n",
              "         b'set': 584,\n",
              "         b'three': 505,\n",
              "         b'London': 114,\n",
              "         b'novels': 50,\n",
              "         b'Absolute': 10,\n",
              "         b'Beginners': 3,\n",
              "         b'Spades': 1,\n",
              "         b'Mr': 272,\n",
              "         b'Love': 116,\n",
              "         b'Justice': 18,\n",
              "         b'read': 674,\n",
              "         b'two': 1522,\n",
              "         b'excellent': 581,\n",
              "         b'last': 744,\n",
              "         b'perhaps': 243,\n",
              "         b'experiment': 33,\n",
              "         b'did': 1349,\n",
              "         b'McInn': 1,\n",
              "         b'love': 1539,\n",
              "         b'sexy': 106,\n",
              "         b'action': 762,\n",
              "         b'sci': 152,\n",
              "         b'fi': 226,\n",
              "         b'sixties': 30,\n",
              "         b\"actress's\": 1,\n",
              "         b'appeared': 91,\n",
              "         b'them': 1258,\n",
              "         b'They': 610,\n",
              "         b'found': 791,\n",
              "         b'sexiest': 7,\n",
              "         b'these': 775,\n",
              "         b'it': 17690,\n",
              "         b\"didn't\": 1056,\n",
              "         b'matter': 215,\n",
              "         b'if': 1978,\n",
              "         b'act': 240,\n",
              "         b'Remember': 36,\n",
              "         b'Candy': 22,\n",
              "         b'reason': 543,\n",
              "         b'disappointed': 325,\n",
              "         b\"wasn't\": 638,\n",
              "         b'nostalg': 1,\n",
              "         b'Sure': 130,\n",
              "         b\"isn't\": 617,\n",
              "         b'blockbuster': 36,\n",
              "         b'nor': 124,\n",
              "         b'does': 962,\n",
              "         b'target': 46,\n",
              "         b'such': 1060,\n",
              "         b'position': 28,\n",
              "         b'Dieter': 4,\n",
              "         b'name': 375,\n",
              "         b'quite': 870,\n",
              "         b'popular': 167,\n",
              "         b'German': 124,\n",
              "         b'musician': 30,\n",
              "         b'who': 4266,\n",
              "         b'either': 329,\n",
              "         b'loved': 547,\n",
              "         b'hated': 82,\n",
              "         b'his': 4625,\n",
              "         b'thats': 77,\n",
              "         b'exactly': 214,\n",
              "         b'what': 2805,\n",
              "         b'about': 4799,\n",
              "         b'autobiography': 13,\n",
              "         b'Bohlen': 3,\n",
              "         b'few': 981,\n",
              "         b'y': 45,\n",
              "         b'During': 58,\n",
              "         b'sleepless': 4,\n",
              "         b'night': 506,\n",
              "         b'switching': 6,\n",
              "         b'through': 1089,\n",
              "         b'channels': 44,\n",
              "         b'embarrassment': 19,\n",
              "         b'What': 939,\n",
              "         b'were': 2481,\n",
              "         b'thinking': 312,\n",
              "         b'If': 1282,\n",
              "         b'life': 1402,\n",
              "         b'after': 1479,\n",
              "         b'Remote': 1,\n",
              "         b'Control': 4,\n",
              "         b'Kari': 6,\n",
              "         b'Wuhrer': 3,\n",
              "         b'Salin': 1,\n",
              "         b'wonder': 204,\n",
              "         b\"she's\": 137,\n",
              "         b'gone': 141,\n",
              "         b'nowhere': 80,\n",
              "         b'why': 870,\n",
              "         b'David': 264,\n",
              "         b'take': 645,\n",
              "         b\"It's\": 1699,\n",
              "         b'pathetic': 128,\n",
              "         b'br': 223,\n",
              "         b'Cute': 7,\n",
              "         b'lively': 17,\n",
              "         b'sisters': 46,\n",
              "         b'Switzerland': 8,\n",
              "         b'often': 344,\n",
              "         b'running': 168,\n",
              "         b'matching': 5,\n",
              "         b'outfits': 18,\n",
              "         b'get': 1827,\n",
              "         b'parents': 162,\n",
              "         b'back': 1055,\n",
              "         b'together': 464,\n",
              "         b'seems': 668,\n",
              "         b'mom': 70,\n",
              "         b'still': 1112,\n",
              "         b'carrying': 28,\n",
              "         b'torch': 2,\n",
              "         b'dad': 68,\n",
              "         b'so': 4521,\n",
              "         b'sail': 1,\n",
              "         b'New': 369,\n",
              "         b'York': 232,\n",
              "         b'stop': 216,\n",
              "         b'marrying': 5,\n",
              "         b'blonde': 43,\n",
              "         b'gold': 54,\n",
              "         b'digger': 7,\n",
              "         b'he': 3096,\n",
              "         b'calls': 45,\n",
              "         b'Precious': 1,\n",
              "         b'Dad': 16,\n",
              "         b'version': 571,\n",
              "         b\"Dickens'\": 10,\n",
              "         b'classic': 580,\n",
              "         b'A': 2257,\n",
              "         b'Christmas': 184,\n",
              "         b'Carol': 28,\n",
              "         b\"'\": 393,\n",
              "         b'directed': 401,\n",
              "         b'Clive': 13,\n",
              "         b'Donner': 6,\n",
              "         b'stars': 469,\n",
              "         b'George': 251,\n",
              "         b'C': 114,\n",
              "         b'Scott': 145,\n",
              "         b'Ebenezer': 6,\n",
              "         b'Scrooge': 20,\n",
              "         b'By': 148,\n",
              "         b'time': 3126,\n",
              "         b'challenge': 35,\n",
              "         b'filmmaker': 93,\n",
              "         b'familiar': 130,\n",
              "         b'material': 142,\n",
              "         b'make': 1523,\n",
              "         b'fresh': 87,\n",
              "         b'new': 742,\n",
              "         b'again': 772,\n",
              "         b'happily': 31,\n",
              "         b'say': 1569,\n",
              "         b'Put': 19,\n",
              "         b'blame': 44,\n",
              "         b'executive': 20,\n",
              "         b'producer': 93,\n",
              "         b'Wes': 32,\n",
              "         b'Craven': 21,\n",
              "         b'financiers': 2,\n",
              "         b'Weinsteins': 1,\n",
              "         b'budget': 545,\n",
              "         b'debacle': 4,\n",
              "         b'thrash': 2,\n",
              "         b'metal': 36,\n",
              "         b'updating': 3,\n",
              "         b'Dracula': 32,\n",
              "         b'condescending': 8,\n",
              "         b'verbal': 6,\n",
              "         b'jab': 2,\n",
              "         b'at': 4814,\n",
              "         b'Bram': 2,\n",
              "         b'Stoker': 1,\n",
              "         b'probably': 616,\n",
              "         b\"wouldn't\": 182,\n",
              "         b'thing': 956,\n",
              "         b'anyway': 106,\n",
              "         b'nothing': 796,\n",
              "         b'much': 2080,\n",
              "         b'us': 575,\n",
              "         b'except': 210,\n",
              "         b'slasher': 145,\n",
              "         b'Hilarious': 9,\n",
              "         b'evocative': 3,\n",
              "         b'confusing': 77,\n",
              "         b'Reminds': 4,\n",
              "         b'me': 2923,\n",
              "         b\"Bunuel's\": 3,\n",
              "         b\"L'Age\": 1,\n",
              "         b\"D'Or\": 2,\n",
              "         b\"Jodorowsky's\": 1,\n",
              "         b'Holy': 13,\n",
              "         b'Mountain': 13,\n",
              "         b'lots': 154,\n",
              "         b'strange': 222,\n",
              "         b'characters': 1371,\n",
              "         b'mucking': 1,\n",
              "         b'looking': 533,\n",
              "         b'laughed': 146,\n",
              "         b'almost': 632,\n",
              "         b'whole': 621,\n",
              "         b'way': 1517,\n",
              "         b'while': 733,\n",
              "         b'keeping': 31,\n",
              "         b'peripheral': 4,\n",
              "         b'eye': 135,\n",
              "         b'bewildered': 6,\n",
              "         b'occasional': 34,\n",
              "         b'disgusting': 64,\n",
              "         b'painful': 93,\n",
              "         b'waste': 333,\n",
              "         b'cast': 894,\n",
              "         b'swear': 24,\n",
              "         b'audience': 348,\n",
              "         b'full': 367,\n",
              "         b'TWICE': 4,\n",
              "         b'minutes': 712,\n",
              "         b'lie': 27,\n",
              "         b'Do': 97,\n",
              "         b'even': 2158,\n",
              "         b'rent': 151,\n",
              "         b'Zeta': 11,\n",
              "         b'Jones': 93,\n",
              "         b'too': 1352,\n",
              "         b'mean': 405,\n",
              "         b'believable': 125,\n",
              "         b'Cusack': 30,\n",
              "         b'OK': 316,\n",
              "         b'Just': 297,\n",
              "         b'felt': 380,\n",
              "         b'sorry': 133,\n",
              "         b'him': 952,\n",
              "         b'actor': 512,\n",
              "         b'case': 337,\n",
              "         b're': 234,\n",
              "         b'straight': 188,\n",
              "         b'video': 526,\n",
              "         b'without': 603,\n",
              "         b'saying': 241,\n",
              "         b\"it's\": 2476,\n",
              "         b'going': 938,\n",
              "         b'rival': 29,\n",
              "         b'Lion': 47,\n",
              "         b'King': 192,\n",
              "         b'said': 485,\n",
              "         b'downright': 38,\n",
              "         b'good': 3727,\n",
              "         b'My': 668,\n",
              "         b'kids': 378,\n",
              "         b'given': 329,\n",
              "         b'anything': 621,\n",
              "         b'cartoon': 163,\n",
              "         b'shock': 57,\n",
              "         b'liked': 503,\n",
              "         b'laugh': 324,\n",
              "         b'ou': 18,\n",
              "         b'Finally': 38,\n",
              "         b'Timon': 15,\n",
              "         b'Pumbaa': 10,\n",
              "         b\"'The\": 150,\n",
              "         b'Hakuna': 2,\n",
              "         b\"Matata'\": 1,\n",
              "         b'irreverent': 7,\n",
              "         b'tale': 250,\n",
              "         b'Which': 58,\n",
              "         b'ask': 106,\n",
              "         b'Why': 293,\n",
              "         b\"King'\": 1,\n",
              "         b'course': 405,\n",
              "         b'Yep': 13,\n",
              "         b\"there's\": 255,\n",
              "         b'Disney': 225,\n",
              "         b'never': 1416,\n",
              "         b'short': 489,\n",
              "         b'narcissism': 1,\n",
              "         b'Indian': 100,\n",
              "         b'Directors': 11,\n",
              "         b'tough': 102,\n",
              "         b'compete': 16,\n",
              "         b'movies': 2754,\n",
              "         b'like': 4428,\n",
              "         b'Laggan': 1,\n",
              "         b'where': 1117,\n",
              "         b'henpecked': 2,\n",
              "         b'Castrated': 1,\n",
              "         b'males': 15,\n",
              "         b'defend': 21,\n",
              "         b'village': 63,\n",
              "         b'half': 454,\n",
              "         b'certifiable': 2,\n",
              "         b'idiots': 22,\n",
              "         b'Devdas': 1,\n",
              "         b'hapless': 16,\n",
              "         b'fedar': 1,\n",
              "         b'festooned': 1,\n",
              "         b'foreign': 52,\n",
              "         b'return': 73,\n",
              "         b'drinking': 37,\n",
              "         b'oblivion': 10,\n",
              "         b'endless': 50,\n",
              "         b'corridors': 6,\n",
              "         b'obli': 1,\n",
              "         b'Nathan': 13,\n",
              "         b'Detroit': 20,\n",
              "         b'runs': 85,\n",
              "         b'illegal': 20,\n",
              "         b'craps': 6,\n",
              "         b'games': 72,\n",
              "         b'high': 543,\n",
              "         b'rollers': 1,\n",
              "         b'NYC': 41,\n",
              "         b'heat': 19,\n",
              "         b\"can't\": 843,\n",
              "         b'find': 851,\n",
              "         b'secure': 10,\n",
              "         b'location': 70,\n",
              "         b'He': 853,\n",
              "         b'bets': 6,\n",
              "         b'chronic': 2,\n",
              "         b'gambler': 9,\n",
              "         b'Sky': 58,\n",
              "         b'Masterson': 16,\n",
              "         b'prim': 4,\n",
              "         b'missionary': 5,\n",
              "         b'Sarah': 44,\n",
              "         b'Brown': 44,\n",
              "         b'out': 3376,\n",
              "         b'dinner': 14,\n",
              "         b'takes': 496,\n",
              "         b'up': 2508,\n",
              "         b'both': 575,\n",
              "         b'some': 3197,\n",
              "         b'surprises': 50,\n",
              "         b'i': 2330,\n",
              "         b'remember': 588,\n",
              "         b'seeing': 604,\n",
              "         b'TV': 880,\n",
              "         b\"couldn't\": 429,\n",
              "         b'believe': 633,\n",
              "         b'TVNZ': 2,\n",
              "         b'let': 304,\n",
              "         b'had': 3121,\n",
              "         b'humor': 329,\n",
              "         b'will': 1573,\n",
              "         b'lost': 290,\n",
              "         b'non': 190,\n",
              "         b\"NZ'ers\": 1,\n",
              "         b'Since': 107,\n",
              "         b'finishing': 8,\n",
              "         b'Back': 63,\n",
              "         b'Y': 13,\n",
              "         b'series': 1033,\n",
              "         b'Matt': 66,\n",
              "         b'Chris': 113,\n",
              "         b'bigger': 33,\n",
              "         b'better': 1169,\n",
              "         b\"NZ's\": 1,\n",
              "         b'greatest': 247,\n",
              "         b'dare': 28,\n",
              "         b'devil': 19,\n",
              "         b'In': 1423,\n",
              "         b'magic': 88,\n",
              "         b'barbarians': 5,\n",
              "         b'demons': 19,\n",
              "         b'abound': 10,\n",
              "         b'diabolical': 6,\n",
              "         b'tyrant': 2,\n",
              "         b'named': 191,\n",
              "         b'Nekhron': 1,\n",
              "         b'mother': 233,\n",
              "         b'Queen': 44,\n",
              "         b'Juliane': 1,\n",
              "         b'lives': 285,\n",
              "         b'realm': 19,\n",
              "         b'ice': 30,\n",
              "         b'wants': 229,\n",
              "         b'conquer': 5,\n",
              "         b'region': 13,\n",
              "         b'fire': 70,\n",
              "         b'ruled': 6,\n",
              "         b'Jerol': 1,\n",
              "         b'beautiful': 499,\n",
              "         b'daughter': 185,\n",
              "         b'Princess': 26,\n",
              "         b'Teegra': 1,\n",
              "         b'kidnapped': 33,\n",
              "         b\"Nekhron's\": 1,\n",
              "         b'goons': 2,\n",
              "         b'warri': 1,\n",
              "         b'pretty': 884,\n",
              "         b'memorable': 99,\n",
              "         b'animals': 91,\n",
              "         b'killing': 107,\n",
              "         b'variety': 41,\n",
              "         b'specifically': 17,\n",
              "         b'similar': 178,\n",
              "         b'Willard': 2,\n",
              "         b'aging': 34,\n",
              "         b'character': 1061,\n",
              "         b'step': 65,\n",
              "         b'down': 684,\n",
              "         b'bit': 612,\n",
              "         b'level': 153,\n",
              "         b'Les': 14,\n",
              "         b'Tremayne': 1,\n",
              "         b'puts': 74,\n",
              "         b'only': 2507,\n",
              "         b'distinguished': 8,\n",
              "         b'performance': 568,\n",
              "         b\"I've\": 1337,\n",
              "         b'man': 950,\n",
              "         b'whose': 172,\n",
              "         b'Except': 28,\n",
              "         b'awkward': 35,\n",
              "         b'refreshing': 49,\n",
              "         b'fairy': 50,\n",
              "         b'fantasy': 153,\n",
              "         b'fun': 651,\n",
              "         b'delightful': 79,\n",
              "         b'undercurrent': 3,\n",
              "         b'adult': 102,\n",
              "         b'cynical': 31,\n",
              "         b'wit': 87,\n",
              "         b'charms': 11,\n",
              "         b'well': 1803,\n",
              "         b'soundtrack': 128,\n",
              "         b'powerfully': 6,\n",
              "         b'moves': 111,\n",
              "         b'epic': 85,\n",
              "         b'Robert': 275,\n",
              "         b'DeNiro': 17,\n",
              "         b'scenes': 847,\n",
              "         b\"doesn't\": 750,\n",
              "         b'across': 221,\n",
              "         b'sm': 5,\n",
              "         b'we': 1451,\n",
              "         b'fabulous': 49,\n",
              "         b'opportunity': 84,\n",
              "         b'see': 2620,\n",
              "         b'happened': 217,\n",
              "         b'shown': 202,\n",
              "         b'goes': 448,\n",
              "         b'before': 904,\n",
              "         b'Simba': 5,\n",
              "         b'presumbably': 1,\n",
              "         b'birth': 44,\n",
              "         b'Kiara': 1,\n",
              "         b'Quite': 38,\n",
              "         b'true': 519,\n",
              "         b'Kin': 1,\n",
              "         b'Well': 573,\n",
              "         b'rented': 224,\n",
              "         b'realllllllly': 1,\n",
              "         b'sucks': 83,\n",
              "         b'family': 699,\n",
              "         b'stepmother': 11,\n",
              "         b'same': 829,\n",
              "         b'stupid': 395,\n",
              "         b'fights': 50,\n",
              "         b'then': 1274,\n",
              "         b'cool': 218,\n",
              "         b'son': 265,\n",
              "         b'comes': 445,\n",
              "         b'camera': 301,\n",
              "         b'likes': 98,\n",
              "         b'photo': 13,\n",
              "         b'damaged': 11,\n",
              "         b'building': 65,\n",
              "         b'weird': 160,\n",
              "         b'asks': 45,\n",
              "         b'f': 184,\n",
              "         b'actually': 898,\n",
              "         b'hopes': 98,\n",
              "         b'took': 282,\n",
              "         b'grain': 6,\n",
              "         b'salt': 11,\n",
              "         b'hoped': 36,\n",
              "         b'best': 1756,\n",
              "         b'About': 63,\n",
              "         b'mish': 12,\n",
              "         b'mash': 12,\n",
              "         b'stuff': 213,\n",
              "         b'happening': 59,\n",
              "         b'apparent': 49,\n",
              "         b'disconnected': 3,\n",
              "         b'lov': 8,\n",
              "         b'initial': 52,\n",
              "         b'premise': 278,\n",
              "         b'led': 45,\n",
              "         b'hunt': 26,\n",
              "         b'problem': 299,\n",
              "         b'quickly': 119,\n",
              "         b'knows': 146,\n",
              "         b\"what's\": 86,\n",
              "         b'happen': 197,\n",
              "         b'within': 125,\n",
              "         b'doubles': 3,\n",
              "         b'behind': 207,\n",
              "         b'mirror': 12,\n",
              "         b'over': 1295,\n",
              "         b'everybody': 57,\n",
              "         b'real': 901,\n",
              "         b'twist': 89,\n",
              "         b'main': 491,\n",
              "         b'long': 802,\n",
              "         b'Other': 59,\n",
              "         b'Paul': 196,\n",
              "         b'Muni': 11,\n",
              "         b'plays': 560,\n",
              "         b'poor': 393,\n",
              "         b'Chinese': 85,\n",
              "         b'farmer': 13,\n",
              "         b'married': 107,\n",
              "         b'arranged': 8,\n",
              "         b'marriage': 72,\n",
              "         b'Luise': 6,\n",
              "         b'Rainer': 9,\n",
              "         b'servant': 10,\n",
              "         b'gets': 567,\n",
              "         b'live': 365,\n",
              "         b\"Muni's\": 2,\n",
              "         b'father': 324,\n",
              "         b'farm': 22,\n",
              "         b'd': 165,\n",
              "         b'made': 2422,\n",
              "         b'theaters': 87,\n",
              "         b'our': 458,\n",
              "         b'area': 64,\n",
              "         b'became': 166,\n",
              "         b'available': 84,\n",
              "         b'DVD': 697,\n",
              "         b'For': 532,\n",
              "         b'once': 386,\n",
              "         b'listened': 12,\n",
              "         b'critics': 128,\n",
              "         b'passed': 61,\n",
              "         b'Despite': 117,\n",
              "         b'line': 416,\n",
              "         b'disappointing': 88,\n",
              "         b'went': 477,\n",
              "         b'st': 128,\n",
              "         b'Mike': 69,\n",
              "         b'Brady': 24,\n",
              "         b'Garfield': 9,\n",
              "         b'minuscule': 2,\n",
              "         b'part': 766,\n",
              "         b'Warriors': 14,\n",
              "         b'person': 292,\n",
              "         b'community': 51,\n",
              "         b'realize': 149,\n",
              "         b'murderous': 15,\n",
              "         b'slugs': 12,\n",
              "         b'small': 400,\n",
              "         b'town': 294,\n",
              "         b'Not': 550,\n",
              "         b'any': 1403,\n",
              "         b'mind': 399,\n",
              "         b'carnivorous': 1,\n",
              "         b'killer': 239,\n",
              "         b'normal': 81,\n",
              "         b'mutated': 12,\n",
              "         b'toxic': 7,\n",
              "         b'g': 118,\n",
              "         b'Honestly': 31,\n",
              "         b'Barbra': 22,\n",
              "         b'know': 1487,\n",
              "         b\"who's\": 109,\n",
              "         b'klicking': 1,\n",
              "         b'those': 1035,\n",
              "         b'NO': 38,\n",
              "         b'my': 3662,\n",
              "         b'review': 341,\n",
              "         b'times': 792,\n",
              "         b'How': 359,\n",
              "         b'many': 1630,\n",
              "         b'instruct': 1,\n",
              "         b'help': 271,\n",
              "         b'here': 919,\n",
              "         b'look': 729,\n",
              "         b'yourself': 118,\n",
              "         b'day': 552,\n",
              "         b'Steven': 89,\n",
              "         b'Spielberg': 32,\n",
              "         b'told': 238,\n",
              "         b'bes': 5,\n",
              "         b'Confidential': 6,\n",
              "         b'meant': 117,\n",
              "         b'piggy': 1,\n",
              "         b'appeal': 93,\n",
              "         b'lurid': 8,\n",
              "         b'magazine': 34,\n",
              "         b'labor': 10,\n",
              "         b'racketeering': 1,\n",
              "         b'theme': 179,\n",
              "         b'tied': 32,\n",
              "         b'headline': 5,\n",
              "         b'Congressional': 1,\n",
              "         b'investigations': 4,\n",
              "         b'despite': 159,\n",
              "         b'grade': 87,\n",
              "         b'script': 753,\n",
              "         b'ideas': 129,\n",
              "         b'plods': 7,\n",
              "         b'thirteen': 5,\n",
              "         b'years': 1484,\n",
              "         b'saw': 1676,\n",
              "         b'expected': 243,\n",
              "         b'Escape': 9,\n",
              "         b'From': 270,\n",
              "         b'rated': 142,\n",
              "         b'Germany': 51,\n",
              "         b'entered': 16,\n",
              "         b'fallback': 1,\n",
              "         b'boring': 501,\n",
              "         b'Afterwards': 5,\n",
              "         b'realized': 83,\n",
              "         b'crap': 277,\n",
              "         b'husband': 192,\n",
              "         b'exhibits': 8,\n",
              "         b'wife': 402,\n",
              "         b'today': 226,\n",
              "         b'via': 38,\n",
              "         b'internet': 41,\n",
              "         b\"I'm\": 1364,\n",
              "         b'Unfortunately': 228,\n",
              "         b'thin': 79,\n",
              "         b'weak': 180,\n",
              "         b'hold': 103,\n",
              "         b'attention': 184,\n",
              "         b'indeed': 124,\n",
              "         b'older': 134,\n",
              "         b'guy': 486,\n",
              "         b'reliving': 3,\n",
              "         b'dream': 87,\n",
              "         b'cheated': 13,\n",
              "         b'Surely': 18,\n",
              "         b'device': 20,\n",
              "         b'bring': 169,\n",
              "         b'something': 979,\n",
              "         b'deeper': 35,\n",
              "         b'somethi': 2,\n",
              "         b'contemporary': 38,\n",
              "         b'masses': 15,\n",
              "         b'Action': 31,\n",
              "         b'comedy': 993,\n",
              "         b'drama': 415,\n",
              "         b'sensuous': 2,\n",
              "         b'would': 2451,\n",
              "         b'feel': 600,\n",
              "         b'watching': 1437,\n",
              "         b'entire': 309,\n",
              "         b'eyes': 205,\n",
              "         b'children': 264,\n",
              "         b'fast': 168,\n",
              "         b'forward': 174,\n",
              "         b'numb': 6,\n",
              "         b'Of': 293,\n",
              "         b'remakes': 25,\n",
              "         b'VHS': 99,\n",
              "         b'addition': 63,\n",
              "         b'copy': 140,\n",
              "         b'enough': 631,\n",
              "         b'variation': 14,\n",
              "         b'scripts': 31,\n",
              "         b'entertaining': 418,\n",
              "         b'watchable': 74,\n",
              "         b'other': 1709,\n",
              "         b'remade': 19,\n",
              "         b'star': 403,\n",
              "         b'casts': 17,\n",
              "         b'Wild': 38,\n",
              "         b'Tigers': 2,\n",
              "         b'Known': 3,\n",
              "         b'showing': 142,\n",
              "         b'cities': 20,\n",
              "         b'sure': 479,\n",
              "         b'SO': 42,\n",
              "         b'artsy': 14,\n",
              "         b'makes': 813,\n",
              "         b'sense': 418,\n",
              "         b'ever': 2105,\n",
              "         b'director': 931,\n",
              "         b'HATE': 5,\n",
              "         b'oh': 116,\n",
              "         b'alternative': 16,\n",
              "         b'try': 330,\n",
              "         b'DESPERATELY': 1,\n",
              "         b'metaphors': 4,\n",
              "         b'EVIDENT': 1,\n",
              "         b'horseshi': 1,\n",
              "         b'surprisingly': 112,\n",
              "         b'original': 933,\n",
              "         b'sequel': 260,\n",
              "         b'nearly': 177,\n",
              "         b'instead': 216,\n",
              "         b'spends': 31,\n",
              "         b'Tim': 92,\n",
              "         b'Thomerson': 3,\n",
              "         b'wisecracking': 3,\n",
              "         b'toned': 13,\n",
              "         b'entry': 50,\n",
              "         b\"'s\": 790,\n",
              "         b'think': 1746,\n",
              "         b'sadly': 70,\n",
              "         b'overlooked': 30,\n",
              "         b'unfortunately': 129,\n",
              "         b'One': 598,\n",
              "         b'Clownhouse': 1,\n",
              "         b'Victor': 48,\n",
              "         b'Salva': 3,\n",
              "         b'horribly': 64,\n",
              "         b'overlook': 7,\n",
              "         b\"Salva's\": 1,\n",
              "         b'legal': 15,\n",
              "         b'sexual': 128,\n",
              "         b'problems': 166,\n",
              "         b'Another': 122,\n",
              "         b\"Cameron's\": 2,\n",
              "         b'Closet': 4,\n",
              "         b'strikes': 24,\n",
              "         b'yet': 502,\n",
              "         b'negative': 93,\n",
              "         b'professional': 66,\n",
              "         b'guess': 270,\n",
              "         b'missed': 131,\n",
              "         b'beginning': 368,\n",
              "         b'intriguing': 82,\n",
              "         b'meet': 121,\n",
              "         b'late': 375,\n",
              "         b'otherwise': 69,\n",
              "         b'empty': 57,\n",
              "         b'bar': 75,\n",
              "         b'entertain': 49,\n",
              "         b'each': 414,\n",
              "         b'invented': 17,\n",
              "         b'stories': 296,\n",
              "         b\"That's\": 184,\n",
              "         b'After': 533,\n",
              "         b'se': 53,\n",
              "         b'Although': 357,\n",
              "         b'likeliness': 1,\n",
              "         b'someone': 419,\n",
              "         b'focusing': 19,\n",
              "         b'THIS': 66,\n",
              "         b'comment': 261,\n",
              "         b'among': 189,\n",
              "         b'low': 550,\n",
              "         b'am': 1094,\n",
              "         b'watcher': 13,\n",
              "         b'pays': 21,\n",
              "         b'production': 454,\n",
              "         ...})"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocabulary#各単語の出現回数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOxW8o3qfVCw",
        "outputId": "0a365bf1-d179-4ef2-87e5-8bc1feace9ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocabulary.most_common()[:3]#最頻出の３つの語を見てみる→<pad>、the、aということが分かる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAgzGiGPfVCw",
        "outputId": "9d521bae-642c-439d-b768-180a6b9e96f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "53893"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocabulary)#何単語あるか"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLcIMFFafVCw"
      },
      "outputs": [],
      "source": [
        "#辞書に含まれる単語を全て覚えなくてもまともな能力を発揮できるはず。語彙のうち、最頻出１万語だけを残して、その他は捨てる\n",
        "vocab_size = 10000\n",
        "truncated_vocabulary = [\n",
        "    word for word, count in vocabulary.most_common()[:vocab_size]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMPUXIfj6Ye_",
        "outputId": "1ebea668-4b55-4ecf-a915-f0db339dfe91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[b'<pad>',\n",
              " b'the',\n",
              " b'a',\n",
              " b'of',\n",
              " b'and',\n",
              " b'to',\n",
              " b'I',\n",
              " b'is',\n",
              " b'in',\n",
              " b'this',\n",
              " b'it',\n",
              " b'was',\n",
              " b'movie',\n",
              " b'that',\n",
              " b'The',\n",
              " b'film',\n",
              " b'with',\n",
              " b'for',\n",
              " b'as',\n",
              " b'on',\n",
              " b'but',\n",
              " b'have',\n",
              " b'This',\n",
              " b'one',\n",
              " b'not',\n",
              " b'be',\n",
              " b'are',\n",
              " b'you',\n",
              " b'an',\n",
              " b'at',\n",
              " b'about',\n",
              " b'by',\n",
              " b'all',\n",
              " b'his',\n",
              " b'so',\n",
              " b'like',\n",
              " b'from',\n",
              " b'who',\n",
              " b'has',\n",
              " b'It',\n",
              " b'good',\n",
              " b'my',\n",
              " b'just',\n",
              " b'very',\n",
              " b'out',\n",
              " b'or',\n",
              " b'story',\n",
              " b'some',\n",
              " b'time',\n",
              " b'had',\n",
              " b'he',\n",
              " b'they',\n",
              " b'really',\n",
              " b'me',\n",
              " b'when',\n",
              " b'what',\n",
              " b'first',\n",
              " b'movies',\n",
              " b'bad',\n",
              " b'see',\n",
              " b'seen',\n",
              " b'up',\n",
              " b'only',\n",
              " b'were',\n",
              " b\"it's\",\n",
              " b'would',\n",
              " b'more',\n",
              " b'made',\n",
              " b'great',\n",
              " b'can',\n",
              " b'been',\n",
              " b'i',\n",
              " b'her',\n",
              " b'no',\n",
              " b'A',\n",
              " b'which',\n",
              " b'even',\n",
              " b'films',\n",
              " b'there',\n",
              " b'ever',\n",
              " b'people',\n",
              " b'much',\n",
              " b'because',\n",
              " b'most',\n",
              " b'plot',\n",
              " b'if',\n",
              " b'than',\n",
              " b'acting',\n",
              " b'get',\n",
              " b'their',\n",
              " b'well',\n",
              " b'into',\n",
              " b'how',\n",
              " b'best',\n",
              " b'think',\n",
              " b'other',\n",
              " b'its',\n",
              " b\"It's\",\n",
              " b'saw',\n",
              " b'could',\n",
              " b'watch',\n",
              " b'many',\n",
              " b\"don't\",\n",
              " b'do',\n",
              " b'will',\n",
              " b'say',\n",
              " b'show',\n",
              " b'love',\n",
              " b'make',\n",
              " b'two',\n",
              " b'way',\n",
              " b'know',\n",
              " b'years',\n",
              " b'after',\n",
              " b'But',\n",
              " b'we',\n",
              " b'watching',\n",
              " b'In',\n",
              " b'never',\n",
              " b'any',\n",
              " b'life',\n",
              " b'characters',\n",
              " b\"I'm\",\n",
              " b'too',\n",
              " b'did',\n",
              " b\"I've\",\n",
              " b'little',\n",
              " b'over',\n",
              " b'If',\n",
              " b'she',\n",
              " b'then',\n",
              " b'being',\n",
              " b'thought',\n",
              " b'them',\n",
              " b'off',\n",
              " b'There',\n",
              " b's',\n",
              " b'funny',\n",
              " b'better',\n",
              " b'also',\n",
              " b'watched',\n",
              " b'where',\n",
              " b'still',\n",
              " b'worst',\n",
              " b'old',\n",
              " b'am',\n",
              " b'through',\n",
              " b'actors',\n",
              " b'And',\n",
              " b'character',\n",
              " b'such',\n",
              " b\"didn't\",\n",
              " b'back',\n",
              " b'got',\n",
              " b'horror',\n",
              " b'those',\n",
              " b'series',\n",
              " b'lot',\n",
              " b'should',\n",
              " b'comedy',\n",
              " b'go',\n",
              " b'your',\n",
              " b'few',\n",
              " b'something',\n",
              " b'does',\n",
              " b'thing',\n",
              " b'him',\n",
              " b'man',\n",
              " b'What',\n",
              " b'going',\n",
              " b'original',\n",
              " b'director',\n",
              " b'work',\n",
              " b'here',\n",
              " b'before',\n",
              " b'real',\n",
              " b'actually',\n",
              " b'cast',\n",
              " b'pretty',\n",
              " b'TV',\n",
              " b'quite',\n",
              " b'why',\n",
              " b'every',\n",
              " b'He',\n",
              " b'find',\n",
              " b'As',\n",
              " b'scenes',\n",
              " b'young',\n",
              " b\"can't\",\n",
              " b'interesting',\n",
              " b'same',\n",
              " b'fan',\n",
              " b'now',\n",
              " b'makes',\n",
              " b'end',\n",
              " b'big',\n",
              " b'When',\n",
              " b'long',\n",
              " b'nothing',\n",
              " b'times',\n",
              " b'found',\n",
              " b\"'s\",\n",
              " b'must',\n",
              " b'these',\n",
              " b'again',\n",
              " b'another',\n",
              " b'part',\n",
              " b'action',\n",
              " b'though',\n",
              " b'always',\n",
              " b'script',\n",
              " b\"doesn't\",\n",
              " b'last',\n",
              " b'new',\n",
              " b'fact',\n",
              " b'year',\n",
              " b'while',\n",
              " b'around',\n",
              " b'look',\n",
              " b'since',\n",
              " b't',\n",
              " b'minutes',\n",
              " b'family',\n",
              " b'DVD',\n",
              " b'down',\n",
              " b'scene',\n",
              " b'things',\n",
              " b'want',\n",
              " b'read',\n",
              " b'seems',\n",
              " b'My',\n",
              " b'fun',\n",
              " b'may',\n",
              " b'world',\n",
              " b'done',\n",
              " b'far',\n",
              " b'come',\n",
              " b'right',\n",
              " b'take',\n",
              " b\"wasn't\",\n",
              " b'You',\n",
              " b'th',\n",
              " b'believe',\n",
              " b'almost',\n",
              " b'enough',\n",
              " b'hard',\n",
              " b'whole',\n",
              " b'anything',\n",
              " b'book',\n",
              " b\"isn't\",\n",
              " b'probably',\n",
              " b'least',\n",
              " b'give',\n",
              " b'bit',\n",
              " b'They',\n",
              " b'American',\n",
              " b'seeing',\n",
              " b'without',\n",
              " b'feel',\n",
              " b'One',\n",
              " b'awful',\n",
              " b'idea',\n",
              " b'making',\n",
              " b'between',\n",
              " b'remember',\n",
              " b'came',\n",
              " b'music',\n",
              " b'kind',\n",
              " b'set',\n",
              " b'excellent',\n",
              " b'John',\n",
              " b'classic',\n",
              " b'us',\n",
              " b'both',\n",
              " b'Well',\n",
              " b'version',\n",
              " b'performance',\n",
              " b'gets',\n",
              " b'plays',\n",
              " b'day',\n",
              " b'Not',\n",
              " b'low',\n",
              " b'might',\n",
              " b'own',\n",
              " b'loved',\n",
              " b'budget',\n",
              " b'role',\n",
              " b'reason',\n",
              " b'high',\n",
              " b'First',\n",
              " b'girl',\n",
              " b'shows',\n",
              " b'looking',\n",
              " b'After',\n",
              " b'For',\n",
              " b'So',\n",
              " b'video',\n",
              " b'special',\n",
              " b'However',\n",
              " b'effects',\n",
              " b'woman',\n",
              " b'different',\n",
              " b'terrible',\n",
              " b'true',\n",
              " b'point',\n",
              " b'start',\n",
              " b'actor',\n",
              " b'put',\n",
              " b'night',\n",
              " b'three',\n",
              " b'played',\n",
              " b'liked',\n",
              " b'yet',\n",
              " b'boring',\n",
              " b'beautiful',\n",
              " b'takes',\n",
              " b'especially',\n",
              " b'based',\n",
              " b'ago',\n",
              " b'main',\n",
              " b'short',\n",
              " b'play',\n",
              " b'guy',\n",
              " b'title',\n",
              " b'said',\n",
              " b'enjoyed',\n",
              " b'anyone',\n",
              " b'wonderful',\n",
              " b'trying',\n",
              " b\"that's\",\n",
              " b'sure',\n",
              " b'went',\n",
              " b'favorite',\n",
              " b'having',\n",
              " b'stars',\n",
              " b'together',\n",
              " b'Hollywood',\n",
              " b'absolutely',\n",
              " b'screen',\n",
              " b'our',\n",
              " b'wrong',\n",
              " b'rather',\n",
              " b'half',\n",
              " b'production',\n",
              " b'episode',\n",
              " b'goes',\n",
              " b'place',\n",
              " b'comes',\n",
              " b'early',\n",
              " b'piece',\n",
              " b'written',\n",
              " b'We',\n",
              " b'heard',\n",
              " b'away',\n",
              " b\"couldn't\",\n",
              " b'second',\n",
              " b'money',\n",
              " b'during',\n",
              " b'friends',\n",
              " b'job',\n",
              " b'THE',\n",
              " b'someone',\n",
              " b'entertaining',\n",
              " b'sense',\n",
              " b'looks',\n",
              " b'She',\n",
              " b'line',\n",
              " b'school',\n",
              " b'drama',\n",
              " b'each',\n",
              " b'flick',\n",
              " b'everything',\n",
              " b'understand',\n",
              " b'No',\n",
              " b'mean',\n",
              " b'course',\n",
              " b'enjoy',\n",
              " b'star',\n",
              " b'wanted',\n",
              " b'wife',\n",
              " b'directed',\n",
              " b'That',\n",
              " b'To',\n",
              " b'small',\n",
              " b'mind',\n",
              " b'seem',\n",
              " b'truly',\n",
              " b'While',\n",
              " b'stupid',\n",
              " b'simply',\n",
              " b\"'\",\n",
              " b'poor',\n",
              " b'All',\n",
              " b'worth',\n",
              " b'everyone',\n",
              " b'once',\n",
              " b'worse',\n",
              " b'top',\n",
              " b'completely',\n",
              " b'used',\n",
              " b'cinema',\n",
              " b'felt',\n",
              " b'kids',\n",
              " b'genre',\n",
              " b'shot',\n",
              " b'home',\n",
              " b'name',\n",
              " b'late',\n",
              " b'tell',\n",
              " b'called',\n",
              " b'until',\n",
              " b'reviews',\n",
              " b'New',\n",
              " b'myself',\n",
              " b'beginning',\n",
              " b'full',\n",
              " b'nice',\n",
              " b'live',\n",
              " b'left',\n",
              " b'At',\n",
              " b'horrible',\n",
              " b'novel',\n",
              " b'How',\n",
              " b'Although',\n",
              " b'thriller',\n",
              " b'documentary',\n",
              " b'women',\n",
              " b'starts',\n",
              " b'performances',\n",
              " b'several',\n",
              " b'else',\n",
              " b'Even',\n",
              " b'audience',\n",
              " b'less',\n",
              " b'perfect',\n",
              " b'often',\n",
              " b'couple',\n",
              " b'writing',\n",
              " b'amazing',\n",
              " b\"I'd\",\n",
              " b'review',\n",
              " b'released',\n",
              " b'seemed',\n",
              " b'history',\n",
              " b'need',\n",
              " b'case',\n",
              " b'Some',\n",
              " b'started',\n",
              " b'later',\n",
              " b'war',\n",
              " b'waste',\n",
              " b'Michael',\n",
              " b'try',\n",
              " b'use',\n",
              " b'either',\n",
              " b'given',\n",
              " b'humor',\n",
              " b'comments',\n",
              " b'B',\n",
              " b'black',\n",
              " b'disappointed',\n",
              " b'although',\n",
              " b'laugh',\n",
              " b'father',\n",
              " b'along',\n",
              " b'gave',\n",
              " b'however',\n",
              " b'quality',\n",
              " b'b',\n",
              " b'expect',\n",
              " b'totally',\n",
              " b'friend',\n",
              " b'gives',\n",
              " b'OK',\n",
              " b'itself',\n",
              " b'sex',\n",
              " b'thinking',\n",
              " b'surprised',\n",
              " b\"you're\",\n",
              " b'brilliant',\n",
              " b'With',\n",
              " b'Now',\n",
              " b'others',\n",
              " b'definitely',\n",
              " b'entire',\n",
              " b'dialogue',\n",
              " b'days',\n",
              " b'ending',\n",
              " b\"he's\",\n",
              " b'television',\n",
              " b'already',\n",
              " b'getting',\n",
              " b'past',\n",
              " b'let',\n",
              " b'James',\n",
              " b'boy',\n",
              " b'camera',\n",
              " b'style',\n",
              " b'problem',\n",
              " b'certainly',\n",
              " b'Just',\n",
              " b'group',\n",
              " b'stories',\n",
              " b'lead',\n",
              " b'o',\n",
              " b'town',\n",
              " b'Why',\n",
              " b'Of',\n",
              " b'maybe',\n",
              " b'person',\n",
              " b'himself',\n",
              " b'playing',\n",
              " b'lost',\n",
              " b'art',\n",
              " b'supposed',\n",
              " b'become',\n",
              " b'doing',\n",
              " b'usually',\n",
              " b'lives',\n",
              " b'men',\n",
              " b'looked',\n",
              " b'moments',\n",
              " b'death',\n",
              " b'hit',\n",
              " b'kid',\n",
              " b'took',\n",
              " b'slow',\n",
              " b'house',\n",
              " b'known',\n",
              " b'sort',\n",
              " b'fine',\n",
              " b'example',\n",
              " b'recently',\n",
              " b'picture',\n",
              " b'premise',\n",
              " b'An',\n",
              " b'His',\n",
              " b'crap',\n",
              " b'knew',\n",
              " b'British',\n",
              " b'direction',\n",
              " b'Robert',\n",
              " b'child',\n",
              " b'decided',\n",
              " b'hour',\n",
              " b'parts',\n",
              " b'Mr',\n",
              " b'help',\n",
              " b'huge',\n",
              " b'expecting',\n",
              " b'From',\n",
              " b'guess',\n",
              " b'fans',\n",
              " b'lines',\n",
              " b'Man',\n",
              " b'works',\n",
              " b'son',\n",
              " b'human',\n",
              " b'attempt',\n",
              " b'David',\n",
              " b'children',\n",
              " b'decent',\n",
              " b'comment',\n",
              " b'extremely',\n",
              " b'finally',\n",
              " b'sequel',\n",
              " b'jokes',\n",
              " b'keep',\n",
              " b'admit',\n",
              " b'actress',\n",
              " b'etc',\n",
              " b'writer',\n",
              " b\"there's\",\n",
              " b'episodes',\n",
              " b'against',\n",
              " b'type',\n",
              " b'game',\n",
              " b'simple',\n",
              " b'musical',\n",
              " b'heart',\n",
              " b'George',\n",
              " b'turned',\n",
              " b'Yes',\n",
              " b'tale',\n",
              " b'storyline',\n",
              " b'under',\n",
              " b'greatest',\n",
              " b'age',\n",
              " b'romantic',\n",
              " b'run',\n",
              " b'opinion',\n",
              " b'rest',\n",
              " b'hilarious',\n",
              " b'Richard',\n",
              " b'able',\n",
              " b'perhaps',\n",
              " b'expected',\n",
              " b'w',\n",
              " b'saying',\n",
              " b'close',\n",
              " b'theater',\n",
              " b'act',\n",
              " b'opening',\n",
              " b'involved',\n",
              " b'killer',\n",
              " b'told',\n",
              " b'typical',\n",
              " b\"I'll\",\n",
              " b'guys',\n",
              " b'Film',\n",
              " b'throughout',\n",
              " b'scary',\n",
              " b'face',\n",
              " b're',\n",
              " b'head',\n",
              " b'turn',\n",
              " b'usual',\n",
              " b'cannot',\n",
              " b'mother',\n",
              " b'serious',\n",
              " b'hours',\n",
              " b'York',\n",
              " b'Its',\n",
              " b'ridiculous',\n",
              " b'bunch',\n",
              " b'feeling',\n",
              " b'next',\n",
              " b'dark',\n",
              " b\"Don't\",\n",
              " b'wants',\n",
              " b'care',\n",
              " b'Unfortunately',\n",
              " b'On',\n",
              " b'predictable',\n",
              " b'word',\n",
              " b'side',\n",
              " b'chance',\n",
              " b'tries',\n",
              " b'car',\n",
              " b'Then',\n",
              " b'fi',\n",
              " b'today',\n",
              " b'Disney',\n",
              " b\"There's\",\n",
              " b'remake',\n",
              " b'self',\n",
              " b'rented',\n",
              " b'sometimes',\n",
              " b'Peter',\n",
              " b'br',\n",
              " b'class',\n",
              " b'dead',\n",
              " b'strange',\n",
              " b'local',\n",
              " b'across',\n",
              " b'acted',\n",
              " b'girls',\n",
              " b'cool',\n",
              " b'animation',\n",
              " b'middle',\n",
              " b'happened',\n",
              " b'silly',\n",
              " b'rating',\n",
              " b'stop',\n",
              " b'instead',\n",
              " b'modern',\n",
              " b\"haven't\",\n",
              " b'interest',\n",
              " b'sound',\n",
              " b'murder',\n",
              " b'doubt',\n",
              " b'matter',\n",
              " b'complete',\n",
              " b'exactly',\n",
              " b'fantastic',\n",
              " b'experience',\n",
              " b\"won't\",\n",
              " b'release',\n",
              " b'talent',\n",
              " b'stuff',\n",
              " b'living',\n",
              " b'white',\n",
              " b'IMDb',\n",
              " b'single',\n",
              " b'obviously',\n",
              " b'Tom',\n",
              " b'upon',\n",
              " b'enjoyable',\n",
              " b'four',\n",
              " b'five',\n",
              " b'except',\n",
              " b'S',\n",
              " b'tried',\n",
              " b'bought',\n",
              " b'reading',\n",
              " b'highly',\n",
              " b'm',\n",
              " b'coming',\n",
              " b'due',\n",
              " b'behind',\n",
              " b'call',\n",
              " b'Most',\n",
              " b'e',\n",
              " b'brother',\n",
              " b'eyes',\n",
              " b'French',\n",
              " b'wonder',\n",
              " b'War',\n",
              " b'number',\n",
              " b'shown',\n",
              " b'sad',\n",
              " b'tells',\n",
              " b'famous',\n",
              " b'feature',\n",
              " b'filmed',\n",
              " b'strong',\n",
              " b'becomes',\n",
              " b'somewhat',\n",
              " b'box',\n",
              " b'Oh',\n",
              " b'hope',\n",
              " b'wrote',\n",
              " b'season',\n",
              " b'obvious',\n",
              " b'happen',\n",
              " b'adaptation',\n",
              " b'clich',\n",
              " b'Paul',\n",
              " b'lame',\n",
              " b'Like',\n",
              " b'viewing',\n",
              " b'poorly',\n",
              " b'moving',\n",
              " b'career',\n",
              " b'evil',\n",
              " b'recommend',\n",
              " b'King',\n",
              " b'husband',\n",
              " b'Oscar',\n",
              " b'named',\n",
              " b'viewer',\n",
              " b'begin',\n",
              " b'violence',\n",
              " b'write',\n",
              " b'non',\n",
              " b'ones',\n",
              " b'view',\n",
              " b'expectations',\n",
              " b'among',\n",
              " b'dull',\n",
              " b'wish',\n",
              " b'masterpiece',\n",
              " b'major',\n",
              " b'straight',\n",
              " b'agree',\n",
              " b'Movie',\n",
              " b'sit',\n",
              " b'hand',\n",
              " b'period',\n",
              " b'directing',\n",
              " b'particularly',\n",
              " b'country',\n",
              " b'fight',\n",
              " b'hate',\n",
              " b'daughter',\n",
              " b'happens',\n",
              " b'English',\n",
              " b'says',\n",
              " b'stand',\n",
              " b'turns',\n",
              " b'Christmas',\n",
              " b'f',\n",
              " b'attention',\n",
              " b\"That's\",\n",
              " b'taken',\n",
              " b'cheesy',\n",
              " b'comic',\n",
              " b'mostly',\n",
              " b\"wouldn't\",\n",
              " b'light',\n",
              " b'entertainment',\n",
              " b'order',\n",
              " b'roles',\n",
              " b'superb',\n",
              " b'dialog',\n",
              " b'easily',\n",
              " b'starring',\n",
              " b'Lee',\n",
              " b'weak',\n",
              " b'mystery',\n",
              " b'Night',\n",
              " b'seriously',\n",
              " b'realistic',\n",
              " b'reasons',\n",
              " b'theme',\n",
              " b'possibly',\n",
              " b'words',\n",
              " b'ten',\n",
              " b'cheap',\n",
              " b'similar',\n",
              " b'brought',\n",
              " b'co',\n",
              " b'including',\n",
              " b'nearly',\n",
              " b'cinematography',\n",
              " b'falls',\n",
              " b'basically',\n",
              " b'produced',\n",
              " b'talking',\n",
              " b'working',\n",
              " b'taking',\n",
              " b'reality',\n",
              " b'store',\n",
              " b'unique',\n",
              " b'forward',\n",
              " b'hell',\n",
              " b'alone',\n",
              " b'T',\n",
              " b'whose',\n",
              " b'sets',\n",
              " b'previous',\n",
              " b'soon',\n",
              " b'themselves',\n",
              " b'lack',\n",
              " b'killed',\n",
              " b'directors',\n",
              " b'c',\n",
              " b'bring',\n",
              " b'week',\n",
              " b'effort',\n",
              " b'average',\n",
              " b'ways',\n",
              " b'female',\n",
              " b'running',\n",
              " b'fast',\n",
              " b'editing',\n",
              " b'kept',\n",
              " b'caught',\n",
              " b'Japanese',\n",
              " b'popular',\n",
              " b'screenplay',\n",
              " b'elements',\n",
              " b'comedies',\n",
              " b'surprise',\n",
              " b'became',\n",
              " b'problems',\n",
              " b'Jack',\n",
              " b'features',\n",
              " b'annoying',\n",
              " b'animated',\n",
              " b'yes',\n",
              " b'd',\n",
              " b'possible',\n",
              " b'cover',\n",
              " b'follow',\n",
              " b'near',\n",
              " b'William',\n",
              " b'list',\n",
              " b'songs',\n",
              " b'cartoon',\n",
              " b'Having',\n",
              " b'voice',\n",
              " b'parents',\n",
              " b'Maybe',\n",
              " b'Star',\n",
              " b'events',\n",
              " b'none',\n",
              " b'body',\n",
              " b'crime',\n",
              " b'h',\n",
              " b'Great',\n",
              " b'Who',\n",
              " b'future',\n",
              " b'Very',\n",
              " b'weird',\n",
              " b'God',\n",
              " b\"aren't\",\n",
              " b'actual',\n",
              " b'despite',\n",
              " b'final',\n",
              " b'Is',\n",
              " b'gore',\n",
              " b'imagine',\n",
              " b'change',\n",
              " b'sister',\n",
              " b'cut',\n",
              " b\"you'll\",\n",
              " b'D',\n",
              " b'lots',\n",
              " b'beyond',\n",
              " b'moment',\n",
              " b'Dr',\n",
              " b'positive',\n",
              " b'level',\n",
              " b'fantasy',\n",
              " b'important',\n",
              " b'minute',\n",
              " b'sci',\n",
              " b'Joe',\n",
              " b'easy',\n",
              " b'era',\n",
              " b'House',\n",
              " b'awesome',\n",
              " b'romance',\n",
              " b'above',\n",
              " b'team',\n",
              " b'subject',\n",
              " b'rent',\n",
              " b'badly',\n",
              " b'happy',\n",
              " b'finds',\n",
              " b'points',\n",
              " b'rate',\n",
              " b\"'The\",\n",
              " b'begins',\n",
              " b'message',\n",
              " b'incredibly',\n",
              " b'concept',\n",
              " b'Italian',\n",
              " b'realize',\n",
              " b'Good',\n",
              " b'recent',\n",
              " b'score',\n",
              " b'By',\n",
              " b'casting',\n",
              " b'America',\n",
              " b'police',\n",
              " b'SPOILERS',\n",
              " b'power',\n",
              " b\"you've\",\n",
              " b'NOT',\n",
              " b\"they're\",\n",
              " b'giving',\n",
              " b'college',\n",
              " b'difficult',\n",
              " b'fall',\n",
              " b'laughed',\n",
              " b'knows',\n",
              " b'save',\n",
              " b'incredible',\n",
              " b'third',\n",
              " b'potential',\n",
              " b'leave',\n",
              " b'song',\n",
              " b'clearly',\n",
              " b'Scott',\n",
              " b'slasher',\n",
              " b'talk',\n",
              " b'laughing',\n",
              " b'Every',\n",
              " b'meets',\n",
              " b'move',\n",
              " b'fairly',\n",
              " b'kill',\n",
              " b'hero',\n",
              " b'plain',\n",
              " b'means',\n",
              " b'p',\n",
              " b'adventure',\n",
              " b'suspense',\n",
              " b'certain',\n",
              " b'joke',\n",
              " b'material',\n",
              " b'rated',\n",
              " b'showing',\n",
              " b'Also',\n",
              " b'mess',\n",
              " b'needs',\n",
              " b'World',\n",
              " b'Ben',\n",
              " b'gone',\n",
              " b'books',\n",
              " b'disappointment',\n",
              " b'cop',\n",
              " b'talented',\n",
              " b'copy',\n",
              " b'Bill',\n",
              " b'Festival',\n",
              " b'showed',\n",
              " b'success',\n",
              " b'interested',\n",
              " b'fails',\n",
              " b'shots',\n",
              " b'stage',\n",
              " b'describe',\n",
              " b'Wow',\n",
              " b'Okay',\n",
              " b\"she's\",\n",
              " b'wasted',\n",
              " b'Jane',\n",
              " b'particular',\n",
              " b'spent',\n",
              " b'whether',\n",
              " b'bored',\n",
              " b'wait',\n",
              " b'clear',\n",
              " b'rare',\n",
              " b'Jim',\n",
              " b'trash',\n",
              " b'consider',\n",
              " ...]"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#truncated_vocabularyの中身を見てみる\n",
        "truncated_vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRq3sJ2JfVCw",
        "outputId": "7afedf2e-bf35-4639-cdfd-5adc12798613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22\n",
            "12\n",
            "11\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "#word_to_idは、truncated_vocabularyのindex番号, wordのうち、index番号が入っている。\n",
        "#This movie was faaaaaantasticの数字はどうか、見てみる。\n",
        "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
        "for word in b\"This movie was faaaaaantastic\".split():#This movie was faaaaaantasticを数字に変える\n",
        "    print(word_to_id.get(word) or vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eRpEXXpu5m1",
        "outputId": "884dd8a9-d555-4f6d-a2d5-93b8b6b215cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{b'<pad>': 0,\n",
              " b'the': 1,\n",
              " b'a': 2,\n",
              " b'of': 3,\n",
              " b'and': 4,\n",
              " b'to': 5,\n",
              " b'I': 6,\n",
              " b'is': 7,\n",
              " b'in': 8,\n",
              " b'this': 9,\n",
              " b'it': 10,\n",
              " b'was': 11,\n",
              " b'movie': 12,\n",
              " b'that': 13,\n",
              " b'The': 14,\n",
              " b'film': 15,\n",
              " b'with': 16,\n",
              " b'for': 17,\n",
              " b'as': 18,\n",
              " b'on': 19,\n",
              " b'but': 20,\n",
              " b'have': 21,\n",
              " b'This': 22,\n",
              " b'one': 23,\n",
              " b'not': 24,\n",
              " b'be': 25,\n",
              " b'are': 26,\n",
              " b'you': 27,\n",
              " b'an': 28,\n",
              " b'at': 29,\n",
              " b'about': 30,\n",
              " b'by': 31,\n",
              " b'all': 32,\n",
              " b'his': 33,\n",
              " b'so': 34,\n",
              " b'like': 35,\n",
              " b'from': 36,\n",
              " b'who': 37,\n",
              " b'has': 38,\n",
              " b'It': 39,\n",
              " b'good': 40,\n",
              " b'my': 41,\n",
              " b'just': 42,\n",
              " b'very': 43,\n",
              " b'out': 44,\n",
              " b'or': 45,\n",
              " b'story': 46,\n",
              " b'some': 47,\n",
              " b'time': 48,\n",
              " b'had': 49,\n",
              " b'he': 50,\n",
              " b'they': 51,\n",
              " b'really': 52,\n",
              " b'me': 53,\n",
              " b'when': 54,\n",
              " b'what': 55,\n",
              " b'first': 56,\n",
              " b'movies': 57,\n",
              " b'bad': 58,\n",
              " b'see': 59,\n",
              " b'seen': 60,\n",
              " b'up': 61,\n",
              " b'only': 62,\n",
              " b'were': 63,\n",
              " b\"it's\": 64,\n",
              " b'would': 65,\n",
              " b'more': 66,\n",
              " b'made': 67,\n",
              " b'great': 68,\n",
              " b'can': 69,\n",
              " b'been': 70,\n",
              " b'i': 71,\n",
              " b'her': 72,\n",
              " b'no': 73,\n",
              " b'A': 74,\n",
              " b'which': 75,\n",
              " b'even': 76,\n",
              " b'films': 77,\n",
              " b'there': 78,\n",
              " b'ever': 79,\n",
              " b'people': 80,\n",
              " b'much': 81,\n",
              " b'because': 82,\n",
              " b'most': 83,\n",
              " b'plot': 84,\n",
              " b'if': 85,\n",
              " b'than': 86,\n",
              " b'acting': 87,\n",
              " b'get': 88,\n",
              " b'their': 89,\n",
              " b'well': 90,\n",
              " b'into': 91,\n",
              " b'how': 92,\n",
              " b'best': 93,\n",
              " b'think': 94,\n",
              " b'other': 95,\n",
              " b'its': 96,\n",
              " b\"It's\": 97,\n",
              " b'saw': 98,\n",
              " b'could': 99,\n",
              " b'watch': 100,\n",
              " b'many': 101,\n",
              " b\"don't\": 102,\n",
              " b'do': 103,\n",
              " b'will': 104,\n",
              " b'say': 105,\n",
              " b'show': 106,\n",
              " b'love': 107,\n",
              " b'make': 108,\n",
              " b'two': 109,\n",
              " b'way': 110,\n",
              " b'know': 111,\n",
              " b'years': 112,\n",
              " b'after': 113,\n",
              " b'But': 114,\n",
              " b'we': 115,\n",
              " b'watching': 116,\n",
              " b'In': 117,\n",
              " b'never': 118,\n",
              " b'any': 119,\n",
              " b'life': 120,\n",
              " b'characters': 121,\n",
              " b\"I'm\": 122,\n",
              " b'too': 123,\n",
              " b'did': 124,\n",
              " b\"I've\": 125,\n",
              " b'little': 126,\n",
              " b'over': 127,\n",
              " b'If': 128,\n",
              " b'she': 129,\n",
              " b'then': 130,\n",
              " b'being': 131,\n",
              " b'thought': 132,\n",
              " b'them': 133,\n",
              " b'off': 134,\n",
              " b'There': 135,\n",
              " b's': 136,\n",
              " b'funny': 137,\n",
              " b'better': 138,\n",
              " b'also': 139,\n",
              " b'watched': 140,\n",
              " b'where': 141,\n",
              " b'still': 142,\n",
              " b'worst': 143,\n",
              " b'old': 144,\n",
              " b'am': 145,\n",
              " b'through': 146,\n",
              " b'actors': 147,\n",
              " b'And': 148,\n",
              " b'character': 149,\n",
              " b'such': 150,\n",
              " b\"didn't\": 151,\n",
              " b'back': 152,\n",
              " b'got': 153,\n",
              " b'horror': 154,\n",
              " b'those': 155,\n",
              " b'series': 156,\n",
              " b'lot': 157,\n",
              " b'should': 158,\n",
              " b'comedy': 159,\n",
              " b'go': 160,\n",
              " b'your': 161,\n",
              " b'few': 162,\n",
              " b'something': 163,\n",
              " b'does': 164,\n",
              " b'thing': 165,\n",
              " b'him': 166,\n",
              " b'man': 167,\n",
              " b'What': 168,\n",
              " b'going': 169,\n",
              " b'original': 170,\n",
              " b'director': 171,\n",
              " b'work': 172,\n",
              " b'here': 173,\n",
              " b'before': 174,\n",
              " b'real': 175,\n",
              " b'actually': 176,\n",
              " b'cast': 177,\n",
              " b'pretty': 178,\n",
              " b'TV': 179,\n",
              " b'quite': 180,\n",
              " b'why': 181,\n",
              " b'every': 182,\n",
              " b'He': 183,\n",
              " b'find': 184,\n",
              " b'As': 185,\n",
              " b'scenes': 186,\n",
              " b'young': 187,\n",
              " b\"can't\": 188,\n",
              " b'interesting': 189,\n",
              " b'same': 190,\n",
              " b'fan': 191,\n",
              " b'now': 192,\n",
              " b'makes': 193,\n",
              " b'end': 194,\n",
              " b'big': 195,\n",
              " b'When': 196,\n",
              " b'long': 197,\n",
              " b'nothing': 198,\n",
              " b'times': 199,\n",
              " b'found': 200,\n",
              " b\"'s\": 201,\n",
              " b'must': 202,\n",
              " b'these': 203,\n",
              " b'again': 204,\n",
              " b'another': 205,\n",
              " b'part': 206,\n",
              " b'action': 207,\n",
              " b'though': 208,\n",
              " b'always': 209,\n",
              " b'script': 210,\n",
              " b\"doesn't\": 211,\n",
              " b'last': 212,\n",
              " b'new': 213,\n",
              " b'fact': 214,\n",
              " b'year': 215,\n",
              " b'while': 216,\n",
              " b'around': 217,\n",
              " b'look': 218,\n",
              " b'since': 219,\n",
              " b't': 220,\n",
              " b'minutes': 221,\n",
              " b'family': 222,\n",
              " b'DVD': 223,\n",
              " b'down': 224,\n",
              " b'scene': 225,\n",
              " b'things': 226,\n",
              " b'want': 227,\n",
              " b'read': 228,\n",
              " b'seems': 229,\n",
              " b'My': 230,\n",
              " b'fun': 231,\n",
              " b'may': 232,\n",
              " b'world': 233,\n",
              " b'done': 234,\n",
              " b'far': 235,\n",
              " b'come': 236,\n",
              " b'right': 237,\n",
              " b'take': 238,\n",
              " b\"wasn't\": 239,\n",
              " b'You': 240,\n",
              " b'th': 241,\n",
              " b'believe': 242,\n",
              " b'almost': 243,\n",
              " b'enough': 244,\n",
              " b'hard': 245,\n",
              " b'whole': 246,\n",
              " b'anything': 247,\n",
              " b'book': 248,\n",
              " b\"isn't\": 249,\n",
              " b'probably': 250,\n",
              " b'least': 251,\n",
              " b'give': 252,\n",
              " b'bit': 253,\n",
              " b'They': 254,\n",
              " b'American': 255,\n",
              " b'seeing': 256,\n",
              " b'without': 257,\n",
              " b'feel': 258,\n",
              " b'One': 259,\n",
              " b'awful': 260,\n",
              " b'idea': 261,\n",
              " b'making': 262,\n",
              " b'between': 263,\n",
              " b'remember': 264,\n",
              " b'came': 265,\n",
              " b'music': 266,\n",
              " b'kind': 267,\n",
              " b'set': 268,\n",
              " b'excellent': 269,\n",
              " b'John': 270,\n",
              " b'classic': 271,\n",
              " b'us': 272,\n",
              " b'both': 273,\n",
              " b'Well': 274,\n",
              " b'version': 275,\n",
              " b'performance': 276,\n",
              " b'gets': 277,\n",
              " b'plays': 278,\n",
              " b'day': 279,\n",
              " b'Not': 280,\n",
              " b'low': 281,\n",
              " b'might': 282,\n",
              " b'own': 283,\n",
              " b'loved': 284,\n",
              " b'budget': 285,\n",
              " b'role': 286,\n",
              " b'reason': 287,\n",
              " b'high': 288,\n",
              " b'First': 289,\n",
              " b'girl': 290,\n",
              " b'shows': 291,\n",
              " b'looking': 292,\n",
              " b'After': 293,\n",
              " b'For': 294,\n",
              " b'So': 295,\n",
              " b'video': 296,\n",
              " b'special': 297,\n",
              " b'However': 298,\n",
              " b'effects': 299,\n",
              " b'woman': 300,\n",
              " b'different': 301,\n",
              " b'terrible': 302,\n",
              " b'true': 303,\n",
              " b'point': 304,\n",
              " b'start': 305,\n",
              " b'actor': 306,\n",
              " b'put': 307,\n",
              " b'night': 308,\n",
              " b'three': 309,\n",
              " b'played': 310,\n",
              " b'liked': 311,\n",
              " b'yet': 312,\n",
              " b'boring': 313,\n",
              " b'beautiful': 314,\n",
              " b'takes': 315,\n",
              " b'especially': 316,\n",
              " b'based': 317,\n",
              " b'ago': 318,\n",
              " b'main': 319,\n",
              " b'short': 320,\n",
              " b'play': 321,\n",
              " b'guy': 322,\n",
              " b'title': 323,\n",
              " b'said': 324,\n",
              " b'enjoyed': 325,\n",
              " b'anyone': 326,\n",
              " b'wonderful': 327,\n",
              " b'trying': 328,\n",
              " b\"that's\": 329,\n",
              " b'sure': 330,\n",
              " b'went': 331,\n",
              " b'favorite': 332,\n",
              " b'having': 333,\n",
              " b'stars': 334,\n",
              " b'together': 335,\n",
              " b'Hollywood': 336,\n",
              " b'absolutely': 337,\n",
              " b'screen': 338,\n",
              " b'our': 339,\n",
              " b'wrong': 340,\n",
              " b'rather': 341,\n",
              " b'half': 342,\n",
              " b'production': 343,\n",
              " b'episode': 344,\n",
              " b'goes': 345,\n",
              " b'place': 346,\n",
              " b'comes': 347,\n",
              " b'early': 348,\n",
              " b'piece': 349,\n",
              " b'written': 350,\n",
              " b'We': 351,\n",
              " b'heard': 352,\n",
              " b'away': 353,\n",
              " b\"couldn't\": 354,\n",
              " b'second': 355,\n",
              " b'money': 356,\n",
              " b'during': 357,\n",
              " b'friends': 358,\n",
              " b'job': 359,\n",
              " b'THE': 360,\n",
              " b'someone': 361,\n",
              " b'entertaining': 362,\n",
              " b'sense': 363,\n",
              " b'looks': 364,\n",
              " b'She': 365,\n",
              " b'line': 366,\n",
              " b'school': 367,\n",
              " b'drama': 368,\n",
              " b'each': 369,\n",
              " b'flick': 370,\n",
              " b'everything': 371,\n",
              " b'understand': 372,\n",
              " b'No': 373,\n",
              " b'mean': 374,\n",
              " b'course': 375,\n",
              " b'enjoy': 376,\n",
              " b'star': 377,\n",
              " b'wanted': 378,\n",
              " b'wife': 379,\n",
              " b'directed': 380,\n",
              " b'That': 381,\n",
              " b'To': 382,\n",
              " b'small': 383,\n",
              " b'mind': 384,\n",
              " b'seem': 385,\n",
              " b'truly': 386,\n",
              " b'While': 387,\n",
              " b'stupid': 388,\n",
              " b'simply': 389,\n",
              " b\"'\": 390,\n",
              " b'poor': 391,\n",
              " b'All': 392,\n",
              " b'worth': 393,\n",
              " b'everyone': 394,\n",
              " b'once': 395,\n",
              " b'worse': 396,\n",
              " b'top': 397,\n",
              " b'completely': 398,\n",
              " b'used': 399,\n",
              " b'cinema': 400,\n",
              " b'felt': 401,\n",
              " b'kids': 402,\n",
              " b'genre': 403,\n",
              " b'shot': 404,\n",
              " b'home': 405,\n",
              " b'name': 406,\n",
              " b'late': 407,\n",
              " b'tell': 408,\n",
              " b'called': 409,\n",
              " b'until': 410,\n",
              " b'reviews': 411,\n",
              " b'New': 412,\n",
              " b'myself': 413,\n",
              " b'beginning': 414,\n",
              " b'full': 415,\n",
              " b'nice': 416,\n",
              " b'live': 417,\n",
              " b'left': 418,\n",
              " b'At': 419,\n",
              " b'horrible': 420,\n",
              " b'novel': 421,\n",
              " b'How': 422,\n",
              " b'Although': 423,\n",
              " b'thriller': 424,\n",
              " b'documentary': 425,\n",
              " b'women': 426,\n",
              " b'starts': 427,\n",
              " b'performances': 428,\n",
              " b'several': 429,\n",
              " b'else': 430,\n",
              " b'Even': 431,\n",
              " b'audience': 432,\n",
              " b'less': 433,\n",
              " b'perfect': 434,\n",
              " b'often': 435,\n",
              " b'couple': 436,\n",
              " b'writing': 437,\n",
              " b'amazing': 438,\n",
              " b\"I'd\": 439,\n",
              " b'review': 440,\n",
              " b'released': 441,\n",
              " b'seemed': 442,\n",
              " b'history': 443,\n",
              " b'need': 444,\n",
              " b'case': 445,\n",
              " b'Some': 446,\n",
              " b'started': 447,\n",
              " b'later': 448,\n",
              " b'war': 449,\n",
              " b'waste': 450,\n",
              " b'Michael': 451,\n",
              " b'try': 452,\n",
              " b'use': 453,\n",
              " b'either': 454,\n",
              " b'given': 455,\n",
              " b'humor': 456,\n",
              " b'comments': 457,\n",
              " b'B': 458,\n",
              " b'black': 459,\n",
              " b'disappointed': 460,\n",
              " b'although': 461,\n",
              " b'laugh': 462,\n",
              " b'father': 463,\n",
              " b'along': 464,\n",
              " b'gave': 465,\n",
              " b'however': 466,\n",
              " b'quality': 467,\n",
              " b'b': 468,\n",
              " b'expect': 469,\n",
              " b'totally': 470,\n",
              " b'friend': 471,\n",
              " b'gives': 472,\n",
              " b'OK': 473,\n",
              " b'itself': 474,\n",
              " b'sex': 475,\n",
              " b'thinking': 476,\n",
              " b'surprised': 477,\n",
              " b\"you're\": 478,\n",
              " b'brilliant': 479,\n",
              " b'With': 480,\n",
              " b'Now': 481,\n",
              " b'others': 482,\n",
              " b'definitely': 483,\n",
              " b'entire': 484,\n",
              " b'dialogue': 485,\n",
              " b'days': 486,\n",
              " b'ending': 487,\n",
              " b\"he's\": 488,\n",
              " b'television': 489,\n",
              " b'already': 490,\n",
              " b'getting': 491,\n",
              " b'past': 492,\n",
              " b'let': 493,\n",
              " b'James': 494,\n",
              " b'boy': 495,\n",
              " b'camera': 496,\n",
              " b'style': 497,\n",
              " b'problem': 498,\n",
              " b'certainly': 499,\n",
              " b'Just': 500,\n",
              " b'group': 501,\n",
              " b'stories': 502,\n",
              " b'lead': 503,\n",
              " b'o': 504,\n",
              " b'town': 505,\n",
              " b'Why': 506,\n",
              " b'Of': 507,\n",
              " b'maybe': 508,\n",
              " b'person': 509,\n",
              " b'himself': 510,\n",
              " b'playing': 511,\n",
              " b'lost': 512,\n",
              " b'art': 513,\n",
              " b'supposed': 514,\n",
              " b'become': 515,\n",
              " b'doing': 516,\n",
              " b'usually': 517,\n",
              " b'lives': 518,\n",
              " b'men': 519,\n",
              " b'looked': 520,\n",
              " b'moments': 521,\n",
              " b'death': 522,\n",
              " b'hit': 523,\n",
              " b'kid': 524,\n",
              " b'took': 525,\n",
              " b'slow': 526,\n",
              " b'house': 527,\n",
              " b'known': 528,\n",
              " b'sort': 529,\n",
              " b'fine': 530,\n",
              " b'example': 531,\n",
              " b'recently': 532,\n",
              " b'picture': 533,\n",
              " b'premise': 534,\n",
              " b'An': 535,\n",
              " b'His': 536,\n",
              " b'crap': 537,\n",
              " b'knew': 538,\n",
              " b'British': 539,\n",
              " b'direction': 540,\n",
              " b'Robert': 541,\n",
              " b'child': 542,\n",
              " b'decided': 543,\n",
              " b'hour': 544,\n",
              " b'parts': 545,\n",
              " b'Mr': 546,\n",
              " b'help': 547,\n",
              " b'huge': 548,\n",
              " b'expecting': 549,\n",
              " b'From': 550,\n",
              " b'guess': 551,\n",
              " b'fans': 552,\n",
              " b'lines': 553,\n",
              " b'Man': 554,\n",
              " b'works': 555,\n",
              " b'son': 556,\n",
              " b'human': 557,\n",
              " b'attempt': 558,\n",
              " b'David': 559,\n",
              " b'children': 560,\n",
              " b'decent': 561,\n",
              " b'comment': 562,\n",
              " b'extremely': 563,\n",
              " b'finally': 564,\n",
              " b'sequel': 565,\n",
              " b'jokes': 566,\n",
              " b'keep': 567,\n",
              " b'admit': 568,\n",
              " b'actress': 569,\n",
              " b'etc': 570,\n",
              " b'writer': 571,\n",
              " b\"there's\": 572,\n",
              " b'episodes': 573,\n",
              " b'against': 574,\n",
              " b'type': 575,\n",
              " b'game': 576,\n",
              " b'simple': 577,\n",
              " b'musical': 578,\n",
              " b'heart': 579,\n",
              " b'George': 580,\n",
              " b'turned': 581,\n",
              " b'Yes': 582,\n",
              " b'tale': 583,\n",
              " b'storyline': 584,\n",
              " b'under': 585,\n",
              " b'greatest': 586,\n",
              " b'age': 587,\n",
              " b'romantic': 588,\n",
              " b'run': 589,\n",
              " b'opinion': 590,\n",
              " b'rest': 591,\n",
              " b'hilarious': 592,\n",
              " b'Richard': 593,\n",
              " b'able': 594,\n",
              " b'perhaps': 595,\n",
              " b'expected': 596,\n",
              " b'w': 597,\n",
              " b'saying': 598,\n",
              " b'close': 599,\n",
              " b'theater': 600,\n",
              " b'act': 601,\n",
              " b'opening': 602,\n",
              " b'involved': 603,\n",
              " b'killer': 604,\n",
              " b'told': 605,\n",
              " b'typical': 606,\n",
              " b\"I'll\": 607,\n",
              " b'guys': 608,\n",
              " b'Film': 609,\n",
              " b'throughout': 610,\n",
              " b'scary': 611,\n",
              " b'face': 612,\n",
              " b're': 613,\n",
              " b'head': 614,\n",
              " b'turn': 615,\n",
              " b'usual': 616,\n",
              " b'cannot': 617,\n",
              " b'mother': 618,\n",
              " b'serious': 619,\n",
              " b'hours': 620,\n",
              " b'York': 621,\n",
              " b'Its': 622,\n",
              " b'ridiculous': 623,\n",
              " b'bunch': 624,\n",
              " b'feeling': 625,\n",
              " b'next': 626,\n",
              " b'dark': 627,\n",
              " b\"Don't\": 628,\n",
              " b'wants': 629,\n",
              " b'care': 630,\n",
              " b'Unfortunately': 631,\n",
              " b'On': 632,\n",
              " b'predictable': 633,\n",
              " b'word': 634,\n",
              " b'side': 635,\n",
              " b'chance': 636,\n",
              " b'tries': 637,\n",
              " b'car': 638,\n",
              " b'Then': 639,\n",
              " b'fi': 640,\n",
              " b'today': 641,\n",
              " b'Disney': 642,\n",
              " b\"There's\": 643,\n",
              " b'remake': 644,\n",
              " b'self': 645,\n",
              " b'rented': 646,\n",
              " b'sometimes': 647,\n",
              " b'Peter': 648,\n",
              " b'br': 649,\n",
              " b'class': 650,\n",
              " b'dead': 651,\n",
              " b'strange': 652,\n",
              " b'local': 653,\n",
              " b'across': 654,\n",
              " b'acted': 655,\n",
              " b'girls': 656,\n",
              " b'cool': 657,\n",
              " b'animation': 658,\n",
              " b'middle': 659,\n",
              " b'happened': 660,\n",
              " b'silly': 661,\n",
              " b'rating': 662,\n",
              " b'stop': 663,\n",
              " b'instead': 664,\n",
              " b'modern': 665,\n",
              " b\"haven't\": 666,\n",
              " b'interest': 667,\n",
              " b'sound': 668,\n",
              " b'murder': 669,\n",
              " b'doubt': 670,\n",
              " b'matter': 671,\n",
              " b'complete': 672,\n",
              " b'exactly': 673,\n",
              " b'fantastic': 674,\n",
              " b'experience': 675,\n",
              " b\"won't\": 676,\n",
              " b'release': 677,\n",
              " b'talent': 678,\n",
              " b'stuff': 679,\n",
              " b'living': 680,\n",
              " b'white': 681,\n",
              " b'IMDb': 682,\n",
              " b'single': 683,\n",
              " b'obviously': 684,\n",
              " b'Tom': 685,\n",
              " b'upon': 686,\n",
              " b'enjoyable': 687,\n",
              " b'four': 688,\n",
              " b'five': 689,\n",
              " b'except': 690,\n",
              " b'S': 691,\n",
              " b'tried': 692,\n",
              " b'bought': 693,\n",
              " b'reading': 694,\n",
              " b'highly': 695,\n",
              " b'm': 696,\n",
              " b'coming': 697,\n",
              " b'due': 698,\n",
              " b'behind': 699,\n",
              " b'call': 700,\n",
              " b'Most': 701,\n",
              " b'e': 702,\n",
              " b'brother': 703,\n",
              " b'eyes': 704,\n",
              " b'French': 705,\n",
              " b'wonder': 706,\n",
              " b'War': 707,\n",
              " b'number': 708,\n",
              " b'shown': 709,\n",
              " b'sad': 710,\n",
              " b'tells': 711,\n",
              " b'famous': 712,\n",
              " b'feature': 713,\n",
              " b'filmed': 714,\n",
              " b'strong': 715,\n",
              " b'becomes': 716,\n",
              " b'somewhat': 717,\n",
              " b'box': 718,\n",
              " b'Oh': 719,\n",
              " b'hope': 720,\n",
              " b'wrote': 721,\n",
              " b'season': 722,\n",
              " b'obvious': 723,\n",
              " b'happen': 724,\n",
              " b'adaptation': 725,\n",
              " b'clich': 726,\n",
              " b'Paul': 727,\n",
              " b'lame': 728,\n",
              " b'Like': 729,\n",
              " b'viewing': 730,\n",
              " b'poorly': 731,\n",
              " b'moving': 732,\n",
              " b'career': 733,\n",
              " b'evil': 734,\n",
              " b'recommend': 735,\n",
              " b'King': 736,\n",
              " b'husband': 737,\n",
              " b'Oscar': 738,\n",
              " b'named': 739,\n",
              " b'viewer': 740,\n",
              " b'begin': 741,\n",
              " b'violence': 742,\n",
              " b'write': 743,\n",
              " b'non': 744,\n",
              " b'ones': 745,\n",
              " b'view': 746,\n",
              " b'expectations': 747,\n",
              " b'among': 748,\n",
              " b'dull': 749,\n",
              " b'wish': 750,\n",
              " b'masterpiece': 751,\n",
              " b'major': 752,\n",
              " b'straight': 753,\n",
              " b'agree': 754,\n",
              " b'Movie': 755,\n",
              " b'sit': 756,\n",
              " b'hand': 757,\n",
              " b'period': 758,\n",
              " b'directing': 759,\n",
              " b'particularly': 760,\n",
              " b'country': 761,\n",
              " b'fight': 762,\n",
              " b'hate': 763,\n",
              " b'daughter': 764,\n",
              " b'happens': 765,\n",
              " b'English': 766,\n",
              " b'says': 767,\n",
              " b'stand': 768,\n",
              " b'turns': 769,\n",
              " b'Christmas': 770,\n",
              " b'f': 771,\n",
              " b'attention': 772,\n",
              " b\"That's\": 773,\n",
              " b'taken': 774,\n",
              " b'cheesy': 775,\n",
              " b'comic': 776,\n",
              " b'mostly': 777,\n",
              " b\"wouldn't\": 778,\n",
              " b'light': 779,\n",
              " b'entertainment': 780,\n",
              " b'order': 781,\n",
              " b'roles': 782,\n",
              " b'superb': 783,\n",
              " b'dialog': 784,\n",
              " b'easily': 785,\n",
              " b'starring': 786,\n",
              " b'Lee': 787,\n",
              " b'weak': 788,\n",
              " b'mystery': 789,\n",
              " b'Night': 790,\n",
              " b'seriously': 791,\n",
              " b'realistic': 792,\n",
              " b'reasons': 793,\n",
              " b'theme': 794,\n",
              " b'possibly': 795,\n",
              " b'words': 796,\n",
              " b'ten': 797,\n",
              " b'cheap': 798,\n",
              " b'similar': 799,\n",
              " b'brought': 800,\n",
              " b'co': 801,\n",
              " b'including': 802,\n",
              " b'nearly': 803,\n",
              " b'cinematography': 804,\n",
              " b'falls': 805,\n",
              " b'basically': 806,\n",
              " b'produced': 807,\n",
              " b'talking': 808,\n",
              " b'working': 809,\n",
              " b'taking': 810,\n",
              " b'reality': 811,\n",
              " b'store': 812,\n",
              " b'unique': 813,\n",
              " b'forward': 814,\n",
              " b'hell': 815,\n",
              " b'alone': 816,\n",
              " b'T': 817,\n",
              " b'whose': 818,\n",
              " b'sets': 819,\n",
              " b'previous': 820,\n",
              " b'soon': 821,\n",
              " b'themselves': 822,\n",
              " b'lack': 823,\n",
              " b'killed': 824,\n",
              " b'directors': 825,\n",
              " b'c': 826,\n",
              " b'bring': 827,\n",
              " b'week': 828,\n",
              " b'effort': 829,\n",
              " b'average': 830,\n",
              " b'ways': 831,\n",
              " b'female': 832,\n",
              " b'running': 833,\n",
              " b'fast': 834,\n",
              " b'editing': 835,\n",
              " b'kept': 836,\n",
              " b'caught': 837,\n",
              " b'Japanese': 838,\n",
              " b'popular': 839,\n",
              " b'screenplay': 840,\n",
              " b'elements': 841,\n",
              " b'comedies': 842,\n",
              " b'surprise': 843,\n",
              " b'became': 844,\n",
              " b'problems': 845,\n",
              " b'Jack': 846,\n",
              " b'features': 847,\n",
              " b'annoying': 848,\n",
              " b'animated': 849,\n",
              " b'yes': 850,\n",
              " b'd': 851,\n",
              " b'possible': 852,\n",
              " b'cover': 853,\n",
              " b'follow': 854,\n",
              " b'near': 855,\n",
              " b'William': 856,\n",
              " b'list': 857,\n",
              " b'songs': 858,\n",
              " b'cartoon': 859,\n",
              " b'Having': 860,\n",
              " b'voice': 861,\n",
              " b'parents': 862,\n",
              " b'Maybe': 863,\n",
              " b'Star': 864,\n",
              " b'events': 865,\n",
              " b'none': 866,\n",
              " b'body': 867,\n",
              " b'crime': 868,\n",
              " b'h': 869,\n",
              " b'Great': 870,\n",
              " b'Who': 871,\n",
              " b'future': 872,\n",
              " b'Very': 873,\n",
              " b'weird': 874,\n",
              " b'God': 875,\n",
              " b\"aren't\": 876,\n",
              " b'actual': 877,\n",
              " b'despite': 878,\n",
              " b'final': 879,\n",
              " b'Is': 880,\n",
              " b'gore': 881,\n",
              " b'imagine': 882,\n",
              " b'change': 883,\n",
              " b'sister': 884,\n",
              " b'cut': 885,\n",
              " b\"you'll\": 886,\n",
              " b'D': 887,\n",
              " b'lots': 888,\n",
              " b'beyond': 889,\n",
              " b'moment': 890,\n",
              " b'Dr': 891,\n",
              " b'positive': 892,\n",
              " b'level': 893,\n",
              " b'fantasy': 894,\n",
              " b'important': 895,\n",
              " b'minute': 896,\n",
              " b'sci': 897,\n",
              " b'Joe': 898,\n",
              " b'easy': 899,\n",
              " b'era': 900,\n",
              " b'House': 901,\n",
              " b'awesome': 902,\n",
              " b'romance': 903,\n",
              " b'above': 904,\n",
              " b'team': 905,\n",
              " b'subject': 906,\n",
              " b'rent': 907,\n",
              " b'badly': 908,\n",
              " b'happy': 909,\n",
              " b'finds': 910,\n",
              " b'points': 911,\n",
              " b'rate': 912,\n",
              " b\"'The\": 913,\n",
              " b'begins': 914,\n",
              " b'message': 915,\n",
              " b'incredibly': 916,\n",
              " b'concept': 917,\n",
              " b'Italian': 918,\n",
              " b'realize': 919,\n",
              " b'Good': 920,\n",
              " b'recent': 921,\n",
              " b'score': 922,\n",
              " b'By': 923,\n",
              " b'casting': 924,\n",
              " b'America': 925,\n",
              " b'police': 926,\n",
              " b'SPOILERS': 927,\n",
              " b'power': 928,\n",
              " b\"you've\": 929,\n",
              " b'NOT': 930,\n",
              " b\"they're\": 931,\n",
              " b'giving': 932,\n",
              " b'college': 933,\n",
              " b'difficult': 934,\n",
              " b'fall': 935,\n",
              " b'laughed': 936,\n",
              " b'knows': 937,\n",
              " b'save': 938,\n",
              " b'incredible': 939,\n",
              " b'third': 940,\n",
              " b'potential': 941,\n",
              " b'leave': 942,\n",
              " b'song': 943,\n",
              " b'clearly': 944,\n",
              " b'Scott': 945,\n",
              " b'slasher': 946,\n",
              " b'talk': 947,\n",
              " b'laughing': 948,\n",
              " b'Every': 949,\n",
              " b'meets': 950,\n",
              " b'move': 951,\n",
              " b'fairly': 952,\n",
              " b'kill': 953,\n",
              " b'hero': 954,\n",
              " b'plain': 955,\n",
              " b'means': 956,\n",
              " b'p': 957,\n",
              " b'adventure': 958,\n",
              " b'suspense': 959,\n",
              " b'certain': 960,\n",
              " b'joke': 961,\n",
              " b'material': 962,\n",
              " b'rated': 963,\n",
              " b'showing': 964,\n",
              " b'Also': 965,\n",
              " b'mess': 966,\n",
              " b'needs': 967,\n",
              " b'World': 968,\n",
              " b'Ben': 969,\n",
              " b'gone': 970,\n",
              " b'books': 971,\n",
              " b'disappointment': 972,\n",
              " b'cop': 973,\n",
              " b'talented': 974,\n",
              " b'copy': 975,\n",
              " b'Bill': 976,\n",
              " b'Festival': 977,\n",
              " b'showed': 978,\n",
              " b'success': 979,\n",
              " b'interested': 980,\n",
              " b'fails': 981,\n",
              " b'shots': 982,\n",
              " b'stage': 983,\n",
              " b'describe': 984,\n",
              " b'Wow': 985,\n",
              " b'Okay': 986,\n",
              " b\"she's\": 987,\n",
              " b'wasted': 988,\n",
              " b'Jane': 989,\n",
              " b'particular': 990,\n",
              " b'spent': 991,\n",
              " b'whether': 992,\n",
              " b'bored': 993,\n",
              " b'wait': 994,\n",
              " b'clear': 995,\n",
              " b'rare': 996,\n",
              " b'Jim': 997,\n",
              " b'trash': 998,\n",
              " b'consider': 999,\n",
              " ...}"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_to_id\n",
        "#単語とID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qlyaBGSfVCw"
      },
      "outputs": [],
      "source": [
        "#ルックアップテーブルの作成→tf.constant、tf.range、tf.lookup.KeyValueTensorInitializerは、ルックアップテーブル作成に使用するメソッド\n",
        "#ルックアップテーブル（LUT）とは、入力された値に対して出力値を割り当てるテーブルのこと。　ルックアップテーブルの説明　https://jp.mathworks.com/help/simulink/ug/anatomy-of-a-lookup-table.html\n",
        "#前処理ステップには、個々の単語をID（つまり語彙のなかでの単語のインデックス）に置き換えるステップが必要になる。\n",
        "#13 章で行ったように、1,000 個の語彙外（oov）バケットを使ってルックアップテーブルを作る。\n",
        "\n",
        "import tensorflow as tf\n",
        "words = tf.constant(truncated_vocabulary)\n",
        "#tf.constant→テンソルライクオブジェクトから定数テンソルを作成する。\n",
        "#https://www.tensorflow.org/api_docs/python/tf/constant\n",
        "\n",
        "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
        "#word_ids　数列を作成する。\n",
        "#https://www.tensorflow.org/api_docs/python/tf/range\n",
        "\n",
        "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "#keys と values テンソルを指定したテーブル初期化子。https://runebook.dev/ja/docs/tensorflow/lookup/keyvaluetensorinitializer\n",
        "num_oov_buckets = 1000\n",
        "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)#語彙外のキーをハッシュ バケットに割り当てる ID テーブルへの文字列。https://runebook.dev/ja/docs/tensorflow/lookup/keyvaluetensorinitializer\n",
        "\n",
        "#難しいので、tfのルックアップテーブルを出力するおまじないという理解で進める\n",
        "\n",
        "#OOV(out-of-vocabulary)\n",
        "#https://yag-ays.github.io/project/out-of-vocab-magnitude/\n",
        "#https://olaris.jp/poststag/xCVh0r-r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObexVOpACrhB",
        "outputId": "2325fd0d-9285-481b-914a-6b447b212ec1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.ops.lookup_ops.StaticVocabularyTable at 0x7fcb9c83b8e0>"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#tableの中身をみてみる StaticVocabularyTable ?\n",
        "table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DVtbrCmfVCw",
        "outputId": "c338e46a-41de-46f6-cebe-1454837d6f00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]])>"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#このルックアップテーブルを使っていくつかの単語のIDを調べてみる。\n",
        "#this、movie、was の3 語はテーブルに含まれているのでID が10,000 未満になっているが、\n",
        "#faaaaaantastic という単語は語彙に含まれていないのでoov バケットにマッピングされており、IDは10,000 以上になっている\n",
        "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRpTmY4rfVCw"
      },
      "outputs": [],
      "source": [
        "def encode_words(X_batch, y_batch):#encode_words（）関数を作成し、今作成したルックアップテーブルを使って単語をエンコードする。\n",
        "    return table.lookup(X_batch), y_batch\n",
        "\n",
        "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
        "train_set = train_set.map(encode_words).prefetch(1)\n",
        "#バッチをプリフェッチする(読み込む)\n",
        "#プリフェッチ (prefetch) とは、コンピュータにおいて、利用が予測されるデータをあらかじめより高速なメモリに読み込んでおき、性能と速度の向上を図る動作である。\n",
        "#プリフェッチの説明　https://qiita.com/Suguru_Toyohara/items/820b0dad955ecd91c7f3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsQ7cMhSF99t",
        "outputId": "3aacfb5c-6037-41d1-d76d-0ba54d68da1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(None, None), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj8fsnkdfVCw",
        "outputId": "378503b2-7122-417b-8633-9c6fe18dbe71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[  22   11   28 ...    0    0    0]\n",
            " [   6   21   70 ...    0    0    0]\n",
            " [4099 6881    1 ...    0    0    0]\n",
            " ...\n",
            " [  22   12  118 ...  331 1047    0]\n",
            " [1757 4101  451 ...    0    0    0]\n",
            " [3365 4392    6 ...    0    0    0]], shape=(32, 60), dtype=int64)\n",
            "tf.Tensor([0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0], shape=(32,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "#X_batch, y_batchの中身を見てみる\n",
        "for X_batch, y_batch in train_set.take(1):\n",
        "    print(X_batch)\n",
        "    print(y_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVI3wxlSfVCw",
        "outputId": "69b7619c-a526-4204-f17f-87f1fbda8d21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 27s 24ms/step - loss: 0.5305 - accuracy: 0.7281\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 15s 19ms/step - loss: 0.3459 - accuracy: 0.8549\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 15s 19ms/step - loss: 0.1934 - accuracy: 0.9313\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 15s 19ms/step - loss: 0.1361 - accuracy: 0.9503\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 15s 19ms/step - loss: 0.1032 - accuracy: 0.9634\n"
          ]
        }
      ],
      "source": [
        "#モデルを作成し、訓練することができる。Keras LSTM\n",
        "\n",
        "#https://keras.io/ja/getting-started/sequential-model-guide/\n",
        "#→Sequentia→l（系列）モデルは層を積み重ねたものです\n",
        "#https://keras.io/ja/layers/embeddings/ \n",
        "#→Embedding→正の整数（インデックス）を固定次元の密ベクトルに変換します． 例）[[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
        "\n",
        "#https://qiita.com/Kosuke-Szk/items/43f330b0888f8e696c3f\n",
        "#Embedding層(埋め込み層),embed_sizeとは？上記HPの図を見る\n",
        "embed_size = 128\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,#https://keras.io/ja/layers/embeddings/　#vocab_size = 10000,num_oov_buckets=1000\n",
        "                           mask_zero=True, #モデルにパディングトークンを無視するように指示　→モデルは実際に意味のあるデータに集中できる。\n",
        "                           input_shape=[None]),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.GRU(128),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, epochs=5)\n",
        "\n",
        "#GRUの説明\n",
        "#https://keras.io/ja/layers/recurrent/\n",
        "#https://wayama.io/article/ml/lec/text/gru/\n",
        "#GRUはLSTMのパラメタの多さ、すなわち計算コストが高いという欠点を補うために作られたモデルです。\n",
        "\n",
        "#https://note.nkmk.me/python-tensorflow-keras-basics/\n",
        "#生成したモデルに訓練（学習）プロセスを設定するにはcompile()を使う。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXjVfwQufVCx"
      },
      "source": [
        "Or using manual masking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-08jazN2fVCx",
        "outputId": "5a8ae9f4-0ead-43ac-ee92-21d7a7073aa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 23s 20ms/step - loss: 0.5426 - accuracy: 0.7156\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 17s 22ms/step - loss: 0.3469 - accuracy: 0.8572\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 15s 19ms/step - loss: 0.1758 - accuracy: 0.9388\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 17s 22ms/step - loss: 0.1320 - accuracy: 0.9520\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 15s 20ms/step - loss: 0.1109 - accuracy: 0.9596\n"
          ]
        }
      ],
      "source": [
        "#マスキング\n",
        "#Embedding層を作るときにmask_zero=Trueとしたが、今回は、Embedding 層がK.not_equal(inputs, 0)を使用している\n",
        "#パディングトークン について　https://qiita.com/takubb/items/fd972f0ac3dba909c293\n",
        "\n",
        "#このままの状態では、モデルはパディングトークンを無視することを学習しなければならない。\n",
        "#しかし、それは訓練前からわかっていることである。モデルにパディングトークンを無視するよう\n",
        "#に指示すれば、モデルは実際に意味のあるデータに集中できる。指示は簡単なことで、Embedding\n",
        "#層を作るときにmask_zero=True を指示すればよい。こうすると、パディングトークン（ID は、下流のすべての層で無視されるようになる。\n",
        "\n",
        "\n",
        "#先ほどのモデルと同じだが、関数型APIを使ってマスキングを手作業している。\n",
        "#https://tensorflow.classcat.com/2021/10/23/keras-2-guide-functional-api/\n",
        "K = keras.backend\n",
        "embed_size = 128\n",
        "inputs = keras.layers.Input(shape=[None])\n",
        "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)#K.not_equal(inputs, 0)を使用\n",
        "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
        "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
        "z = keras.layers.GRU(128)(z, mask=mask)\n",
        "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
        "model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, epochs=5)\n",
        "\n",
        "#K.not_equal(inputs, 0)（ただし、K = keras.backend）と等しいマスクテンソル（mask tensor）を作る。マスクテンソルは入力と同じ形の論\n",
        "#理テンソルで、単語IDが0 のところはFalse、それ以外のところはTrueになっている"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEji3rU3fVCx"
      },
      "source": [
        "## Reusing Pretrained Embeddings（事前学習済みの埋め込みの再利用）\n",
        "\n",
        "事前訓練済みモデルのコンポーネントは、TensorFlow Hubプロジェクトにより、独自モデルで\n",
        "簡単に再利用できるようになっている。再利用できるコンポーネントはモジュール（module）と\n",
        "呼ばれている。単純にTF Hubリポジトリ（https://tfhub.dev）をブラウズし、必要なものを見つ\n",
        "けてプロジェクトにコード例をコピーすれば、モジュールは事前訓練済みの重みとともに自動的に\n",
        "16.2 感情分析537\n",
        "ダウンロードされ、モデルに組み込まれる。\n",
        "\n",
        "たとえば、私たちの感情分析モデルでnnlm-en-dim50 文埋め込みモジュールVer.1 を再利用し\n",
        "てみよう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t2oXrN4fVCx"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_duqSVxfVCx"
      },
      "outputs": [],
      "source": [
        "TFHUB_CACHE_DIR = os.path.join(os.curdir, \"my_tfhub_cache\")\n",
        "os.environ[\"TFHUB_CACHE_DIR\"] = TFHUB_CACHE_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9G0rK346fVCx"
      },
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "model = keras.Sequential([\n",
        "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
        "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
        "    keras.layers.Dense(128, activation=\"relu\"),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "#hub.KerasLayer 層は、指定されたURLからモジュールをダウンロードする。このモジュール\n",
        "#は文エンコーダ（sentence encoder）であり、文字列を入力として、個々の文字列を1つのベクトルの形にエンコードする（この場合は、50 次元のベクトル）。\n",
        "##その後ろに、2つの単純なDense層を追加すると、優れた文解析モデルが作れる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wjMQzyPfVCx",
        "outputId": "6192d563-2c2e-4000-f69a-f65cdbf66486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe.descriptor.txt\n",
            "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/saved_model.pb\n",
            "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/variables/variables.index\n",
            "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/variables/variables.data-00000-of-00001\n",
            "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/assets/tokens.txt\n"
          ]
        }
      ],
      "source": [
        "#フォルダパスの確認\n",
        "for dirpath, dirnames, filenames in os.walk(TFHUB_CACHE_DIR):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirpath, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN93vRXUfVCx",
        "outputId": "8168171b-44e4-4628-ddce-f828d95be1da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.5461 - accuracy: 0.7267\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.5130 - accuracy: 0.7495\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 0.5081 - accuracy: 0.7532\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.5047 - accuracy: 0.7540\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.5018 - accuracy: 0.7566\n"
          ]
        }
      ],
      "source": [
        "#IMDbレビューデータセットをロードし、直接モデルを訓練する。データセットの前処理は不要である。（バッチへの分割とプリフェッチを除き）\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
        "train_size = info.splits[\"train\"].num_examples\n",
        "batch_size = 32\n",
        "train_set = datasets[\"train\"].batch(batch_size).prefetch(1)\n",
        "history = model.fit(train_set, epochs=5)#先ほど読み込んだ事前学習済みモデルをつかう\n",
        "\n",
        "\n",
        "#https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\n",
        "\n",
        "#TF Hub モジュールのURLの最後の部分で、モデルのVer.1 を指定していたことに注意しよう。このようにバージョンを指定することにより、モジュールの新バージョンがリリースされても、それでモデルが動作しなくなることを避けられる。\n",
        "#好都合なことに、ウェブブラウザでのこのURLを入力すると、このモジュールのドキュメントが読める。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BLkegXNTGLv"
      },
      "source": [
        "今までは時系列データ、文字RNNを使ったテキスト生成、単語レベルRNNモデルを使った感\n",
        "情分析、独自単語埋め込みの訓練と事前学習済みの埋め込みの再利用について説明してきた。次は\n",
        "NLPのもう1 つの重要タスクであるニューラル機械翻訳（NMT：neural machine translation）に\n",
        "ついて見てみよう。最初は純粋なエンコーダ－デコーダ・モデルを使い、次に注意機構を使ってそ\n",
        "れを改良する。最後に、Transformerというとてつもないアーキテクチャを紹介する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_2NAMq7fVCx"
      },
      "source": [
        "## Automatic Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-q1jNjrfVCx"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFE8qUe-fVCx"
      },
      "outputs": [],
      "source": [
        "vocab_size = 100\n",
        "embed_size = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipFpR3FlfVCx"
      },
      "outputs": [],
      "source": [
        "import tensorflow_addons as tfa\n",
        "\n",
        "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
        "\n",
        "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
        "encoder_embeddings = embeddings(encoder_inputs)\n",
        "decoder_embeddings = embeddings(decoder_inputs)\n",
        "\n",
        "encoder = keras.layers.LSTM(512, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
        "encoder_state = [state_h, state_c]\n",
        "\n",
        "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "\n",
        "decoder_cell = keras.layers.LSTMCell(512)\n",
        "output_layer = keras.layers.Dense(vocab_size)\n",
        "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,\n",
        "                                                 output_layer=output_layer)\n",
        "final_outputs, final_state, final_sequence_lengths = decoder(\n",
        "    decoder_embeddings, initial_state=encoder_state,\n",
        "    sequence_length=sequence_lengths)\n",
        "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
        "\n",
        "model = keras.models.Model(\n",
        "    inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
        "    outputs=[Y_proba])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut8ZZ-AyfVCx"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeKyfG0IfVCx",
        "outputId": "d8286dea-f761-46ae-e994-865c3c8f22bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "32/32 [==============================] - 6s 6ms/sample - loss: 4.6053\n",
            "Epoch 2/2\n",
            "32/32 [==============================] - 3s 3ms/sample - loss: 4.6031\n"
          ]
        }
      ],
      "source": [
        "X = np.random.randint(100, size=10*1000).reshape(1000, 10)\n",
        "Y = np.random.randint(100, size=15*1000).reshape(1000, 15)\n",
        "X_decoder = np.c_[np.zeros((1000, 1)), Y[:, :-1]]\n",
        "seq_lengths = np.full([1000], 15)\n",
        "\n",
        "history = model.fit([X, X_decoder, seq_lengths], Y, epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7TTkoagfVCx"
      },
      "source": [
        "### Bidirectional Recurrent Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd64OQsNfVCx",
        "outputId": "2d303186-ce00-4dcf-b002-4451e85b414d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_10 (GRU)                 (None, None, 10)          660       \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 20)          1320      \n",
            "=================================================================\n",
            "Total params: 1,980\n",
            "Trainable params: 1,980\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(10, return_sequences=True, input_shape=[None, 10]),\n",
        "    keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EGKSC-nfVCx"
      },
      "source": [
        "### Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrPwq5vXfVCx"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(keras.layers.Layer):\n",
        "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        if max_dims % 2 == 1: max_dims += 1 # max_dims must be even\n",
        "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
        "        pos_emb = np.empty((1, max_steps, max_dims))\n",
        "        pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
        "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
        "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
        "    def call(self, inputs):\n",
        "        shape = tf.shape(inputs)\n",
        "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "574AXVQDfVCy"
      },
      "outputs": [],
      "source": [
        "max_steps = 201\n",
        "max_dims = 512\n",
        "pos_emb = PositionalEncoding(max_steps, max_dims)\n",
        "PE = pos_emb(np.zeros((1, max_steps, max_dims), np.float32))[0].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEHesvfDfVCy",
        "outputId": "19288b79-f6ed-40a6-86c3-fa3d4711969c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 648x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "i1, i2, crop_i = 100, 101, 150\n",
        "p1, p2, p3 = 22, 60, 35\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(9, 5))\n",
        "ax1.plot([p1, p1], [-1, 1], \"k--\", label=\"$p = {}$\".format(p1))\n",
        "ax1.plot([p2, p2], [-1, 1], \"k--\", label=\"$p = {}$\".format(p2), alpha=0.5)\n",
        "ax1.plot(p3, PE[p3, i1], \"bx\", label=\"$p = {}$\".format(p3))\n",
        "ax1.plot(PE[:,i1], \"b-\", label=\"$i = {}$\".format(i1))\n",
        "ax1.plot(PE[:,i2], \"r-\", label=\"$i = {}$\".format(i2))\n",
        "ax1.plot([p1, p2], [PE[p1, i1], PE[p2, i1]], \"bo\")\n",
        "ax1.plot([p1, p2], [PE[p1, i2], PE[p2, i2]], \"ro\")\n",
        "ax1.legend(loc=\"center right\", fontsize=14, framealpha=0.95)\n",
        "ax1.set_ylabel(\"$P_{(p,i)}$\", rotation=0, fontsize=16)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.hlines(0, 0, max_steps - 1, color=\"k\", linewidth=1, alpha=0.3)\n",
        "ax1.axis([0, max_steps - 1, -1, 1])\n",
        "ax2.imshow(PE.T[:crop_i], cmap=\"gray\", interpolation=\"bilinear\", aspect=\"auto\")\n",
        "ax2.hlines(i1, 0, max_steps - 1, color=\"b\")\n",
        "cheat = 2 # need to raise the red line a bit, or else it hides the blue one\n",
        "ax2.hlines(i2+cheat, 0, max_steps - 1, color=\"r\")\n",
        "ax2.plot([p1, p1], [0, crop_i], \"k--\")\n",
        "ax2.plot([p2, p2], [0, crop_i], \"k--\", alpha=0.5)\n",
        "ax2.plot([p1, p2], [i2+cheat, i2+cheat], \"ro\")\n",
        "ax2.plot([p1, p2], [i1, i1], \"bo\")\n",
        "ax2.axis([0, max_steps - 1, 0, crop_i])\n",
        "ax2.set_xlabel(\"$p$\", fontsize=16)\n",
        "ax2.set_ylabel(\"$i$\", rotation=0, fontsize=16)\n",
        "save_fig(\"positional_embedding_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naD7hULWfVCy"
      },
      "outputs": [],
      "source": [
        "embed_size = 512; max_steps = 500; vocab_size = 10000\n",
        "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
        "encoder_embeddings = embeddings(encoder_inputs)\n",
        "decoder_embeddings = embeddings(decoder_inputs)\n",
        "positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\n",
        "encoder_in = positional_encoding(encoder_embeddings)\n",
        "decoder_in = positional_encoding(decoder_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzPOwuKcfVCy"
      },
      "source": [
        "Here is a (very) simplified Transformer (the actual architecture has skip connections, layer norm, dense nets, and most importantly it uses Multi-Head Attention instead of regular Attention):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlo9ONYPfVCy"
      },
      "outputs": [],
      "source": [
        "Z = encoder_in\n",
        "for N in range(6):\n",
        "    Z = keras.layers.Attention(use_scale=True)([Z, Z])\n",
        "\n",
        "encoder_outputs = Z\n",
        "Z = decoder_in\n",
        "for N in range(6):\n",
        "    Z = keras.layers.Attention(use_scale=True, causal=True)([Z, Z])\n",
        "    Z = keras.layers.Attention(use_scale=True)([Z, encoder_outputs])\n",
        "\n",
        "outputs = keras.layers.TimeDistributed(\n",
        "    keras.layers.Dense(vocab_size, activation=\"softmax\"))(Z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RdeeSZxfVCy"
      },
      "source": [
        "Here's a basic implementation of the `MultiHeadAttention` layer. One will likely be added to `keras.layers` in the near future. Note that `Conv1D` layers with `kernel_size=1` (and the default `padding=\"valid\"` and `strides=1`) is equivalent to a `TimeDistributed(Dense(...))` layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxJOHa9dfVCy"
      },
      "outputs": [],
      "source": [
        "K = keras.backend\n",
        "\n",
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "    def __init__(self, n_heads, causal=False, use_scale=False, **kwargs):\n",
        "        self.n_heads = n_heads\n",
        "        self.causal = causal\n",
        "        self.use_scale = use_scale\n",
        "        super().__init__(**kwargs)\n",
        "    def build(self, batch_input_shape):\n",
        "        self.dims = batch_input_shape[0][-1]\n",
        "        self.q_dims, self.v_dims, self.k_dims = [self.dims // self.n_heads] * 3 # could be hyperparameters instead\n",
        "        self.q_linear = keras.layers.Conv1D(self.n_heads * self.q_dims, kernel_size=1, use_bias=False)\n",
        "        self.v_linear = keras.layers.Conv1D(self.n_heads * self.v_dims, kernel_size=1, use_bias=False)\n",
        "        self.k_linear = keras.layers.Conv1D(self.n_heads * self.k_dims, kernel_size=1, use_bias=False)\n",
        "        self.attention = keras.layers.Attention(causal=self.causal, use_scale=self.use_scale)\n",
        "        self.out_linear = keras.layers.Conv1D(self.dims, kernel_size=1, use_bias=False)\n",
        "        super().build(batch_input_shape)\n",
        "    def _multi_head_linear(self, inputs, linear):\n",
        "        shape = K.concatenate([K.shape(inputs)[:-1], [self.n_heads, -1]])\n",
        "        projected = K.reshape(linear(inputs), shape)\n",
        "        perm = K.permute_dimensions(projected, [0, 2, 1, 3])\n",
        "        return K.reshape(perm, [shape[0] * self.n_heads, shape[1], -1])\n",
        "    def call(self, inputs):\n",
        "        q = inputs[0]\n",
        "        v = inputs[1]\n",
        "        k = inputs[2] if len(inputs) > 2 else v\n",
        "        shape = K.shape(q)\n",
        "        q_proj = self._multi_head_linear(q, self.q_linear)\n",
        "        v_proj = self._multi_head_linear(v, self.v_linear)\n",
        "        k_proj = self._multi_head_linear(k, self.k_linear)\n",
        "        multi_attended = self.attention([q_proj, v_proj, k_proj])\n",
        "        shape_attended = K.shape(multi_attended)\n",
        "        reshaped_attended = K.reshape(multi_attended, [shape[0], self.n_heads, shape_attended[1], shape_attended[2]])\n",
        "        perm = K.permute_dimensions(reshaped_attended, [0, 2, 1, 3])\n",
        "        concat = K.reshape(perm, [shape[0], shape_attended[1], -1])\n",
        "        return self.out_linear(concat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkgU_OYhfVCy",
        "outputId": "e6dc361c-ceb0-4276-8d92-282828c9a9c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer multi_head_attention is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer multi_head_attention is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TensorShape([2, 50, 512])"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Q = np.random.rand(2, 50, 512)\n",
        "V = np.random.rand(2, 80, 512)\n",
        "multi_attn = MultiHeadAttention(8)\n",
        "multi_attn([Q, V]).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMVKjCB4fVCy"
      },
      "source": [
        "# Exercise solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1o3R1DufVCy"
      },
      "source": [
        "## 1. to 7."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bnY2TxTfVCy"
      },
      "source": [
        "See Appendix A."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02HT81gifVCy"
      },
      "source": [
        "## 8.\n",
        "_Exercise:_ Embedded Reber grammars _were used by Hochreiter and Schmidhuber in [their paper](https://homl.info/93) about LSTMs. They are artificial grammars that produce strings such as \"BPBTSXXVPSEPE.\" Check out Jenny Orr's [nice introduction](https://homl.info/108) to this topic. Choose a particular embedded Reber grammar (such as the one represented on Jenny Orr's page), then train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar, and 50% that don't._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxVPVaJ2fVCy"
      },
      "source": [
        "First we need to build a function that generates strings based on a grammar. The grammar will be represented as a list of possible transitions for each state. A transition specifies the string to output (or a grammar to generate it) and the next state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zTJ67ECfVCy"
      },
      "outputs": [],
      "source": [
        "default_reber_grammar = [\n",
        "    [(\"B\", 1)],           # (state 0) =B=>(state 1)\n",
        "    [(\"T\", 2), (\"P\", 3)], # (state 1) =T=>(state 2) or =P=>(state 3)\n",
        "    [(\"S\", 2), (\"X\", 4)], # (state 2) =S=>(state 2) or =X=>(state 4)\n",
        "    [(\"T\", 3), (\"V\", 5)], # and so on...\n",
        "    [(\"X\", 3), (\"S\", 6)],\n",
        "    [(\"P\", 4), (\"V\", 6)],\n",
        "    [(\"E\", None)]]        # (state 6) =E=>(terminal state)\n",
        "\n",
        "embedded_reber_grammar = [\n",
        "    [(\"B\", 1)],\n",
        "    [(\"T\", 2), (\"P\", 3)],\n",
        "    [(default_reber_grammar, 4)],\n",
        "    [(default_reber_grammar, 5)],\n",
        "    [(\"T\", 6)],\n",
        "    [(\"P\", 6)],\n",
        "    [(\"E\", None)]]\n",
        "\n",
        "def generate_string(grammar):\n",
        "    state = 0\n",
        "    output = []\n",
        "    while state is not None:\n",
        "        index = np.random.randint(len(grammar[state]))\n",
        "        production, state = grammar[state][index]\n",
        "        if isinstance(production, list):\n",
        "            production = generate_string(grammar=production)\n",
        "        output.append(production)\n",
        "    return \"\".join(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl79cCCIfVCy"
      },
      "source": [
        "Let's generate a few strings based on the default Reber grammar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC41MUSLfVCy",
        "outputId": "b1da6af4-5627-4c90-daf6-c811a4521953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BTXXTTVPXTVPXTTVPSE BPVPSE BTXSE BPVVE BPVVE BTSXSE BPTVPXTTTVVE BPVVE BTXSE BTXXVPSE BPTTTTTTTTVVE BTXSE BPVPSE BTXSE BPTVPSE BTXXTVPSE BPVVE BPVVE BPVVE BPTTVVE BPVVE BPVVE BTXXVVE BTXXVVE BTXXVPXVVE "
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "for _ in range(25):\n",
        "    print(generate_string(default_reber_grammar), end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKGR-5v5fVCy"
      },
      "source": [
        "Looks good. Now let's generate a few strings based on the embedded Reber grammar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRThCCZafVCy",
        "outputId": "29e11b55-c827-42a5-f6bd-9c407a9207ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BTBPTTTVPXTVPXTTVPSETE BPBPTVPSEPE BPBPVVEPE BPBPVPXVVEPE BPBTXXTTTTVVEPE BPBPVPSEPE BPBTXXVPSEPE BPBTSSSSSSSXSEPE BTBPVVETE BPBTXXVVEPE BPBTXXVPSEPE BTBTXXVVETE BPBPVVEPE BPBPVVEPE BPBTSXSEPE BPBPVVEPE BPBPTVPSEPE BPBTXXVVEPE BTBPTVPXVVETE BTBPVVETE BTBTSSSSSSSXXVVETE BPBTSSSXXTTTTVPSEPE BTBPTTVVETE BPBTXXTVVEPE BTBTXSETE "
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "for _ in range(25):\n",
        "    print(generate_string(embedded_reber_grammar), end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9qNEWlafVCz"
      },
      "source": [
        "Okay, now we need a function to generate strings that do not respect the grammar. We could generate a random string, but the task would be a bit too easy, so instead we will generate a string that respects the grammar, and we will corrupt it by changing just one character:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCbgEqLwfVCz"
      },
      "outputs": [],
      "source": [
        "POSSIBLE_CHARS = \"BEPSTVX\"\n",
        "\n",
        "def generate_corrupted_string(grammar, chars=POSSIBLE_CHARS):\n",
        "    good_string = generate_string(grammar)\n",
        "    index = np.random.randint(len(good_string))\n",
        "    good_char = good_string[index]\n",
        "    bad_char = np.random.choice(sorted(set(chars) - set(good_char)))\n",
        "    return good_string[:index] + bad_char + good_string[index + 1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRTmo19zfVCz"
      },
      "source": [
        "Let's look at a few corrupted strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGzi87JffVCz",
        "outputId": "dce0d5d2-72a3-4ea5-f4e9-37613b98227f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BTBPTTTPPXTVPXTTVPSETE BPBTXEEPE BPBPTVVVEPE BPBTSSSSXSETE BPTTXSEPE BTBPVPXTTTTTTEVETE BPBTXXSVEPE BSBPTTVPSETE BPBXVVEPE BEBTXSETE BPBPVPSXPE BTBPVVVETE BPBTSXSETE BPBPTTTPTTTTTVPSEPE BTBTXXTTSTVPSETE BBBTXSETE BPBTPXSEPE BPBPVPXTTTTVPXTVPXVPXTTTVVEVE BTBXXXTVPSETE BEBTSSSSSXXVPXTVVETE BTBXTTVVETE BPBTXSTPE BTBTXXTTTVPSBTE BTBTXSETX BTBTSXSSTE "
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "for _ in range(25):\n",
        "    print(generate_corrupted_string(embedded_reber_grammar), end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBh6Rt7mfVCz"
      },
      "source": [
        "We cannot feed strings directly to an RNN, so we need to encode them somehow. One option would be to one-hot encode each character. Another option is to use embeddings. Let's go for the second option (but since there are just a handful of characters, one-hot encoding would probably be a good option as well). For embeddings to work, we need to convert each string into a sequence of character IDs. Let's write a function for that, using each character's index in the string of possible characters \"BEPSTVX\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AMeHUVRfVCz"
      },
      "outputs": [],
      "source": [
        "def string_to_ids(s, chars=POSSIBLE_CHARS):\n",
        "    return [chars.index(c) for c in s]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5cIufuAfVCz",
        "outputId": "26e46dbe-e1bf-4fea-ff0a-dccca1649cb4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 4, 4, 4, 6, 6, 5, 5, 1, 4, 1]"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "string_to_ids(\"BTTTXXVVETE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMlN1jBefVCz"
      },
      "source": [
        "We can now generate the dataset, with 50% good strings, and 50% bad strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5eqIQWQfVCz"
      },
      "outputs": [],
      "source": [
        "def generate_dataset(size):\n",
        "    good_strings = [string_to_ids(generate_string(embedded_reber_grammar))\n",
        "                    for _ in range(size // 2)]\n",
        "    bad_strings = [string_to_ids(generate_corrupted_string(embedded_reber_grammar))\n",
        "                   for _ in range(size - size // 2)]\n",
        "    all_strings = good_strings + bad_strings\n",
        "    X = tf.ragged.constant(all_strings, ragged_rank=1)\n",
        "    y = np.array([[1.] for _ in range(len(good_strings))] +\n",
        "                 [[0.] for _ in range(len(bad_strings))])\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wYTD7VifVCz"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "X_train, y_train = generate_dataset(10000)\n",
        "X_valid, y_valid = generate_dataset(2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJdvU9FofVCz"
      },
      "source": [
        "Let's take a look at the first training sequence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbT-Y6ctfVCz",
        "outputId": "6e695d3d-2b04-4f3e-e37a-be315ee7e3c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(22,), dtype=int32, numpy=\n",
              "array([0, 4, 0, 2, 4, 4, 4, 5, 2, 6, 4, 5, 2, 6, 4, 4, 5, 2, 3, 1, 4, 1],\n",
              "      dtype=int32)>"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn8xn0grfVCz"
      },
      "source": [
        "What class does it belong to?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFZJq3IYfVCz",
        "outputId": "2f2cc1ac-4832-44c1-e3da-d5d0be082728"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.])"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXMocVsXfVCz"
      },
      "source": [
        "Perfect! We are ready to create the RNN to identify good strings. We build a simple sequence binary classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4cU5E8NfVCz",
        "outputId": "bf227b88-bc54-40a2-df18-34cf89924c61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ageron/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/Users/ageron/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 5s 42us/sample - loss: 0.6847 - accuracy: 0.5138 - val_loss: 8.1518 - val_accuracy: 0.6115\n",
            "Epoch 2/20\n",
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 3s 28us/sample - loss: 0.6524 - accuracy: 0.5571 - val_loss: 7.9259 - val_accuracy: 0.6085\n",
            "Epoch 3/20\n",
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 3s 28us/sample - loss: 0.6686 - accuracy: 0.5783 - val_loss: 7.7483 - val_accuracy: 0.6110\n",
            "Epoch 4/20\n",
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 3s 28us/sample - loss: 0.6201 - accuracy: 0.5969 - val_loss: 7.5567 - val_accuracy: 0.6110\n",
            "Epoch 5/20\n",
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 3s 28us/sample - loss: 0.5705 - accuracy: 0.6428 - val_loss: 6.9117 - val_accuracy: 0.7075\n",
            "Epoch 6/20\n",
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 3s 29us/sample - loss: 0.5660 - accuracy: 0.7008 - val_loss: 5.7277 - val_accuracy: 0.7580\n",
            "Epoch 7/20\n",
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 3s 28us/sample - loss: 0.3997 - accuracy: 0.8336 - val_loss: 4.3641 - val_accuracy: 0.8550\n",
            "Epoch 8/20\n",
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 3s 29us/sample - loss: 0.1771 - accuracy: 0.8958 - val_loss: 1.5009 - val_accuracy: 0.9605\n",
            "Epoch 9/20\n",
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 3s 29us/sample - loss: 0.2710 - accuracy: 0.9566 - val_loss: 3.2648 - val_accuracy: 0.9005\n",
            "Epoch 10/20\n",
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 3s 29us/sample - loss: 0.2574 - accuracy: 0.9620 - val_loss: 1.0385 - val_accuracy: 0.9790\n",
            "Epoch 11/20\n",
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 3s 29us/sample - loss: 0.0356 - accuracy: 0.9845 - val_loss: 0.1081 - val_accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 4s 29us/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 3s 29us/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0144 - val_accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 3s 28us/sample - loss: 8.1710e-04 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 3s 29us/sample - loss: 5.8225e-04 - accuracy: 1.0000 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 3s 29us/sample - loss: 5.8369e-04 - accuracy: 1.0000 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 4s 30us/sample - loss: 3.8744e-04 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 4s 29us/sample - loss: 4.2988e-04 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 4s 29us/sample - loss: 2.7449e-04 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "313/313 [========================================================================================================================================================================================================================================================================================================================================================================] - 3s 29us/sample - loss: 2.9469e-04 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "embedding_size = 5\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.InputLayer(input_shape=[None], dtype=tf.int32, ragged=True),\n",
        "    keras.layers.Embedding(input_dim=len(POSSIBLE_CHARS), output_dim=embedding_size),\n",
        "    keras.layers.GRU(30),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.02, momentum = 0.95, nesterov=True)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ45scZJfVCz"
      },
      "source": [
        "Now let's test our RNN on two tricky strings: the first one is bad while the second one is good. They only differ by the second to last character. If the RNN gets this right, it shows that it managed to notice the pattern that the second letter should always be equal to the second to last letter. That requires a fairly long short-term memory (which is the reason why we used a GRU cell)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSDX5lVZfVCz",
        "outputId": "fe2145b5-7016-4ca5-8369-efcbfb2861c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Estimated probability that these are Reber strings:\n",
            "BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE: 0.40%\n",
            "BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE: 99.96%\n"
          ]
        }
      ],
      "source": [
        "test_strings = [\"BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE\",\n",
        "                \"BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE\"]\n",
        "X_test = tf.ragged.constant([string_to_ids(s) for s in test_strings], ragged_rank=1)\n",
        "\n",
        "y_proba = model.predict(X_test)\n",
        "print()\n",
        "print(\"Estimated probability that these are Reber strings:\")\n",
        "for index, string in enumerate(test_strings):\n",
        "    print(\"{}: {:.2f}%\".format(string, 100 * y_proba[index][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkFe2fZwfVCz"
      },
      "source": [
        "Ta-da! It worked fine. The RNN found the correct answers with very high confidence. :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DLFqXXMfVCz"
      },
      "source": [
        "## 9.\n",
        "_Exercise: Train an Encoder–Decoder model that can convert a date string from one format to another (e.g., from \"April 22, 2019\" to \"2019-04-22\")._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fFlHj7DfVCz"
      },
      "source": [
        "Let's start by creating the dataset. We will use random days between 1000-01-01 and 9999-12-31:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fomwjxi4fVCz"
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "\n",
        "# cannot use strftime()'s %B format since it depends on the locale\n",
        "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
        "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
        "\n",
        "def random_dates(n_dates):\n",
        "    min_date = date(1000, 1, 1).toordinal()\n",
        "    max_date = date(9999, 12, 31).toordinal()\n",
        "\n",
        "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
        "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
        "\n",
        "    x = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n",
        "    y = [dt.isoformat() for dt in dates]\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdEWNQ7ufVC0"
      },
      "source": [
        "Here are a few random dates, displayed in both the input format and the target format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o7fjF6VfVC0",
        "outputId": "e4ccef5a-2245-46f5-c707-a15eda511eb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input                    Target                   \n",
            "--------------------------------------------------\n",
            "September 20, 7075       7075-09-20               \n",
            "May 15, 8579             8579-05-15               \n",
            "January 11, 7103         7103-01-11               \n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "n_dates = 3\n",
        "x_example, y_example = random_dates(n_dates)\n",
        "print(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\n",
        "print(\"-\" * 50)\n",
        "for idx in range(n_dates):\n",
        "    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNTP0dhNfVC0"
      },
      "source": [
        "Let's get the list of all possible characters in the inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK4Trxb6fVC0",
        "outputId": "c82e747d-65f0-41b1-81de-25a3eaa18e2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' ,0123456789ADFJMNOSabceghilmnoprstuvy'"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS) + \"0123456789, \")))\n",
        "INPUT_CHARS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVNIiO3UfVC0"
      },
      "source": [
        "And here's the list of possible characters in the outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jK0qPVb1fVC0"
      },
      "outputs": [],
      "source": [
        "OUTPUT_CHARS = \"0123456789-\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1LY93_BfVC0"
      },
      "source": [
        "Let's write a function to convert a string to a list of character IDs, as we did in the previous exercise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rai79ddpfVC0"
      },
      "outputs": [],
      "source": [
        "def date_str_to_ids(date_str, chars=INPUT_CHARS):\n",
        "    return [chars.index(c) for c in date_str]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0JbRB2HfVC0",
        "outputId": "150a9eb6-1933-465b-dfed-f2a3f3bd7432"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[7, 11, 19, 22, 11, 16, 9, 11, 20, 38, 28, 26, 37, 38, 33, 26, 33, 31]"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "date_str_to_ids(x_example[0], INPUT_CHARS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPAdBRbGfVC0",
        "outputId": "c8106cd7-3095-43cc-829e-b5bba1ee1cf8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[7, 0, 7, 5, 10, 0, 9, 10, 2, 0]"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "date_str_to_ids(y_example[0], OUTPUT_CHARS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw8ONCkwfVC0"
      },
      "outputs": [],
      "source": [
        "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
        "    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n",
        "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
        "    return (X + 1).to_tensor() # using 0 as the padding token ID\n",
        "\n",
        "def create_dataset(n_dates):\n",
        "    x, y = random_dates(n_dates)\n",
        "    return prepare_date_strs(x, INPUT_CHARS), prepare_date_strs(y, OUTPUT_CHARS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGjU3fDUfVC0"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "X_train, Y_train = create_dataset(10000)\n",
        "X_valid, Y_valid = create_dataset(2000)\n",
        "X_test, Y_test = create_dataset(2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKk-6_0efVC0",
        "outputId": "6a8fab4d-591a-4f28-dc1f-f2579c689f4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 8,  1,  8,  6, 11,  1, 10, 11,  3,  1], dtype=int32)>"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhdoOdjHfVC0"
      },
      "source": [
        "### First version: a very basic seq2seq model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoN9X39BfVC0"
      },
      "source": [
        "Let's first try the simplest possible model: we feed in the input sequence, which first goes through the encoder (an embedding layer followed by a single LSTM layer), which outputs a vector, then it goes through a decoder (a single LSTM layer, followed by a dense output layer), which outputs a sequence of vectors, each representing the estimated probabilities for all possible output character.\n",
        "\n",
        "Since the decoder expects a sequence as input, we repeat the vector (which is output by the encoder) as many times as the longest possible output sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16eZLt7KfVC0",
        "outputId": "610128cb-371b-471b-8872-821ed1dfc186"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 1.8111 - accuracy: 0.3533 - val_loss: 1.3581 - val_accuracy: 0.4965\n",
            "Epoch 2/20\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 1.3518 - accuracy: 0.5103 - val_loss: 1.1915 - val_accuracy: 0.5694\n",
            "Epoch 3/20\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 1.1706 - accuracy: 0.5908 - val_loss: 0.9983 - val_accuracy: 0.6398\n",
            "Epoch 4/20\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.9158 - accuracy: 0.6686 - val_loss: 0.8012 - val_accuracy: 0.6987\n",
            "Epoch 5/20\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.7058 - accuracy: 0.7308 - val_loss: 0.6224 - val_accuracy: 0.7599\n",
            "Epoch 6/20\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.7756 - accuracy: 0.7203 - val_loss: 0.6541 - val_accuracy: 0.7599\n",
            "Epoch 7/20\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.5379 - accuracy: 0.8034 - val_loss: 0.4174 - val_accuracy: 0.8440\n",
            "Epoch 8/20\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.4867 - accuracy: 0.8262 - val_loss: 0.4188 - val_accuracy: 0.8480\n",
            "Epoch 9/20\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.2979 - accuracy: 0.8951 - val_loss: 0.2549 - val_accuracy: 0.9126\n",
            "Epoch 10/20\n",
            "313/313 [==============================] - 5s 14ms/step - loss: 0.1785 - accuracy: 0.9479 - val_loss: 0.1461 - val_accuracy: 0.9594\n",
            "Epoch 11/20\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.1830 - accuracy: 0.9557 - val_loss: 0.1644 - val_accuracy: 0.9550\n",
            "Epoch 12/20\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0775 - accuracy: 0.9857 - val_loss: 0.0595 - val_accuracy: 0.9901\n",
            "Epoch 13/20\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0400 - accuracy: 0.9953 - val_loss: 0.0342 - val_accuracy: 0.9957\n",
            "Epoch 14/20\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0248 - accuracy: 0.9979 - val_loss: 0.0231 - val_accuracy: 0.9983\n",
            "Epoch 15/20\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0161 - accuracy: 0.9991 - val_loss: 0.0149 - val_accuracy: 0.9995\n",
            "Epoch 16/20\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0108 - accuracy: 0.9997 - val_loss: 0.0106 - val_accuracy: 0.9996\n",
            "Epoch 17/20\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0074 - accuracy: 0.9999 - val_loss: 0.0077 - val_accuracy: 0.9999\n",
            "Epoch 18/20\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 0.9999\n",
            "Epoch 19/20\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "embedding_size = 32\n",
        "max_output_length = Y_train.shape[1]\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "encoder = keras.models.Sequential([\n",
        "    keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1,\n",
        "                           output_dim=embedding_size,\n",
        "                           input_shape=[None]),\n",
        "    keras.layers.LSTM(128)\n",
        "])\n",
        "\n",
        "decoder = keras.models.Sequential([\n",
        "    keras.layers.LSTM(128, return_sequences=True),\n",
        "    keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    encoder,\n",
        "    keras.layers.RepeatVector(max_output_length),\n",
        "    decoder\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.Nadam()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, Y_train, epochs=20,\n",
        "                    validation_data=(X_valid, Y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq_YFSZxfVC0"
      },
      "source": [
        "Looks great, we reach 100% validation accuracy! Let's use the model to make some predictions. We will need to be able to convert a sequence of character IDs to a readable string:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFwPx7zRfVC0"
      },
      "outputs": [],
      "source": [
        "def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n",
        "    return [\"\".join([(\"?\" + chars)[index] for index in sequence])\n",
        "            for sequence in ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYMFUoDkfVC0"
      },
      "source": [
        "Now we can use the model to convert some dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HkHTIfhfVC1"
      },
      "outputs": [],
      "source": [
        "X_new = prepare_date_strs([\"September 17, 2009\", \"July 14, 1789\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyJpXKurfVC1",
        "outputId": "3381abae-2193-4cbd-871b-9deaa9e3be8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2009-09-17\n",
            "1789-07-14\n"
          ]
        }
      ],
      "source": [
        "#ids = model.predict_classes(X_new)\n",
        "ids = np.argmax(model.predict(X_new), axis=-1)\n",
        "for date_str in ids_to_date_strs(ids):\n",
        "    print(date_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX34tdHrfVC1"
      },
      "source": [
        "Perfect! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tQVFL1nfVC1"
      },
      "source": [
        "However, since the model was only trained on input strings of length 18 (which is the length of the longest date), it does not perform well if we try to use it to make predictions on shorter sequences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTlTYgPzfVC1"
      },
      "outputs": [],
      "source": [
        "X_new = prepare_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdCXAM_kfVC1",
        "outputId": "cde2f5bc-db11-426d-a878-e054cb9a2d0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020-01-02\n",
            "1789-02-14\n"
          ]
        }
      ],
      "source": [
        "#ids = model.predict_classes(X_new)\n",
        "ids = np.argmax(model.predict(X_new), axis=-1)\n",
        "for date_str in ids_to_date_strs(ids):\n",
        "    print(date_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80s0Nrk1fVC1"
      },
      "source": [
        "Oops! We need to ensure that we always pass sequences of the same length as during training, using padding if necessary. Let's write a little helper function for that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2njkWtjfVC1"
      },
      "outputs": [],
      "source": [
        "max_input_length = X_train.shape[1]\n",
        "\n",
        "def prepare_date_strs_padded(date_strs):\n",
        "    X = prepare_date_strs(date_strs)\n",
        "    if X.shape[1] < max_input_length:\n",
        "        X = tf.pad(X, [[0, 0], [0, max_input_length - X.shape[1]]])\n",
        "    return X\n",
        "\n",
        "def convert_date_strs(date_strs):\n",
        "    X = prepare_date_strs_padded(date_strs)\n",
        "    #ids = model.predict_classes(X)\n",
        "    ids = np.argmax(model.predict(X), axis=-1)\n",
        "    return ids_to_date_strs(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5uMseI6fVC1",
        "outputId": "61351673-04c2-4a7c-e223-b18a99d5d4a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['2020-05-02', '1789-07-14']"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "convert_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yUh2RHJfVC1"
      },
      "source": [
        "Cool! Granted, there are certainly much easier ways to write a date conversion tool (e.g., using regular expressions or even basic string manipulation), but you have to admit that using neural networks is way cooler. ;-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlPMwXVBfVC1"
      },
      "source": [
        "However, real-life sequence-to-sequence problems will usually be harder, so for the sake of completeness, let's build a more powerful model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUGzUWNAfVC1"
      },
      "source": [
        "### Second version: feeding the shifted targets to the decoder (teacher forcing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4AYjYNSfVC1"
      },
      "source": [
        "Instead of feeding the decoder a simple repetition of the encoder's output vector, we can feed it the target sequence, shifted by one time step to the right. This way, at each time step the decoder will know what the previous target character was. This should help is tackle more complex sequence-to-sequence problems.\n",
        "\n",
        "Since the first output character of each target sequence has no previous character, we will need a new token to represent the start-of-sequence (sos).\n",
        "\n",
        "During inference, we won't know the target, so what will we feed the decoder? We can just predict one character at a time, starting with an sos token, then feeding the decoder all the characters that were predicted so far (we will look at this in more details later in this notebook).\n",
        "\n",
        "But if the decoder's LSTM expects to get the previous target as input at each step, how shall we pass it it the vector output by the encoder? Well, one option is to ignore the output vector, and instead use the encoder's LSTM state as the initial state of the decoder's LSTM (which requires that encoder's LSTM must have the same number of units as the decoder's LSTM).\n",
        "\n",
        "Now let's create the decoder's inputs (for training, validation and testing). The sos token will be represented using the last possible output character's ID + 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edIYMr2zfVC1"
      },
      "outputs": [],
      "source": [
        "sos_id = len(OUTPUT_CHARS) + 1\n",
        "\n",
        "def shifted_output_sequences(Y):\n",
        "    sos_tokens = tf.fill(dims=(len(Y), 1), value=sos_id)\n",
        "    return tf.concat([sos_tokens, Y[:, :-1]], axis=1)\n",
        "\n",
        "X_train_decoder = shifted_output_sequences(Y_train)\n",
        "X_valid_decoder = shifted_output_sequences(Y_valid)\n",
        "X_test_decoder = shifted_output_sequences(Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG7FM40HfVC1"
      },
      "source": [
        "Let's take a look at the decoder's training inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvPhhxMxfVC1",
        "outputId": "26708bd3-fcc2-453d-c9d0-3b2803505d5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10000, 10), dtype=int32, numpy=\n",
              "array([[12,  8,  1, ..., 10, 11,  3],\n",
              "       [12,  9,  6, ...,  6, 11,  2],\n",
              "       [12,  8,  2, ...,  2, 11,  2],\n",
              "       ...,\n",
              "       [12, 10,  8, ...,  2, 11,  4],\n",
              "       [12,  2,  2, ...,  3, 11,  3],\n",
              "       [12,  8,  9, ...,  8, 11,  3]], dtype=int32)>"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjKSuIhkfVC1"
      },
      "source": [
        "Now let's build the model. It's not a simple sequential model anymore, so let's use the functional API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MAsQYb3fVC1",
        "outputId": "72b6e3d3-1e23-4f69-b780-1a14330743b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 1.6898 - accuracy: 0.3714 - val_loss: 1.4141 - val_accuracy: 0.4603\n",
            "Epoch 2/10\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 1.2118 - accuracy: 0.5541 - val_loss: 0.9360 - val_accuracy: 0.6653\n",
            "Epoch 3/10\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.6399 - accuracy: 0.7766 - val_loss: 0.4054 - val_accuracy: 0.8631\n",
            "Epoch 4/10\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.2207 - accuracy: 0.9463 - val_loss: 0.1069 - val_accuracy: 0.9869\n",
            "Epoch 5/10\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0805 - accuracy: 0.9910 - val_loss: 0.0445 - val_accuracy: 0.9976\n",
            "Epoch 6/10\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0297 - accuracy: 0.9993 - val_loss: 0.0237 - val_accuracy: 0.9992\n",
            "Epoch 7/10\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0743 - accuracy: 0.9857 - val_loss: 0.0702 - val_accuracy: 0.9889\n",
            "Epoch 8/10\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0187 - accuracy: 0.9995 - val_loss: 0.0112 - val_accuracy: 0.9999\n",
            "Epoch 9/10\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "encoder_embedding_size = 32\n",
        "decoder_embedding_size = 32\n",
        "lstm_units = 128\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "encoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
        "encoder_embedding = keras.layers.Embedding(\n",
        "    input_dim=len(INPUT_CHARS) + 1,\n",
        "    output_dim=encoder_embedding_size)(encoder_input)\n",
        "_, encoder_state_h, encoder_state_c = keras.layers.LSTM(\n",
        "    lstm_units, return_state=True)(encoder_embedding)\n",
        "encoder_state = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "decoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
        "decoder_embedding = keras.layers.Embedding(\n",
        "    input_dim=len(OUTPUT_CHARS) + 2,\n",
        "    output_dim=decoder_embedding_size)(decoder_input)\n",
        "decoder_lstm_output = keras.layers.LSTM(lstm_units, return_sequences=True)(\n",
        "    decoder_embedding, initial_state=encoder_state)\n",
        "decoder_output = keras.layers.Dense(len(OUTPUT_CHARS) + 1,\n",
        "                                    activation=\"softmax\")(decoder_lstm_output)\n",
        "\n",
        "model = keras.models.Model(inputs=[encoder_input, decoder_input],\n",
        "                           outputs=[decoder_output])\n",
        "\n",
        "optimizer = keras.optimizers.Nadam()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit([X_train, X_train_decoder], Y_train, epochs=10,\n",
        "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYJDFKkefVC1"
      },
      "source": [
        "This model also reaches 100% validation accuracy, but it does so even faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKRCCj3OfVC1"
      },
      "source": [
        "Let's once again use the model to make some predictions. This time we need to predict characters one by one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3Tm63r6fVC1"
      },
      "outputs": [],
      "source": [
        "sos_id = len(OUTPUT_CHARS) + 1\n",
        "\n",
        "def predict_date_strs(date_strs):\n",
        "    X = prepare_date_strs_padded(date_strs)\n",
        "    Y_pred = tf.fill(dims=(len(X), 1), value=sos_id)\n",
        "    for index in range(max_output_length):\n",
        "        pad_size = max_output_length - Y_pred.shape[1]\n",
        "        X_decoder = tf.pad(Y_pred, [[0, 0], [0, pad_size]])\n",
        "        Y_probas_next = model.predict([X, X_decoder])[:, index:index+1]\n",
        "        Y_pred_next = tf.argmax(Y_probas_next, axis=-1, output_type=tf.int32)\n",
        "        Y_pred = tf.concat([Y_pred, Y_pred_next], axis=1)\n",
        "    return ids_to_date_strs(Y_pred[:, 1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqiEpC8OfVC2",
        "outputId": "afd671a4-30bf-475f-cdbd-585320f1272e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1789-07-14', '2020-05-01']"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpIasiJvfVC2"
      },
      "source": [
        "Works fine! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz677X-yfVC2"
      },
      "source": [
        "### Third version: using TF-Addons's seq2seq implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quyU5VgDfVC2"
      },
      "source": [
        "Let's build exactly the same model, but using TF-Addon's seq2seq API. The implementation below is almost very similar to the TFA example higher in this notebook, except without the model input to specify the output sequence length, for simplicity (but you can easily add it back in if you need it for your projects, when the output sequences have very different lengths)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKPWfuXRfVC2",
        "outputId": "dacfeada-5cc1-4816-895c-f9f4051ba577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 1.6757 - accuracy: 0.3683 - val_loss: 1.4602 - val_accuracy: 0.4214\n",
            "Epoch 2/15\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 1.3873 - accuracy: 0.4566 - val_loss: 1.2904 - val_accuracy: 0.4957\n",
            "Epoch 3/15\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 1.0471 - accuracy: 0.6109 - val_loss: 0.7737 - val_accuracy: 0.7276\n",
            "Epoch 4/15\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.5056 - accuracy: 0.8296 - val_loss: 0.2695 - val_accuracy: 0.9305\n",
            "Epoch 5/15\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.1677 - accuracy: 0.9657 - val_loss: 0.0870 - val_accuracy: 0.9912\n",
            "Epoch 6/15\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.1007 - accuracy: 0.9850 - val_loss: 0.0492 - val_accuracy: 0.9975\n",
            "Epoch 7/15\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0308 - accuracy: 0.9993 - val_loss: 0.0228 - val_accuracy: 0.9996\n",
            "Epoch 8/15\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0168 - accuracy: 0.9999 - val_loss: 0.0144 - val_accuracy: 0.9999\n",
            "Epoch 9/15\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.0095 - val_accuracy: 0.9999\n",
            "Epoch 10/15\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 0.9999\n",
            "Epoch 11/15\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 0.9999\n",
            "Epoch 12/15\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 13/15\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
            "Epoch 14/15\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
            "Epoch 15/15\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_addons as tfa\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "encoder_embedding_size = 32\n",
        "decoder_embedding_size = 32\n",
        "units = 128\n",
        "\n",
        "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
        "\n",
        "encoder_embeddings = keras.layers.Embedding(\n",
        "    len(INPUT_CHARS) + 1, encoder_embedding_size)(encoder_inputs)\n",
        "\n",
        "decoder_embedding_layer = keras.layers.Embedding(\n",
        "    len(OUTPUT_CHARS) + 2, decoder_embedding_size)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "encoder = keras.layers.LSTM(units, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
        "encoder_state = [state_h, state_c]\n",
        "\n",
        "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "\n",
        "decoder_cell = keras.layers.LSTMCell(units)\n",
        "output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
        "\n",
        "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,\n",
        "                                                 sampler,\n",
        "                                                 output_layer=output_layer)\n",
        "final_outputs, final_state, final_sequence_lengths = decoder(\n",
        "    decoder_embeddings,\n",
        "    initial_state=encoder_state)\n",
        "Y_proba = keras.layers.Activation(\"softmax\")(final_outputs.rnn_output)\n",
        "\n",
        "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                           outputs=[Y_proba])\n",
        "optimizer = keras.optimizers.Nadam()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit([X_train, X_train_decoder], Y_train, epochs=15,\n",
        "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIMI6vd6fVC2"
      },
      "source": [
        "And once again, 100% validation accuracy! To use the model, we can just reuse the `predict_date_strs()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hWVas9IfVC2",
        "outputId": "1c25940f-608b-4047-c6aa-7b6029ccd236",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1789-07-14', '2020-05-01']"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MbxLvi-fVC2"
      },
      "source": [
        "However, there's a much more efficient way to perform inference. Until now, during inference, we've run the model once for each new character. Instead, we can create a new decoder, based on the previously trained layers, but using a `GreedyEmbeddingSampler` instead of a `TrainingSampler`.\n",
        "\n",
        "At each time step, the `GreedyEmbeddingSampler` will compute the argmax of the decoder's outputs, and run the resulting token IDs through the decoder's embedding layer. Then it will feed the resulting embeddings to the decoder's LSTM cell at the next time step. This way, we only need to run the decoder once to get the full prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GioJW2M4fVC2"
      },
      "outputs": [],
      "source": [
        "inference_sampler = tfa.seq2seq.sampler.GreedyEmbeddingSampler(\n",
        "    embedding_fn=decoder_embedding_layer)\n",
        "inference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
        "    decoder_cell, inference_sampler, output_layer=output_layer,\n",
        "    maximum_iterations=max_output_length)\n",
        "batch_size = tf.shape(encoder_inputs)[:1]\n",
        "start_tokens = tf.fill(dims=batch_size, value=sos_id)\n",
        "final_outputs, final_state, final_sequence_lengths = inference_decoder(\n",
        "    start_tokens,\n",
        "    initial_state=encoder_state,\n",
        "    start_tokens=start_tokens,\n",
        "    end_token=0)\n",
        "\n",
        "inference_model = keras.models.Model(inputs=[encoder_inputs],\n",
        "                                     outputs=[final_outputs.sample_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLUBCVUMfVC2"
      },
      "source": [
        "A few notes:\n",
        "* The `GreedyEmbeddingSampler` needs the `start_tokens` (a vector containing the start-of-sequence ID for each decoder sequence), and the `end_token` (the decoder will stop decoding a sequence once the model outputs this token).\n",
        "* We must set `maximum_iterations` when creating the `BasicDecoder`, or else it may run into an infinite loop (if the model never outputs the end token for at least one of the sequences). This would force you would to restart the Jupyter kernel.\n",
        "* The decoder inputs are not needed anymore, since all the decoder inputs are generated dynamically based on the outputs from the previous time step.\n",
        "* The model's outputs are `final_outputs.sample_id` instead of the softmax of `final_outputs.rnn_outputs`. This allows us to directly get the argmax of the model's outputs. If you prefer to have access to the logits, you can replace `final_outputs.sample_id` with `final_outputs.rnn_outputs`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx_XqGO-fVC2"
      },
      "source": [
        "Now we can write a simple function that uses the model to perform the date format conversion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-YnkaKCfVC2"
      },
      "outputs": [],
      "source": [
        "def fast_predict_date_strs(date_strs):\n",
        "    X = prepare_date_strs_padded(date_strs)\n",
        "    Y_pred = inference_model.predict(X)\n",
        "    return ids_to_date_strs(Y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaiYxQrofVC2",
        "outputId": "854b5711-de3d-4216-93b4-6387e8bd2add",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1789-07-14', '2020-05-01']"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fast_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-t8PNUSfVC2"
      },
      "source": [
        "Let's check that it really is faster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "811vRZjbfVC2",
        "outputId": "5b807f69-ab47-4d80-d65c-605aaaf012df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "199 ms ± 3.94 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%timeit predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18xqYAI3fVC2",
        "outputId": "1192bdf4-e27e-4dc5-84cd-e6a928621e69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18.3 ms ± 366 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit fast_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQB9d0y8fVC2"
      },
      "source": [
        "That's more than a 10x speedup! And it would be even more if we were handling longer sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nccTrdQ1fVC2"
      },
      "source": [
        "### Fourth version: using TF-Addons's seq2seq implementation with a scheduled sampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHkylIANfVC2"
      },
      "source": [
        "**Warning**: due to a TF bug, this version only works using TensorFlow 2.2 or above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZcVX1Z2fVC2"
      },
      "source": [
        "When we trained the previous model, at each time step _t_ we gave the model the target token for time step _t_ - 1. However, at inference time, the model did not get the previous target at each time step. Instead, it got the previous prediction. So there is a discrepancy between training and inference, which may lead to disappointing performance. To alleviate this, we can gradually replace the targets with the predictions, during training. For this, we just need to replace the `TrainingSampler` with a `ScheduledEmbeddingTrainingSampler`, and use a Keras callback to gradually increase the `sampling_probability` (i.e., the probability that the decoder will use the prediction from the previous time step rather than the target for the previous time step)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltM-Dj7jfVC2",
        "outputId": "daa9edf3-0c39-41b2-a0d8-c7050b330327"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ageron/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 6s 19ms/step - loss: 1.6759 - accuracy: 0.3681 - val_loss: 1.4611 - val_accuracy: 0.4198\n",
            "Epoch 2/20\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 1.3872 - accuracy: 0.4583 - val_loss: 1.2827 - val_accuracy: 0.5021\n",
            "Epoch 3/20\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 1.0425 - accuracy: 0.6152 - val_loss: 0.8165 - val_accuracy: 0.7000\n",
            "Epoch 4/20\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.6353 - accuracy: 0.7673 - val_loss: 0.4365 - val_accuracy: 0.8464\n",
            "Epoch 5/20\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.3764 - accuracy: 0.8765 - val_loss: 0.2795 - val_accuracy: 0.9166\n",
            "Epoch 6/20\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.2506 - accuracy: 0.9269 - val_loss: 0.1805 - val_accuracy: 0.9489\n",
            "Epoch 7/20\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.1427 - accuracy: 0.9625 - val_loss: 0.1115 - val_accuracy: 0.9718\n",
            "Epoch 8/20\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.0853 - accuracy: 0.9804 - val_loss: 0.0785 - val_accuracy: 0.9809\n",
            "Epoch 9/20\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.1010 - accuracy: 0.9797 - val_loss: 0.1198 - val_accuracy: 0.9746\n",
            "Epoch 10/20\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.0447 - accuracy: 0.9917 - val_loss: 0.0306 - val_accuracy: 0.9949\n",
            "Epoch 11/20\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.0241 - accuracy: 0.9961 - val_loss: 0.0205 - val_accuracy: 0.9968\n",
            "Epoch 12/20\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.0705 - accuracy: 0.9861 - val_loss: 0.0823 - val_accuracy: 0.9860\n",
            "Epoch 13/20\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.0182 - accuracy: 0.9977 - val_loss: 0.0117 - val_accuracy: 0.9980\n",
            "Epoch 14/20\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.0088 - accuracy: 0.9990 - val_loss: 0.0085 - val_accuracy: 0.9990\n",
            "Epoch 15/20\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.0059 - accuracy: 0.9994 - val_loss: 0.0061 - val_accuracy: 0.9993\n",
            "Epoch 16/20\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.0045 - accuracy: 0.9996 - val_loss: 0.0048 - val_accuracy: 0.9996\n",
            "Epoch 17/20\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.0038 - accuracy: 0.9997 - val_loss: 0.0039 - val_accuracy: 0.9995\n",
            "Epoch 18/20\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 0.0024 - val_accuracy: 0.9999\n",
            "Epoch 19/20\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.0020 - accuracy: 0.9999 - val_loss: 0.0031 - val_accuracy: 0.9992\n",
            "Epoch 20/20\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.0022 - val_accuracy: 0.9999\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_addons as tfa\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "n_epochs = 20\n",
        "encoder_embedding_size = 32\n",
        "decoder_embedding_size = 32\n",
        "units = 128\n",
        "\n",
        "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
        "\n",
        "encoder_embeddings = keras.layers.Embedding(\n",
        "    len(INPUT_CHARS) + 1, encoder_embedding_size)(encoder_inputs)\n",
        "\n",
        "decoder_embedding_layer = keras.layers.Embedding(\n",
        "    len(OUTPUT_CHARS) + 2, decoder_embedding_size)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "encoder = keras.layers.LSTM(units, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
        "encoder_state = [state_h, state_c]\n",
        "\n",
        "sampler = tfa.seq2seq.sampler.ScheduledEmbeddingTrainingSampler(\n",
        "    sampling_probability=0.,\n",
        "    embedding_fn=decoder_embedding_layer)\n",
        "# we must set the sampling_probability after creating the sampler\n",
        "# (see https://github.com/tensorflow/addons/pull/1714)\n",
        "sampler.sampling_probability = tf.Variable(0.)\n",
        "\n",
        "decoder_cell = keras.layers.LSTMCell(units)\n",
        "output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
        "\n",
        "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,\n",
        "                                                 sampler,\n",
        "                                                 output_layer=output_layer)\n",
        "final_outputs, final_state, final_sequence_lengths = decoder(\n",
        "    decoder_embeddings,\n",
        "    initial_state=encoder_state)\n",
        "Y_proba = keras.layers.Activation(\"softmax\")(final_outputs.rnn_output)\n",
        "\n",
        "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                           outputs=[Y_proba])\n",
        "optimizer = keras.optimizers.Nadam()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "def update_sampling_probability(epoch, logs):\n",
        "    proba = min(1.0, epoch / (n_epochs - 10))\n",
        "    sampler.sampling_probability.assign(proba)\n",
        "\n",
        "sampling_probability_cb = keras.callbacks.LambdaCallback(\n",
        "    on_epoch_begin=update_sampling_probability)\n",
        "history = model.fit([X_train, X_train_decoder], Y_train, epochs=n_epochs,\n",
        "                    validation_data=([X_valid, X_valid_decoder], Y_valid),\n",
        "                    callbacks=[sampling_probability_cb])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_UKVI_GfVC3"
      },
      "source": [
        "Not quite 100% validation accuracy, but close enough!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHwRNsp9fVC3"
      },
      "source": [
        "For inference, we could do the exact same thing as earlier, using a `GreedyEmbeddingSampler`. However, just for the sake of completeness, let's use a `SampleEmbeddingSampler` instead. It's almost the same thing, except that instead of using the argmax of the model's output to find the token ID, it treats the outputs as logits and uses them to sample a token ID randomly. This can be useful when you want to generate text. The `softmax_temperature` argument serves the \n",
        "same purpose as when we generated Shakespeare-like text (the higher this argument, the more random the generated text will be)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbJ-US0EfVC3"
      },
      "outputs": [],
      "source": [
        "softmax_temperature = tf.Variable(1.)\n",
        "\n",
        "inference_sampler = tfa.seq2seq.sampler.SampleEmbeddingSampler(\n",
        "    embedding_fn=decoder_embedding_layer,\n",
        "    softmax_temperature=softmax_temperature)\n",
        "inference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
        "    decoder_cell, inference_sampler, output_layer=output_layer,\n",
        "    maximum_iterations=max_output_length)\n",
        "batch_size = tf.shape(encoder_inputs)[:1]\n",
        "start_tokens = tf.fill(dims=batch_size, value=sos_id)\n",
        "final_outputs, final_state, final_sequence_lengths = inference_decoder(\n",
        "    start_tokens,\n",
        "    initial_state=encoder_state,\n",
        "    start_tokens=start_tokens,\n",
        "    end_token=0)\n",
        "\n",
        "inference_model = keras.models.Model(inputs=[encoder_inputs],\n",
        "                                     outputs=[final_outputs.sample_id])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_fNvvwmfVC3"
      },
      "outputs": [],
      "source": [
        "def creative_predict_date_strs(date_strs, temperature=1.0):\n",
        "    softmax_temperature.assign(temperature)\n",
        "    X = prepare_date_strs_padded(date_strs)\n",
        "    Y_pred = inference_model.predict(X)\n",
        "    return ids_to_date_strs(Y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ptZ5a54fVC3",
        "outputId": "48d4405a-0564-441f-8711-4e956c9a4902"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1789-07-14', '2020-05-01']"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "creative_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTbTAWvDfVC3"
      },
      "source": [
        "Dates look good at room temperature. Now let's heat things up a bit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qD1ydlqcfVC3",
        "outputId": "9e2acc15-e58a-482f-dd5a-f15afb952a6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['2289607-12', '9272-03-01']"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "creative_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"],\n",
        "                           temperature=5.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx5bGM4zfVC3"
      },
      "source": [
        "Oops, the dates are overcooked, now. Let's call them \"creative\" dates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv47ZS-ZfVC3"
      },
      "source": [
        "### Fifth version: using TFA seq2seq, the Keras subclassing API and attention mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Pr2xXffVC3"
      },
      "source": [
        "The sequences in this problem are pretty short, but if we wanted to tackle longer sequences, we would probably have to use attention mechanisms. While it's possible to code our own implementation, it's simpler and more efficient to use TF-Addons's implementation instead. Let's do that now, this time using Keras' subclassing API.\n",
        "\n",
        "**Warning**: due to a TensorFlow bug (see [this issue](https://github.com/tensorflow/addons/issues/1153) for details), the `get_initial_state()` method fails in eager mode, so for now we have to use the subclassing API, as Keras automatically calls `tf.function()` on the `call()` method (so it runs in graph mode)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHIDOXKUfVC3"
      },
      "source": [
        "In this implementation, we've reverted back to using the `TrainingSampler`, for simplicity (but you can easily tweak it to use a `ScheduledEmbeddingTrainingSampler` instead). We also use a `GreedyEmbeddingSampler` during inference, so this class is pretty easy to use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM71IREIfVC3"
      },
      "outputs": [],
      "source": [
        "class DateTranslation(keras.models.Model):\n",
        "    def __init__(self, units=128, encoder_embedding_size=32,\n",
        "                 decoder_embedding_size=32, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder_embedding = keras.layers.Embedding(\n",
        "            input_dim=len(INPUT_CHARS) + 1,\n",
        "            output_dim=encoder_embedding_size)\n",
        "        self.encoder = keras.layers.LSTM(units,\n",
        "                                         return_sequences=True,\n",
        "                                         return_state=True)\n",
        "        self.decoder_embedding = keras.layers.Embedding(\n",
        "            input_dim=len(OUTPUT_CHARS) + 2,\n",
        "            output_dim=decoder_embedding_size)\n",
        "        self.attention = tfa.seq2seq.LuongAttention(units)\n",
        "        decoder_inner_cell = keras.layers.LSTMCell(units)\n",
        "        self.decoder_cell = tfa.seq2seq.AttentionWrapper(\n",
        "            cell=decoder_inner_cell,\n",
        "            attention_mechanism=self.attention)\n",
        "        output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(\n",
        "            cell=self.decoder_cell,\n",
        "            sampler=tfa.seq2seq.sampler.TrainingSampler(),\n",
        "            output_layer=output_layer)\n",
        "        self.inference_decoder = tfa.seq2seq.BasicDecoder(\n",
        "            cell=self.decoder_cell,\n",
        "            sampler=tfa.seq2seq.sampler.GreedyEmbeddingSampler(\n",
        "                embedding_fn=self.decoder_embedding),\n",
        "            output_layer=output_layer,\n",
        "            maximum_iterations=max_output_length)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        encoder_input, decoder_input = inputs\n",
        "        encoder_embeddings = self.encoder_embedding(encoder_input)\n",
        "        encoder_outputs, encoder_state_h, encoder_state_c = self.encoder(\n",
        "            encoder_embeddings,\n",
        "            training=training)\n",
        "        encoder_state = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "        self.attention(encoder_outputs,\n",
        "                       setup_memory=True)\n",
        "        \n",
        "        decoder_embeddings = self.decoder_embedding(decoder_input)\n",
        "\n",
        "        decoder_initial_state = self.decoder_cell.get_initial_state(\n",
        "            decoder_embeddings)\n",
        "        decoder_initial_state = decoder_initial_state.clone(\n",
        "            cell_state=encoder_state)\n",
        "        \n",
        "        if training:\n",
        "            decoder_outputs, _, _ = self.decoder(\n",
        "                decoder_embeddings,\n",
        "                initial_state=decoder_initial_state,\n",
        "                training=training)\n",
        "        else:\n",
        "            start_tokens = tf.zeros_like(encoder_input[:, 0]) + sos_id\n",
        "            decoder_outputs, _, _ = self.inference_decoder(\n",
        "                decoder_embeddings,\n",
        "                initial_state=decoder_initial_state,\n",
        "                start_tokens=start_tokens,\n",
        "                end_token=0)\n",
        "\n",
        "        return tf.nn.softmax(decoder_outputs.rnn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOPA3cy-fVC3",
        "outputId": "e5fc8700-e88b-46af-f1d0-008d9901ae03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "313/313 [==============================] - 7s 21ms/step - loss: 2.1549 - accuracy: 0.2295 - val_loss: 2.1450 - val_accuracy: 0.2239\n",
            "Epoch 2/25\n",
            "313/313 [==============================] - 6s 19ms/step - loss: 1.8147 - accuracy: 0.3492 - val_loss: 1.4931 - val_accuracy: 0.4476\n",
            "Epoch 3/25\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 1.3585 - accuracy: 0.4909 - val_loss: 1.3168 - val_accuracy: 0.5100\n",
            "Epoch 4/25\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 1.2787 - accuracy: 0.5293 - val_loss: 1.1767 - val_accuracy: 0.5624\n",
            "Epoch 5/25\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 1.1236 - accuracy: 0.5776 - val_loss: 1.0769 - val_accuracy: 0.5907\n",
            "Epoch 6/25\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 1.0369 - accuracy: 0.6073 - val_loss: 1.0159 - val_accuracy: 0.6199\n",
            "Epoch 7/25\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 0.9752 - accuracy: 0.6295 - val_loss: 0.9723 - val_accuracy: 0.6346\n",
            "Epoch 8/25\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 0.9794 - accuracy: 0.6315 - val_loss: 0.9444 - val_accuracy: 0.6371\n",
            "Epoch 9/25\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 0.9338 - accuracy: 0.6415 - val_loss: 0.9296 - val_accuracy: 0.6381\n",
            "Epoch 10/25\n",
            "313/313 [==============================] - 6s 19ms/step - loss: 0.9439 - accuracy: 0.6418 - val_loss: 0.9028 - val_accuracy: 0.6574\n",
            "Epoch 11/25\n",
            "313/313 [==============================] - 6s 19ms/step - loss: 0.8807 - accuracy: 0.6637 - val_loss: 0.9835 - val_accuracy: 0.6369\n",
            "Epoch 12/25\n",
            "313/313 [==============================] - 6s 19ms/step - loss: 0.7307 - accuracy: 0.6953 - val_loss: 0.8942 - val_accuracy: 0.6873\n",
            "Epoch 13/25\n",
            "313/313 [==============================] - 6s 19ms/step - loss: 0.5833 - accuracy: 0.7327 - val_loss: 0.6944 - val_accuracy: 0.7391\n",
            "Epoch 14/25\n",
            "313/313 [==============================] - 6s 19ms/step - loss: 0.4664 - accuracy: 0.7940 - val_loss: 0.6228 - val_accuracy: 0.7885\n",
            "Epoch 15/25\n",
            "313/313 [==============================] - 6s 19ms/step - loss: 0.3205 - accuracy: 0.8740 - val_loss: 0.4825 - val_accuracy: 0.8780\n",
            "Epoch 16/25\n",
            "313/313 [==============================] - 6s 19ms/step - loss: 0.2329 - accuracy: 0.9216 - val_loss: 0.3851 - val_accuracy: 0.9118\n",
            "Epoch 17/25\n",
            "313/313 [==============================] - 7s 21ms/step - loss: 0.2480 - accuracy: 0.9372 - val_loss: 0.2785 - val_accuracy: 0.9111\n",
            "Epoch 18/25\n",
            "313/313 [==============================] - 7s 22ms/step - loss: 0.1182 - accuracy: 0.9801 - val_loss: 0.1372 - val_accuracy: 0.9786\n",
            "Epoch 19/25\n",
            "313/313 [==============================] - 7s 22ms/step - loss: 0.0643 - accuracy: 0.9937 - val_loss: 0.0681 - val_accuracy: 0.9909\n",
            "Epoch 20/25\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 0.0446 - accuracy: 0.9952 - val_loss: 0.0487 - val_accuracy: 0.9934\n",
            "Epoch 21/25\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 0.0247 - accuracy: 0.9987 - val_loss: 0.0228 - val_accuracy: 0.9987\n",
            "Epoch 22/25\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 0.0456 - accuracy: 0.9918 - val_loss: 0.0207 - val_accuracy: 0.9985\n",
            "Epoch 23/25\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 0.0131 - accuracy: 0.9997 - val_loss: 0.0127 - val_accuracy: 0.9993\n",
            "Epoch 24/25\n",
            "313/313 [==============================] - 6s 19ms/step - loss: 0.0360 - accuracy: 0.9933 - val_loss: 0.0146 - val_accuracy: 0.9990\n",
            "Epoch 25/25\n",
            "313/313 [==============================] - 6s 19ms/step - loss: 0.0092 - accuracy: 0.9998 - val_loss: 0.0089 - val_accuracy: 0.9992\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = DateTranslation()\n",
        "optimizer = keras.optimizers.Nadam()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit([X_train, X_train_decoder], Y_train, epochs=25,\n",
        "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM9GehY9fVC3"
      },
      "source": [
        "Not quite 100% validation accuracy, but close. It took a bit longer to converge this time, but there were also more parameters and more computations per iteration. And we did not use a scheduled sampler.\n",
        "\n",
        "To use the model, we can write yet another little function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW3O_rQ-fVC3"
      },
      "outputs": [],
      "source": [
        "def fast_predict_date_strs_v2(date_strs):\n",
        "    X = prepare_date_strs_padded(date_strs)\n",
        "    X_decoder = tf.zeros(shape=(len(X), max_output_length), dtype=tf.int32)\n",
        "    Y_probas = model.predict([X, X_decoder])\n",
        "    Y_pred = tf.argmax(Y_probas, axis=-1)\n",
        "    return ids_to_date_strs(Y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MICfxRNfVC3",
        "outputId": "1c52de3b-fbec-4d95-875d-29302a24f138"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1789-07-14', '2020-05-01']"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fast_predict_date_strs_v2([\"July 14, 1789\", \"May 01, 2020\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVRkXdjafVC3"
      },
      "source": [
        "There are still a few interesting features from TF-Addons that you may want to look at:\n",
        "* Using a `BeamSearchDecoder` rather than a `BasicDecoder` for inference. Instead of outputing the character with the highest probability, this decoder keeps track of the several candidates, and keeps only the most likely sequences of candidates (see chapter 16 in the book for more details).\n",
        "* Setting masks or specifying `sequence_length` if the input or target sequences may have very different lengths.\n",
        "* Using a `ScheduledOutputTrainingSampler`, which gives you more flexibility than the `ScheduledEmbeddingTrainingSampler` to decide how to feed the output at time _t_ to the cell at time _t_+1. By default it feeds the outputs directly to cell, without computing the argmax ID and passing it through an embedding layer. Alternatively, you specify a `next_inputs_fn` function that will be used to convert the cell outputs to inputs at the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaKCjTryfVC3"
      },
      "source": [
        "## 10.\n",
        "_Exercise: Go through TensorFlow's [Neural Machine Translation with Attention tutorial](https://homl.info/nmttuto)._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niNNDFFRfVC3"
      },
      "source": [
        "Simply open the Colab and follow its instructions. Alternatively, if you want a simpler example of using TF-Addons's seq2seq implementation for Neural Machine Translation (NMT), look at the solution to the previous question. The last model implementation will give you a simpler example of using TF-Addons to build an NMT model using attention mechanisms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phjYrjfUfVC3"
      },
      "source": [
        "## 11.\n",
        "_Exercise: Use one of the recent language models (e.g., GPT) to generate more convincing Shakespearean text._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDgSqJbDfVC4"
      },
      "source": [
        "The simplest way to use recent language models is to use the excellent [transformers library](https://huggingface.co/transformers/), open sourced by Hugging Face. It provides many modern neural net architectures (including BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet and more) for Natural Language Processing (NLP), including many pretrained models. It relies on either TensorFlow or PyTorch. Best of all: it's amazingly simple to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-E_ODFxfVC4"
      },
      "source": [
        "First, let's load a pretrained model. In this example, we will use OpenAI's GPT model, with an additional Language Model on top (just a linear layer with weights tied to the input embeddings). Let's import it and load the pretrained weights (this will download about 445MB of data to `~/.cache/torch/transformers`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3mVyyu1fVC4"
      },
      "outputs": [],
      "source": [
        "from transformers import TFOpenAIGPTLMHeadModel\n",
        "\n",
        "model = TFOpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHOKvnYmfVC4"
      },
      "source": [
        "Next we will need a specialized tokenizer for this model. This one will try to use the [spaCy](https://spacy.io/) and [ftfy](https://pypi.org/project/ftfy/) libraries if they are installed, or else it will fall back to BERT's `BasicTokenizer` followed by Byte-Pair Encoding (which should be fine for most use cases)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "na3rjTHhfVC4"
      },
      "outputs": [],
      "source": [
        "from transformers import OpenAIGPTTokenizer\n",
        "\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFNKa40EfVC4"
      },
      "source": [
        "Now let's use the tokenizer to tokenize and encode the prompt text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5oJ4_CPfVC4",
        "outputId": "65b896a8-91de-4b5c-908e-d48e2a545f48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10), dtype=int32, numpy=\n",
              "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
              "        16187]], dtype=int32)>"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_text = \"This royal throne of kings, this sceptred isle\"\n",
        "encoded_prompt = tokenizer.encode(prompt_text,\n",
        "                                  add_special_tokens=False,\n",
        "                                  return_tensors=\"tf\")\n",
        "encoded_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU9-DKL9fVC4"
      },
      "source": [
        "Easy! Next, let's use the model to generate text after the prompt. We will generate 5 different sentences, each starting with the prompt text, followed by 40 additional tokens. For an explanation of what all the hyperparameters do, make sure to check out this great [blog post](https://huggingface.co/blog/how-to-generate) by Patrick von Platen (from Hugging Face). You can play around with the hyperparameters to try to obtain better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrFQm2ZzfVC4",
        "outputId": "849f6593-69ba-4aaa-bb8d-c5750ab55563"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 50), dtype=int32, numpy=\n",
              "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
              "        16187,   239,   784,   645,  1184,   558,  1886,   688,  6437,\n",
              "          240,   784,   645,   507,   641,  5486,   240,   600,   636,\n",
              "          868,   604,   694,  2816,   485,  1894,   822,   481,  1491,\n",
              "          600,   880,  6061,   239,   256, 40477,   256,   600,   635,\n",
              "          538,   604,  1816,   525,   239],\n",
              "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
              "        16187,   488,  1288,   989,   640, 16605,   239,   256, 40477,\n",
              "          674,   481, 12744,  3912,   488,  3912,  5936,  2441,   811,\n",
              "          488,  1040,   485,   754,  3952,   239, 40477,   481,  1375,\n",
              "         1981,   833,  1210,   481, 17384,   488,   481,  3089,   488,\n",
              "          481,  4815,   509,   498,  1424],\n",
              "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
              "        16187,   980,   987,  1074, 13138,   240,   531,   501,   517,\n",
              "          836,   525, 12659,   485,  2642,   512,   239,   500,   616,\n",
              "         7339,   704,   989,  1259, 38752,   481,  9606,   498,   481,\n",
              "         6903,   239,   500,   616,  7339,   704,  3064,   994,   580,\n",
              "         3953,   617,   616,  4741,   488],\n",
              "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
              "        16187, 10595,   485,   510,   239,   244, 40477,   244,   481,\n",
              "         1424,  6404,   498,  1922,    23, 37492,   257,   244, 40477,\n",
              "          244,  3491,   240,   244,   603,   481,   618,   556,   246,\n",
              "         3386,   498,   524,   756,   239,   244,   616,  1276,   509,\n",
              "         1098, 10945,   498,   246,  6785],\n",
              "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
              "        16187,   544,  2203,   239,   616,   544,   246,  6460,   260,\n",
              "          850,   629,  4844,  3064,  3766,   240,   246,  1082,   806,\n",
              "         9606,   640, 32581,   240,   595,  7914,  1243,   488, 18535,\n",
              "          239,   249,   587,   538,   788,   775,  2319,   498,  1013,\n",
              "          525,   544,   595,   754,  1074]], dtype=int32)>"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_sequences = 5\n",
        "length = 40\n",
        "\n",
        "generated_sequences = model.generate(\n",
        "    input_ids=encoded_prompt,\n",
        "    do_sample=True,\n",
        "    max_length=length + len(encoded_prompt[0]),\n",
        "    temperature=1.0,\n",
        "    top_k=0,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.0,\n",
        "    num_return_sequences=num_sequences,\n",
        ")\n",
        "\n",
        "generated_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCHvDoBwfVC4"
      },
      "source": [
        "Now let's decode the generated sequences and print them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq5Nqx2afVC4",
        "outputId": "f7296f2b-c43b-48d6-82bd-f7dc5414c723"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "this royal throne of kings, this sceptred isle. even if someone had given them permission, even if it were required, they would never have been allowed to live through the hell they've survived.'\n",
            "'they couldn't have known that.\n",
            "--------------------------------------------------------------------------------\n",
            "this royal throne of kings, this sceptred isle and these people are royalty.'\n",
            " then the mute prince and prince edward broke off and went to their rooms. \n",
            " the talk passed again between the princes and the guards and the princess was of great\n",
            "--------------------------------------------------------------------------------\n",
            "this royal throne of kings, this sceptred isle has its own highness, an alatte that waits to save you. in this kingdom your people must emulate the kings of the realm. in this kingdom your kin should be saved from this pit and\n",
            "--------------------------------------------------------------------------------\n",
            "this royal throne of kings, this sceptred isle belongs to me. \" \n",
            " \" the great throne of penvynne? \" \n",
            " \" indeed, \" said the king with a nod of his head. \" this world was once composed of a magical\n",
            "--------------------------------------------------------------------------------\n",
            "this royal throne of kings, this sceptred isle is empty. this is a modern - day fedaykin court, a place where kings are governed, not emperors and judges. i don't see any sign of life that is not their own\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for sequence in generated_sequences:\n",
        "    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
        "    print(text)\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo_as5rQfVC4"
      },
      "source": [
        "You can try more recent (and larger) models, such as GPT-2, CTRL, Transformer-XL or XLNet, which are all available as pretrained models in the transformers library, including variants with Language Models on top. The preprocessing steps vary slightly between models, so make sure to check out this [generation example](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py) from the transformers documentation (this example uses PyTorch, but it will work with very little tweaks, such as adding `TF` at the beginning of the model class name, removing the `.to()` method calls, and using `return_tensors=\"tf\"` instead of `\"pt\"`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owjh-84HfVC4"
      },
      "source": [
        "Hope you enjoyed this chapter! :)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
