{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_1$ and $\\ell_2$ regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 2us/step\n",
      "40960/29515 [=========================================] - 0s 2us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 2s 0us/step\n",
      "26435584/26421880 [==============================] - 2s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 1s 0us/step\n",
      "4431872/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "# or l1(0.1) for ℓ1 regularization with a factor of 0.1\n",
    "# or l1_l2(0.1, 0.01) for both ℓ1 and ℓ2 regularization, with factors 0.1 and 0.01 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 14s 5ms/step - loss: 1.6199 - accuracy: 0.8110 - val_loss: 0.7077 - val_accuracy: 0.8370\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.7190 - accuracy: 0.8267 - val_loss: 0.7109 - val_accuracy: 0.8340\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(100, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(10, activation=\"softmax\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 10s 5ms/step - loss: 1.6258 - accuracy: 0.8127 - val_loss: 0.7562 - val_accuracy: 0.8106\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.7197 - accuracy: 0.8245 - val_loss: 0.6878 - val_accuracy: 0.8382\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 10s 5ms/step - loss: 0.5719 - accuracy: 0.8028 - val_loss: 0.3609 - val_accuracy: 0.8684\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4220 - accuracy: 0.8450 - val_loss: 0.3448 - val_accuracy: 0.8780\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.6617 - accuracy: 0.7611 - val_loss: 0.5781 - val_accuracy: 0.8402\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5548 - accuracy: 0.7962 - val_loss: 0.5412 - val_accuracy: 0.8472\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5258 - accuracy: 0.8063 - val_loss: 0.4999 - val_accuracy: 0.8566\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5052 - accuracy: 0.8123 - val_loss: 0.4794 - val_accuracy: 0.8606\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4924 - accuracy: 0.8169 - val_loss: 0.4647 - val_accuracy: 0.8616\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4832 - accuracy: 0.8202 - val_loss: 0.4871 - val_accuracy: 0.8566\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4714 - accuracy: 0.8254 - val_loss: 0.5078 - val_accuracy: 0.8482\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.4643 - accuracy: 0.8289 - val_loss: 0.4518 - val_accuracy: 0.8632\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4614 - accuracy: 0.8305 - val_loss: 0.4343 - val_accuracy: 0.8724\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4526 - accuracy: 0.8312 - val_loss: 0.4358 - val_accuracy: 0.8644\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4477 - accuracy: 0.8343 - val_loss: 0.4043 - val_accuracy: 0.8756\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4435 - accuracy: 0.8343 - val_loss: 0.5164 - val_accuracy: 0.8568\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4426 - accuracy: 0.8347 - val_loss: 0.4204 - val_accuracy: 0.8752\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4373 - accuracy: 0.8373 - val_loss: 0.4569 - val_accuracy: 0.8634\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.4319 - accuracy: 0.8395 - val_loss: 0.4477 - val_accuracy: 0.8654\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4298 - accuracy: 0.8411 - val_loss: 0.4316 - val_accuracy: 0.8730\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4285 - accuracy: 0.8419 - val_loss: 0.5145 - val_accuracy: 0.8630\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4278 - accuracy: 0.8407 - val_loss: 0.4636 - val_accuracy: 0.8778\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4247 - accuracy: 0.8430 - val_loss: 0.4744 - val_accuracy: 0.8722\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4180 - accuracy: 0.8432 - val_loss: 0.4095 - val_accuracy: 0.8754\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 20\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4657 - accuracy: 0.8580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4657217264175415, 0.8579999804496765]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3371 - accuracy: 0.8817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33707743883132935, 0.8816909193992615]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4195 - accuracy: 0.8436\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([model(X_test_scaled, training=True)\n",
    "                     for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_std = y_probas.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(X_test_scaled[:1]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.48, 0.  , 0.23, 0.  , 0.28]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.31, 0.  , 0.63]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.3 , 0.  , 0.67]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.16, 0.  , 0.81]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.13, 0.  , 0.73]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.17, 0.  , 0.58, 0.  , 0.25]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.85, 0.  , 0.06, 0.  , 0.08]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.48, 0.  , 0.52]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.23, 0.  , 0.75]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.06, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.5 , 0.  , 0.05, 0.  , 0.45]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.43, 0.  , 0.  , 0.  , 0.56]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.12, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.67, 0.  , 0.2 , 0.  , 0.13]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.31, 0.  , 0.49, 0.  , 0.2 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.74, 0.  , 0.  , 0.  , 0.25]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.21, 0.  , 0.78]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.85, 0.  , 0.04, 0.  , 0.11]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.29, 0.  , 0.57, 0.  , 0.14]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.51, 0.  , 0.43, 0.  , 0.06]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.25, 0.  , 0.19, 0.  , 0.56]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.04, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.19, 0.  , 0.7 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.23, 0.  , 0.68]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.51, 0.  , 0.28, 0.  , 0.21]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.33, 0.  , 0.62]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.71, 0.  , 0.28]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.12, 0.  , 0.81]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.58, 0.  , 0.41]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.53, 0.  , 0.35]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.11, 0.  , 0.8 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.09, 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.48, 0.  , 0.45]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.54, 0.  , 0.4 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.8 , 0.  , 0.19]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.27, 0.  , 0.66]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.05, 0.  , 0.94]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.2 , 0.  , 0.78]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.25, 0.  , 0.34, 0.  , 0.41]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.51, 0.  , 0.14, 0.  , 0.35]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.24, 0.  , 0.74]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.14, 0.  , 0.83]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.25, 0.  , 0.25, 0.  , 0.49]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.31, 0.  , 0.17, 0.  , 0.52]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.72, 0.  , 0.26]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.78, 0.  , 0.13]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.4 , 0.  , 0.32, 0.  , 0.27]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.1 , 0.  , 0.85]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.6 , 0.  , 0.4 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.1 , 0.  , 0.88]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.44, 0.  , 0.46]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.11, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.07, 0.  , 0.89]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.92, 0.  , 0.06]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.22, 0.  , 0.72]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.2 , 0.  , 0.79]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.14, 0.  , 0.83]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.55, 0.  , 0.3 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.32, 0.  , 0.16, 0.  , 0.51]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.68, 0.  , 0.03, 0.  , 0.29]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.43, 0.  , 0.44]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.6 , 0.  , 0.39]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.32, 0.  , 0.57, 0.  , 0.11]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.15, 0.  , 0.1 , 0.  , 0.75]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.04, 0.  , 0.93]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.27, 0.  , 0.61]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.  , 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.1 , 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.95, 0.  , 0.  , 0.  , 0.05]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.03, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.08, 0.  , 0.86]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.15, 0.  , 0.81]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.28, 0.  , 0.07, 0.  , 0.66]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.62, 0.  , 0.24]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.12, 0.  , 0.77]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.31, 0.  , 0.6 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.33, 0.  , 0.66]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.02, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.26, 0.  , 0.69]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.03, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.77, 0.  , 0.07, 0.02, 0.14]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.32, 0.  , 0.6 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.73, 0.  , 0.01, 0.  , 0.26]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.15, 0.  , 0.84]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.01, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.54, 0.01, 0.39]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.07, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.55, 0.  , 0.16, 0.  , 0.29]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.35, 0.  , 0.51]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.72, 0.  , 0.2 , 0.  , 0.07]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.29, 0.  , 0.45, 0.  , 0.27]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.26, 0.  , 0.74]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_probas[:, :1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.17, 0.  , 0.24, 0.  , 0.59]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_proba[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.24, 0.  , 0.22, 0.  , 0.29]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_std = y_probas.std(axis=0)\n",
    "np.round(y_std[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8702"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " mc_alpha_dropout (MCAlphaDr  (None, 784)              0         \n",
      " opout)                                                          \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " mc_alpha_dropout_1 (MCAlpha  (None, 300)              0         \n",
      " Dropout)                                                        \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " mc_alpha_dropout_2 (MCAlpha  (None, 100)              0         \n",
      " Dropout)                                                        \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "mc_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the model with MC Dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.15, 0.  , 0.26, 0.  , 0.59]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.mean([mc_model.predict(X_test_scaled[:1]) for sample in range(100)], axis=0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                           kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4748 - accuracy: 0.8320 - val_loss: 0.3725 - val_accuracy: 0.8654\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3539 - accuracy: 0.8722 - val_loss: 0.3704 - val_accuracy: 0.8682\n"
     ]
    }
   ],
   "source": [
    "MaxNormDense = partial(keras.layers.Dense,\n",
    "                       activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       kernel_constraint=keras.constraints.max_norm(1.))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    MaxNormDense(300),\n",
    "    MaxNormDense(100),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deep Learning on CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.\n",
    "*Exercise: Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the ELU activation function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 activation=\"elu\",\n",
    "                                 kernel_initializer=\"he_normal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.\n",
    "*Exercise: Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with `keras.datasets.cifar10.load_data()`. The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you'll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model's architecture or hyperparameters.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the output layer to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a Nadam optimizer with a learning rate of 5e-5. I tried learning rates 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3 and 1e-2, and I compared their learning curves for 10 epochs each (using the TensorBoard callback, below). The learning rates 3e-5 and 1e-4 were pretty good, so I tried 5e-5, which turned out to be slightly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-5)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the CIFAR10 dataset. We also want to use early stopping, so we need a validation set. Let's use the first 5,000 images of the original training set as the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the callbacks we need and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-505728458e1c8599\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-505728458e1c8599\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=./my_cifar10_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 21s 12ms/step - loss: 4.0002 - accuracy: 0.1643 - val_loss: 2.1159 - val_accuracy: 0.2338\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 2.0378 - accuracy: 0.2566 - val_loss: 2.1490 - val_accuracy: 0.2252\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.9214 - accuracy: 0.2993 - val_loss: 1.9659 - val_accuracy: 0.2840\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.8486 - accuracy: 0.3279 - val_loss: 1.9724 - val_accuracy: 0.3064\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.7903 - accuracy: 0.3522 - val_loss: 1.7818 - val_accuracy: 0.3592\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.7434 - accuracy: 0.3663 - val_loss: 1.7422 - val_accuracy: 0.3786\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.7010 - accuracy: 0.3851 - val_loss: 1.7309 - val_accuracy: 0.3672\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.6607 - accuracy: 0.3967 - val_loss: 1.6576 - val_accuracy: 0.3984\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.6308 - accuracy: 0.4067 - val_loss: 1.6478 - val_accuracy: 0.4016\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.6057 - accuracy: 0.4186 - val_loss: 1.6899 - val_accuracy: 0.3736\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.5803 - accuracy: 0.4243 - val_loss: 1.6587 - val_accuracy: 0.4044\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.5609 - accuracy: 0.4363 - val_loss: 1.6438 - val_accuracy: 0.4130\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.5413 - accuracy: 0.4414 - val_loss: 1.5845 - val_accuracy: 0.4298\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 1.5255 - accuracy: 0.4463 - val_loss: 1.5882 - val_accuracy: 0.4240\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 20s 15ms/step - loss: 1.5097 - accuracy: 0.4547 - val_loss: 1.5622 - val_accuracy: 0.4288\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.4926 - accuracy: 0.4597 - val_loss: 1.5721 - val_accuracy: 0.4360\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.4765 - accuracy: 0.4682 - val_loss: 1.5783 - val_accuracy: 0.4408\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.4628 - accuracy: 0.4758 - val_loss: 1.5342 - val_accuracy: 0.4446\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.4479 - accuracy: 0.4769 - val_loss: 1.5438 - val_accuracy: 0.4470\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.4365 - accuracy: 0.4792 - val_loss: 1.5641 - val_accuracy: 0.4470\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.4262 - accuracy: 0.4864 - val_loss: 1.5326 - val_accuracy: 0.4546\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.4115 - accuracy: 0.4912 - val_loss: 1.5260 - val_accuracy: 0.4556\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.4032 - accuracy: 0.4958 - val_loss: 1.5249 - val_accuracy: 0.4462\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.3883 - accuracy: 0.4994 - val_loss: 1.5349 - val_accuracy: 0.4424\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.3767 - accuracy: 0.5044 - val_loss: 1.5200 - val_accuracy: 0.4632\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.3658 - accuracy: 0.5086 - val_loss: 1.5375 - val_accuracy: 0.4528\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.3590 - accuracy: 0.5105 - val_loss: 1.5124 - val_accuracy: 0.4646\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.3493 - accuracy: 0.5144 - val_loss: 1.5237 - val_accuracy: 0.4592\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.3411 - accuracy: 0.5183 - val_loss: 1.5077 - val_accuracy: 0.4768\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3282 - accuracy: 0.5213 - val_loss: 1.5800 - val_accuracy: 0.4530\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.3204 - accuracy: 0.5240 - val_loss: 1.5382 - val_accuracy: 0.4626\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3111 - accuracy: 0.5306 - val_loss: 1.4866 - val_accuracy: 0.4764\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.3042 - accuracy: 0.5293 - val_loss: 1.4982 - val_accuracy: 0.4778\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.2953 - accuracy: 0.5340 - val_loss: 1.5133 - val_accuracy: 0.4704\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.2876 - accuracy: 0.5357 - val_loss: 1.5391 - val_accuracy: 0.4600\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2789 - accuracy: 0.5385 - val_loss: 1.4944 - val_accuracy: 0.4820\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.2678 - accuracy: 0.5424 - val_loss: 1.4840 - val_accuracy: 0.4782\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.2625 - accuracy: 0.5448 - val_loss: 1.5331 - val_accuracy: 0.4680\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.2552 - accuracy: 0.5506 - val_loss: 1.5090 - val_accuracy: 0.4688\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.2465 - accuracy: 0.5524 - val_loss: 1.4767 - val_accuracy: 0.4860\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.2368 - accuracy: 0.5543 - val_loss: 1.5056 - val_accuracy: 0.4756\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2316 - accuracy: 0.5567 - val_loss: 1.5097 - val_accuracy: 0.4780\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.2251 - accuracy: 0.5590 - val_loss: 1.5480 - val_accuracy: 0.4696\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.2169 - accuracy: 0.5637 - val_loss: 1.5248 - val_accuracy: 0.4748\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2075 - accuracy: 0.5666 - val_loss: 1.5075 - val_accuracy: 0.4782\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.2016 - accuracy: 0.5668 - val_loss: 1.5278 - val_accuracy: 0.4740\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1926 - accuracy: 0.5708 - val_loss: 1.5135 - val_accuracy: 0.4804\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1846 - accuracy: 0.5744 - val_loss: 1.5528 - val_accuracy: 0.4730\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.1757 - accuracy: 0.5781 - val_loss: 1.5184 - val_accuracy: 0.4818\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1712 - accuracy: 0.5808 - val_loss: 1.5016 - val_accuracy: 0.4904\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.1646 - accuracy: 0.5819 - val_loss: 1.5271 - val_accuracy: 0.4812\n",
      "Epoch 52/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1541 - accuracy: 0.5865 - val_loss: 1.5589 - val_accuracy: 0.4624\n",
      "Epoch 53/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1462 - accuracy: 0.5886 - val_loss: 1.5303 - val_accuracy: 0.4814\n",
      "Epoch 54/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.1417 - accuracy: 0.5903 - val_loss: 1.5797 - val_accuracy: 0.4754\n",
      "Epoch 55/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.1370 - accuracy: 0.5912 - val_loss: 1.5342 - val_accuracy: 0.4868\n",
      "Epoch 56/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.1288 - accuracy: 0.5949 - val_loss: 1.5721 - val_accuracy: 0.4736\n",
      "Epoch 57/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.1215 - accuracy: 0.5955 - val_loss: 1.5475 - val_accuracy: 0.4804\n",
      "Epoch 58/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.1164 - accuracy: 0.6005 - val_loss: 1.5364 - val_accuracy: 0.4804\n",
      "Epoch 59/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.1110 - accuracy: 0.5976 - val_loss: 1.5166 - val_accuracy: 0.4882\n",
      "Epoch 60/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.1037 - accuracy: 0.6031 - val_loss: 1.5655 - val_accuracy: 0.4858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e88e9ea310>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 4ms/step - loss: 1.4767 - accuracy: 0.4860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4766956567764282, 0.4860000014305115]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the lowest validation loss gets about 47.6% accuracy on the validation set. It took 27 epochs to reach the lowest validation loss, with roughly 8 seconds per epoch on my laptop (without a GPU). Let's see if we can improve performance using Batch Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.\n",
    "*Exercise: Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is very similar to the code above, with a few changes:\n",
    "\n",
    "* I added a BN layer after every Dense layer (before the activation function), except for the output layer. I also added a BN layer before the first hidden layer.\n",
    "* I changed the learning rate to 5e-4. I experimented with 1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3 and 3e-3, and I chose the one with the best validation performance after 20 epochs.\n",
    "* I renamed the run directories to run_bn_* and the model file name to my_cifar10_bn_model.h5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 37s 19ms/step - loss: 1.8461 - accuracy: 0.3368 - val_loss: 1.6690 - val_accuracy: 0.4062\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.6716 - accuracy: 0.4047 - val_loss: 1.5697 - val_accuracy: 0.4356\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.6027 - accuracy: 0.4304 - val_loss: 1.5443 - val_accuracy: 0.4456\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.5513 - accuracy: 0.4468 - val_loss: 1.5078 - val_accuracy: 0.4570\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.5060 - accuracy: 0.4658 - val_loss: 1.4446 - val_accuracy: 0.4804\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.4709 - accuracy: 0.4799 - val_loss: 1.4118 - val_accuracy: 0.4986\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.4348 - accuracy: 0.4886 - val_loss: 1.4356 - val_accuracy: 0.4910\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.4058 - accuracy: 0.5008 - val_loss: 1.3867 - val_accuracy: 0.4974\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 29s 20ms/step - loss: 1.3816 - accuracy: 0.5128 - val_loss: 1.3551 - val_accuracy: 0.5218\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.3600 - accuracy: 0.5186 - val_loss: 1.3422 - val_accuracy: 0.5218\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.3422 - accuracy: 0.5255 - val_loss: 1.3375 - val_accuracy: 0.5254\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.3170 - accuracy: 0.5369 - val_loss: 1.3647 - val_accuracy: 0.5108\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.2963 - accuracy: 0.5410 - val_loss: 1.3634 - val_accuracy: 0.5238\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.2793 - accuracy: 0.5475 - val_loss: 1.3345 - val_accuracy: 0.5308\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.2621 - accuracy: 0.5536 - val_loss: 1.3634 - val_accuracy: 0.5248\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.2502 - accuracy: 0.5604 - val_loss: 1.3701 - val_accuracy: 0.5244\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.2313 - accuracy: 0.5653 - val_loss: 1.3253 - val_accuracy: 0.5360\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.2174 - accuracy: 0.5691 - val_loss: 1.3293 - val_accuracy: 0.5332\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.2016 - accuracy: 0.5736 - val_loss: 1.3493 - val_accuracy: 0.5250\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.1914 - accuracy: 0.5806 - val_loss: 1.3411 - val_accuracy: 0.5404\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 34s 24ms/step - loss: 1.1777 - accuracy: 0.5859 - val_loss: 1.3630 - val_accuracy: 0.5332\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.1619 - accuracy: 0.5908 - val_loss: 1.3483 - val_accuracy: 0.5310\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.1559 - accuracy: 0.5953 - val_loss: 1.3355 - val_accuracy: 0.5372\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.1355 - accuracy: 0.6010 - val_loss: 1.3344 - val_accuracy: 0.5432\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.1261 - accuracy: 0.6055 - val_loss: 1.3417 - val_accuracy: 0.5378\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.1145 - accuracy: 0.6082 - val_loss: 1.3309 - val_accuracy: 0.5406\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.1032 - accuracy: 0.6129 - val_loss: 1.3400 - val_accuracy: 0.5378\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.0953 - accuracy: 0.6169 - val_loss: 1.3463 - val_accuracy: 0.5336\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0800 - accuracy: 0.6219 - val_loss: 1.3528 - val_accuracy: 0.5404\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 34s 24ms/step - loss: 1.0739 - accuracy: 0.6222 - val_loss: 1.3450 - val_accuracy: 0.5482\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0586 - accuracy: 0.6263 - val_loss: 1.3536 - val_accuracy: 0.5350\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.0528 - accuracy: 0.6289 - val_loss: 1.3777 - val_accuracy: 0.5342\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.0373 - accuracy: 0.6354 - val_loss: 1.3259 - val_accuracy: 0.5508\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.0309 - accuracy: 0.6355 - val_loss: 1.3393 - val_accuracy: 0.5492\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 29s 20ms/step - loss: 1.0205 - accuracy: 0.6381 - val_loss: 1.3642 - val_accuracy: 0.5456\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.0118 - accuracy: 0.6449 - val_loss: 1.3824 - val_accuracy: 0.5410\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 0.9969 - accuracy: 0.6492 - val_loss: 1.3580 - val_accuracy: 0.5504\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 1.3253 - accuracy: 0.5360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.325295090675354, 0.5360000133514404]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_bn_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Is the model converging faster than before?* Much faster! The previous model took 27 epochs to reach the lowest validation loss, while the new model achieved that same loss in just 5 epochs and continued to make progress until the 16th epoch. The BN layers stabilized training and allowed us to use a much larger learning rate, so convergence was faster.\n",
    "* *Does BN produce a better model?* Yes! The final model is also much better, with 54.0% accuracy instead of 47.6%. It's still not a very good model, but at least it's much better than before (a Convolutional Neural Network would do much better, but that's a different topic, see chapter 14).\n",
    "* *How does BN affect training speed?* Although the model converged much faster, each epoch took about 12s instead of 8s, because of the extra computations required by the BN layers. But overall the training time (wall time) was shortened significantly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.\n",
    "*Exercise: Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 22s 12ms/step - loss: 1.9028 - accuracy: 0.3216 - val_loss: 1.8474 - val_accuracy: 0.3482\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.6984 - accuracy: 0.3999 - val_loss: 1.7947 - val_accuracy: 0.3474\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.6019 - accuracy: 0.4346 - val_loss: 1.7130 - val_accuracy: 0.3970\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.5365 - accuracy: 0.4613 - val_loss: 1.6063 - val_accuracy: 0.4456\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.4802 - accuracy: 0.4825 - val_loss: 1.5612 - val_accuracy: 0.4598\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.4378 - accuracy: 0.4985 - val_loss: 1.5162 - val_accuracy: 0.4764\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3913 - accuracy: 0.5130 - val_loss: 1.5164 - val_accuracy: 0.4646\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3511 - accuracy: 0.5289 - val_loss: 1.5060 - val_accuracy: 0.4752\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.3169 - accuracy: 0.5411 - val_loss: 1.5019 - val_accuracy: 0.4760\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2862 - accuracy: 0.5548 - val_loss: 1.4801 - val_accuracy: 0.4940\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2527 - accuracy: 0.5647 - val_loss: 1.5412 - val_accuracy: 0.4780\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2225 - accuracy: 0.5761 - val_loss: 1.4797 - val_accuracy: 0.4886\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1950 - accuracy: 0.5870 - val_loss: 1.4856 - val_accuracy: 0.5064\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1753 - accuracy: 0.5909 - val_loss: 1.4951 - val_accuracy: 0.5038\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1491 - accuracy: 0.6016 - val_loss: 1.5279 - val_accuracy: 0.5060\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1228 - accuracy: 0.6102 - val_loss: 1.5121 - val_accuracy: 0.5142\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.0953 - accuracy: 0.6228 - val_loss: 1.5514 - val_accuracy: 0.5012\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.0854 - accuracy: 0.6264 - val_loss: 1.5266 - val_accuracy: 0.5100\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0594 - accuracy: 0.6348 - val_loss: 1.5499 - val_accuracy: 0.5170\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.0483 - accuracy: 0.6422 - val_loss: 1.5574 - val_accuracy: 0.5036\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0228 - accuracy: 0.6501 - val_loss: 1.5682 - val_accuracy: 0.5020\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9996 - accuracy: 0.6586 - val_loss: 1.5585 - val_accuracy: 0.5004\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 0.9768 - accuracy: 0.6650 - val_loss: 1.5622 - val_accuracy: 0.4940\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.9794 - accuracy: 0.6676 - val_loss: 1.5562 - val_accuracy: 0.4946\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.9553 - accuracy: 0.6748 - val_loss: 1.5503 - val_accuracy: 0.5084\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.9342 - accuracy: 0.6820 - val_loss: 1.6446 - val_accuracy: 0.5074\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9343 - accuracy: 0.6816 - val_loss: 1.5864 - val_accuracy: 0.5084\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.9117 - accuracy: 0.6903 - val_loss: 1.6282 - val_accuracy: 0.4898\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.8965 - accuracy: 0.6950 - val_loss: 1.5946 - val_accuracy: 0.5034\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.8834 - accuracy: 0.7010 - val_loss: 1.6607 - val_accuracy: 0.4932\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.8625 - accuracy: 0.7063 - val_loss: 1.6911 - val_accuracy: 0.5026\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.8477 - accuracy: 0.7130 - val_loss: 1.7228 - val_accuracy: 0.5020\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 1.4797 - accuracy: 0.4886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.479697346687317, 0.4885999858379364]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=7e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 4ms/step - loss: 1.4797 - accuracy: 0.4886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.479697346687317, 0.4885999858379364]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 47.9% accuracy, which is not much better than the original model (47.6%), and not as good as the model using batch normalization (54.0%). However, convergence was almost as fast as with the BN model, plus each epoch took only 7 seconds. So it's by far the fastest model to train so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e.\n",
    "*Exercise: Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 28s 16ms/step - loss: 1.8815 - accuracy: 0.3278 - val_loss: 1.7211 - val_accuracy: 0.4020\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.6534 - accuracy: 0.4176 - val_loss: 1.6282 - val_accuracy: 0.4244\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.5634 - accuracy: 0.4507 - val_loss: 1.6071 - val_accuracy: 0.4334\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.5011 - accuracy: 0.4714 - val_loss: 1.5902 - val_accuracy: 0.4504\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.4433 - accuracy: 0.4918 - val_loss: 1.5304 - val_accuracy: 0.4636\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.3919 - accuracy: 0.5127 - val_loss: 1.5480 - val_accuracy: 0.4864\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.3560 - accuracy: 0.5258 - val_loss: 1.5320 - val_accuracy: 0.4854\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3157 - accuracy: 0.5400 - val_loss: 1.4859 - val_accuracy: 0.4964\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.2770 - accuracy: 0.5566 - val_loss: 1.4942 - val_accuracy: 0.4918\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.2459 - accuracy: 0.5671 - val_loss: 1.5197 - val_accuracy: 0.5060\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.2124 - accuracy: 0.5801 - val_loss: 1.5773 - val_accuracy: 0.5022\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.1874 - accuracy: 0.5881 - val_loss: 1.5308 - val_accuracy: 0.5014\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.1554 - accuracy: 0.5995 - val_loss: 1.5327 - val_accuracy: 0.5048\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.1303 - accuracy: 0.6106 - val_loss: 1.5517 - val_accuracy: 0.5138\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.1111 - accuracy: 0.6172 - val_loss: 1.5888 - val_accuracy: 0.5076\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.0853 - accuracy: 0.6274 - val_loss: 1.6145 - val_accuracy: 0.5080\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.0606 - accuracy: 0.6362 - val_loss: 1.6028 - val_accuracy: 0.5130\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.0411 - accuracy: 0.6436 - val_loss: 1.6190 - val_accuracy: 0.5106\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.0109 - accuracy: 0.6516 - val_loss: 1.7037 - val_accuracy: 0.5156\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 0.9968 - accuracy: 0.6607 - val_loss: 1.7084 - val_accuracy: 0.5026\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 0.9766 - accuracy: 0.6663 - val_loss: 1.6989 - val_accuracy: 0.5098\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 0.9577 - accuracy: 0.6748 - val_loss: 1.5839 - val_accuracy: 0.5148\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 0.9410 - accuracy: 0.6785 - val_loss: 1.7262 - val_accuracy: 0.5128\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.9230 - accuracy: 0.6872 - val_loss: 1.6928 - val_accuracy: 0.5184\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 0.9009 - accuracy: 0.6927 - val_loss: 1.8381 - val_accuracy: 0.5066\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 0.8953 - accuracy: 0.6975 - val_loss: 1.6995 - val_accuracy: 0.5026\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 0.8711 - accuracy: 0.7041 - val_loss: 1.6940 - val_accuracy: 0.5130\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 0.8543 - accuracy: 0.7123 - val_loss: 1.7396 - val_accuracy: 0.5036\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 1.4859 - accuracy: 0.4964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4859399795532227, 0.49639999866485596]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_alpha_dropout_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_alpha_dropout_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_alpha_dropout_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model reaches 48.9% accuracy on the validation set. That's very slightly better than without dropout (47.6%). With an extensive hyperparameter search, it might be possible to do better (I tried dropout rates of 5%, 10%, 20% and 40%, and learning rates 1e-4, 3e-4, 5e-4, and 1e-3), but probably not much better in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use MC Dropout now. We will need the `MCAlphaDropout` class we used earlier, so let's just copy it here for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a new model, identical to the one we just trained (with the same weights), but with `MCAlphaDropout` dropout layers instead of `AlphaDropout` layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's add a couple utility functions. The first will run the model many times (10 by default) and it will return the mean predicted class probabilities. The second will use these mean probabilities to predict the most likely class for each instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make predictions for all the instances in the validation set, and compute the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.497"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get no accuracy improvement in this case (we're still at 48.9% accuracy).\n",
    "\n",
    "So the best model we got in this exercise is the Batch Normalization model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f.\n",
    "*Exercise: Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 6s 14ms/step - loss: nan - accuracy: 0.1368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.999999747378752e-06,\n",
       " 9.615227699279785,\n",
       " 2.611842393875122,\n",
       " 4.002453429358346)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "352/352 [==============================] - 6s 15ms/step - loss: 2.0517 - accuracy: 0.2890 - val_loss: 1.7945 - val_accuracy: 0.3772\n",
      "Epoch 2/15\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 1.7612 - accuracy: 0.3758 - val_loss: 1.6345 - val_accuracy: 0.4232\n",
      "Epoch 3/15\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 1.6161 - accuracy: 0.4255 - val_loss: 1.6279 - val_accuracy: 0.4294\n",
      "Epoch 4/15\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 1.5418 - accuracy: 0.4572 - val_loss: 1.6117 - val_accuracy: 0.4348\n",
      "Epoch 5/15\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 1.4917 - accuracy: 0.4715 - val_loss: 1.5634 - val_accuracy: 0.4650\n",
      "Epoch 6/15\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 1.4498 - accuracy: 0.4854 - val_loss: 1.5302 - val_accuracy: 0.4766\n",
      "Epoch 7/15\n",
      "352/352 [==============================] - 5s 16ms/step - loss: 1.4133 - accuracy: 0.4984 - val_loss: 1.6117 - val_accuracy: 0.4444\n",
      "Epoch 8/15\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 1.3495 - accuracy: 0.5204 - val_loss: 1.5603 - val_accuracy: 0.4684\n",
      "Epoch 9/15\n",
      "352/352 [==============================] - 7s 21ms/step - loss: 1.2734 - accuracy: 0.5469 - val_loss: 1.5494 - val_accuracy: 0.4708\n",
      "Epoch 10/15\n",
      "352/352 [==============================] - 6s 17ms/step - loss: 1.2063 - accuracy: 0.5702 - val_loss: 1.5418 - val_accuracy: 0.4952\n",
      "Epoch 11/15\n",
      "352/352 [==============================] - 5s 14ms/step - loss: 1.1381 - accuracy: 0.5927 - val_loss: 1.5292 - val_accuracy: 0.5034\n",
      "Epoch 12/15\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 1.0699 - accuracy: 0.6168 - val_loss: 1.5053 - val_accuracy: 0.5110\n",
      "Epoch 13/15\n",
      "352/352 [==============================] - 6s 17ms/step - loss: 1.0011 - accuracy: 0.6412 - val_loss: 1.5124 - val_accuracy: 0.5262\n",
      "Epoch 14/15\n",
      "352/352 [==============================] - 8s 23ms/step - loss: 0.9349 - accuracy: 0.6669 - val_loss: 1.5353 - val_accuracy: 0.5284\n",
      "Epoch 15/15\n",
      "352/352 [==============================] - 7s 21ms/step - loss: 0.8951 - accuracy: 0.6828 - val_loss: 1.5629 - val_accuracy: 0.5270\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train_scaled) / batch_size) * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One cycle allowed us to train the model in just 15 epochs, each taking only 2 seconds (thanks to the larger batch size). This is several times faster than the fastest model we trained so far. Moreover, we improved the model's performance (from 47.6% to 52.0%). The batch normalized model reaches a slightly better performance (54%), but it's much slower to train."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
